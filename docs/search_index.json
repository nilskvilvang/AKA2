[["index.html", "Anvendt kvantitativ analyse Introduksjon Versjoner", " Anvendt kvantitativ analyse Siste endring 07 mai, 2022 Introduksjon Dette notatet gir en introduksjon til anvendt kvantitativ analyse rettet mot samfunnsvitenskapene. Innholdet er under utvikling og oppdateres jevnlig om enn noe uregelmessig. Tilbakemeldinger og innspill bes gitt til Nils Kvilvang. Bilde: Ukjent opphav Versjoner Dato Endring 22.apr 2022 Grunnversjon 22.apr 2022 Lagt til kapittel om regresjonsanalyse 23.apr 2022 Oppdatert kode embedding av filer. Korrektur. Endret på rekkefølge to delkapitler. 01.mai 2022 Lagt til PCA. 01.mai 2022 Oppdatert regresjonskapittel med flere analyser av uteliggere. 07.mai 2022 Lagt til delkapittel om Exploratory Factor Analysis "],["hvorfor-dette-notatet.html", "Hvorfor dette notatet? Programvare", " Hvorfor dette notatet? Dette notatet oppsummerer ulike forelesningsnotater og undervisningsopplegg i anvendt kvantitativ analyse. Hensikten er å forsøke å formidle sentrale konsepter og teknikker, samt gi oppskrift på prosedyre i ulike verktøy. Programvare Notatet er skrevet med bruk av R (R Core Team, 2021) og RStudio (RStudio Team, 2022). R er en velkjent programvare innenfor statistikk, dataanalyse, datamodellering osv. R har noen store fordeler; det er gratis, det kjører på alle operativsystemer, og det har et svært stort bruker- og utviklermiljø som i all hovedsak deler alt gratis. Det er også enkelt å finne løsninger på det meste gjennom veiledninger og brukerforum på nett. Selve R er et programmeringsspråk og utviklermiljø for statistikk som gir en kjernefunksjonalitet innenfor datahåndtering, kalkulasjoner, dataanalyse, datamodellering, grafisk framstilling av data o.l. R kommer med 14 basispakker. Det store potensialet ligger imidlertid i at brukerne av R utvikler tilleggspakker som man bruker i R, det finnes pr. april 2022 over 19000 ulike pakker som bygger på kjernen i R. Alle pakkene tilbyr ulike tilrettelagte løsninger for ulike problemer/analyser. Den største ulempen med R er at brukergrensesnittet er veldig ulikt hva vi kjenner fra Microsoft Office-typen brukergrensesnitt, så det vil ta litt tid å bli kjent med programmet. I tillegg er brukergrensesnittet kodebasert og ikke menybasert, og kan (dessverre) virke avskrekkende. Likevel, det er et utrolig kraftig verktøy hvis man tar seg tid til å lære seg det grunnleggende. Brukere av notatet vil sannsynligvis kjøre programvare som enten SPSS eller Stata. Konsepter og eksempler i notatet er som sagt laget og kjørt i R, men tilhørende videoer for framgangsmåte i SPSS og Stata for å få fram tilsvarende analyser på samme eksempler som i notatet ligger tilgjengelig. I tillegg er det på sin plass å nevne alternativer til store og dyre programvarepakker som SPSS og Stata som mer og mer framstår som reelle og gode alternativer. To av disse, jamovi og JASP, har sterkt voksende bruk (også i akademia) rundt om i verden. Dette er grafisk tiltalende og funksjonsrike statistikkpakker som kjører med R i bakgrunnen (alle analyser i jamovi og JASP bruker R), og som også kan inkludere R kode direkte. Det gjør at man kan utnytte alle pakkene skrevet for R direkte i de grafiske brukergrensesnitt. I tillegg kan man (i varierende grad mellom de to) hente ut R-kode fra analyser man gjør gjennom det grafiske brukergrensesnittet - noe som kan gjøre en overgang/introduksjon til R lettere om man vil den veien. Leseren står selvsagt fritt til å hoppe elegant over alle verktøy som ikke er interessante. Det er klare fordeler og ulemper med alle, men forhåpentligvis vil de fleste finne et verktøy de kan bruke i utvalget vi har tatt med. Der det er naturlig, som ved bruk av R og RStudio, er kodingen inkludert slik at eksempler skal kunne replikeres av leseren, men vi går ikke inn på R utover dette. Kodingen er gjengitt fortløpende der analysen er gjort. Vi har brukt R/RStudio og rmarkdown (Allaire et al., 2022)  en såkalt pakke til R  i produksjonen av dette notatet. R baserer seg som sagt på bruk av ulike pakker som er utviklet av forskjellige utviklere og som er fritt tilgjengelig. Mange av disse har også datasett inkludert slik at det er enkelt å replikere eksempler. Så langt det er mulig har vi basert oss på at det vi viser som eksempler skal være replikerbare for leseren. Vi har lite fokus på matematikk og formler i den forstand at vi ikke utleder i dybden forklaring på ulike formler. Vi tror det er fullt mulig å ha en praktisk forståelse og anvendt bruk av kvantitative analsyer uten å ha dyptgående kjennskap til de matematiske eller statistiske forklaringene. Likevel, det er heller ikke slik at vi absolutt skal unngå dette. Vi har derfor inkludert noe bakgrunnskunnskap for å skape en ramme rundt kjernen i notatet der vi tenker det kan være interessant for de som ønsker å fordype seg noe mer. div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} "],["regresjonsanalyse.html", "1 Regresjonsanalyse 1.1 Innledning 1.2 Teori 1.3 Enkel, lineær regresjonsanalyse 1.4 Standard multippel regresjonsanalyse 1.5 Hierarkisk multippel regresjonsanalyse 1.6 Stegvis mutlippel regresjonsanalyse", " 1 Regresjonsanalyse 1.1 Innledning Regresjonsanalyse er et særtilfelle av variansanalyse, og er i følge Mehmetoglu &amp; Mittner (2020) muligens en mest brukte analysemetoden for dataanalyse, eller arbeidshesten i foskning på økonomiske og sosiale forhold (Thrane, 2019). Det er først og fremst metodens er fleksibilitet som en hovedgrunn til dette. En regresjonsanalyse er en statistisk analyse som undersøker sammenhengen mellom en kontinuerlig avhengig variabel og en eller flere kontinuerlige og/eller kategoriske uavhengige variabler. Selv om korrelasjon kan være veldig hjelpsomt å forstå vil en regresjonsanalyse søke å ta vår forståelse av sammenhengen litt videre, til for eksempel å forsøke å predikere nivået i en avhengig variabel ut fra nivået på den/de uavhengige variabler. Hvis vi lykkes med dette vil vi kunne klare å si noe om forventet verdi på et fenomen vi er interessert i ut fra kjente verdier på andre variabler. La oss anta at vi har variabler som beliggenhet (avstand fra sentrum), areal, etasje, solforhold, antall rom, antall bad, standard på bad og liknende for en leilighet kan vi bruke disse uavhengige variablene til å predikere en salgssum for denne boligen (som en avhengig variabel). Vi lager da en modell for dette forholdet - i dette tilfellet en regresjonsmodell. Vi går dermed fra å spørre om det er en sammenheng til å spørre hvilken sammenheng det er. La oss forsøke å illustrere prinsippet med regresjonsanalyse gjennom er såkalt Venndiagram. Den gule sirkelen illustrerer det forholdet vi er interessert i å finne ut noe om. Den representerer det vi kaller den avhengige variabelen - fordi det vi ønsker å finne ut er avhengig av andre forhold (andre variabler). Vi kan si at den gule sirkelen viser variasjonen i drivstofforbruket til alle biler vi har med i undersøkelsen vår, og vi betegner denne variabelen \\(Y\\). Biler har ulikt drivstofforbruk, så vi har altså en variasjon i drivstofforbruket mellom bilene. Den blå sirkelen viser variasjonen i motorstørrelse (vi kaller denne variabelen for \\(x_1\\)). Ulike biler har ulik motorstørrelse, og vi tenker at større motor betyr mer drivstofforbruk enn mindre motor. Den grønne sirkelen representerer en variabel vi har kalt kjørestil (\\(x_2\\)). Vi har en hypotese om at vi kan predikere (forutsi) drifstofforbruket til en gitt bil ut fra motorstørrelse og kjørestil. Så det vi ønsker å se på er hvor mye av korrelasjonen mellom drivstofforbruk og motorstørrelse skyldes faktisk motorstørrelse, og hvor mye skyldes kjørestil. Vi tenker også at kjørestil og drivstofforbruk er korrelert (det er naturlig å tenke seg at personer med en aggresiv kjørestil har biler med større motorer - det er altså en korrelasjon mellom kjørestil og motorstørrelse). Vi ser dette i figuren under. Korrelasjonen mellom drivstofforbruk og motorstørrelse er gitt i områdene merket 1 og 2. Korrelasjonen mellom drivstofforbruk og kjørestil er gitt i områdene 2 og 3. Korrelasjonen mellom kjørestil og motorstørrelse er gitt i 2 og 4. Området 2 viser den delte variasjonen mellom drivstofforbruk, motorstørrelse og kjørestil. Det vil innebære at vi kan bruke regresjonsanalsye til å isolere ut område 1 ved å se på motorstørrelsens totale korrelasjon med drivstofforbruk og trekke fra den delen av den totale korrelasjonen som deles med kjørestil (område 2). Da finner vi motorstørrelsens (\\(x_1\\)s) unike bidrag. Det samme kan vi gjøre for kjørestil, der det unike bidraget utgjøres av område 3. Vi kan selvsagt ha flere prediktorer (uavhengige variabler) - noe vi veldig ofte vil ha. Det vi gjør er i prinsippet det samme: vi tar bort biter av korrelasjonen mellom motorstørrelse og drivstofforbruk som skyldes samvariasjon med andre variabler slik at vi får isolert den delen av korrelasjonen som utelukkende skyldes motorstørelse. Man kan tenke seg en ny variabel med rød sirkel. Igjen - regresjonsanalysen forsøker å isolere den unike delen for korrelasjonen mellom motorstørrelse og drivstofforbruk (og det samme for de andre variablene: den unike delen). Når vi klarer å isolere den unike korrelasjonen kan vi også si at vi har isolert den unike kausale effekten motorstørrelse har på drivstofforbruket (gitt at vi har inkludert alle relevant uavhengige variabler i modellen, noe vi i praksis sjelden vil klare). Den videre innledningen til regresjonsanalyse tar utgangspunkt i eksempelet i Løvås (2013) (boka kom ut i 4. utgave i 2018). Illustrasjonene som er brukt er hentet fra bokas nettressurser. Løvås bok «Statistikk for universiteter og høgskoler» kan anbefales som introduksjonsbok til statistikk på universitets- og høgskolenivået. En annen bok som fungerer fint til dette formålet er Jan Ubøes Statistikk for økonomifag (vi har brukt 4. utgave, 2014 - 5. utgave kom i 2015) (Ubøe, 2014). 1.2 Teori Eksempelet i Løvås (2013) dreier seg om sammenhengen mellom motorstørrelse og drivstofforbruk. Vi kan måle motorstørrelse i hestekrefter (hk) og drivstofforbruk i liter/mil. For å vise sammenhengen kan vi sette opp ligningen \\(Y_i=\\alpha\\:+\\beta x_i+e_i\\) der \\(Y=drivstofforbruket\\) \\(\\alpha=konstantleddet\\) (krysningspunktet på y-aksen, altså Y-ver dien om x er 0) \\(\\beta=linjens\\:stigningstall\\) (dersom x øker med 1, øker y med \\(\\beta\\) \\(e=forstyrrelsen\\) (vi antar at det er flere ting som forstyrrer forholdet mellom motorstørrelse og drivstofforbruk - drivstofforbruket er ikke bare avhengig av motorstørrelse). Vi skal snakke mye om residualer i regresjonsanalyse - residualer er dette restleddet/feilleddet/forstyrrelsen. En av forutsetningene i regresjonsanalyse er knyttet til fordelingen av disse residualene, men det kommer vi tilbake til. Dersom vi ikke hadde hatt et feilledd kunne vi framstilt denne ligningen slik: Når vi plotter inn et antall observasjoner av motorstørrelse og drivstofforbruk kan det se slik ut: Det vi i en lineær regresjonsanalyse gjør er å finne den rette linja som best passer til disse observasjonene. Vi ønsker altså å finne en rett linje som best «beskriver» observasjonene. Tenk deg at vi trekker den rette linja som samlet sett ligger nærmest punktene og deretter tar bort punktene. Det vi sitter igjen med er regresjonslinja. Denne linja gir oss da tilgang til alle punkter som ligger på linja som en modell på sammenhengen mellom de to variablene. Selv om vi bare hadde noen observasjoner på gitte punkter på x-aksen har vi gjennom regresjonslinja fått tilgang til alle tenkelige punkter på x-linja og kan anta et drivstofforbruk ut fra det (ved å gå opp fra x-aksen, finne skjæringspunktet med regresjonslinja, og deretter gå inn på y-aksen og lese av drivstofforbruket). Den prediksjonen vi da gjør er vår beste gjetning på hvor stort drivstofforbruket vil være for en gitt motorstørrelse. Dette vil selvsagt være en kvalifisert gjetning - nettopp fordi det er en modell. Og alle modeller er feil, men noen modeller er nyttige likevel. I eksempelet kan vi for eksempel tenke oss to mulige linjer: Begge linjene er forsøk på å lage en rett linje som har kortest mulig avvik. Vi kan deretter legge sammen de absolutte vertikale avstandene (de stiplede linjene) fra observasjonspunktene ned til den rette linja. I prinsippet er da den rette linja som medfører minst samlet avstand fra observasjonspunktene den rette linja som best representerer observasjonspunktene, og vi kan si vi har laget en modell for sammenhengen mellom motorstørrelse og drivstofforbruk. Siden vi har en sammenhengende rett linje har vi også mulighet til å mene noe om drivstofforbruk på motorstørrelser vi ikke har målt/har observasjoner på. Vi har med andre ord en modell for å predikere drivstofforbruk ut fra motorstørrelse. Uavhengige variabler i regresjonsanalyser kalles også ofte prediktorer, fordi vi bruker de til å predikere en verdi for den avhengige variabelen. 1.2.1 Minste kvadratsum (Ordinary Least Squares - OLS) Imidlertid er absoluttverdier matematisk problematiske (Løvås, 2013). Pre-datamaskiner ble det derfor utviklet en alternativ måte som kalles «minste kvadraters metode» - derav begrepet OLS («Ordinary Least Squares»). Det finnes andre måter å tilnærme seg dette, men i dette kurset går vi kun inn på OLS-regresjon.Hvis vi fortsetter eksempelet over kan vi tenke oss en mengde forslag på ulike linjer som forsøker å beskrive sammenhengen mellom de to variablene: Man regner deretter ut kvadratene som dannes av hvert punkt og avstanden til den rette linja. Den linja som har den laveste kvadratsummen («least squares») er den linja som best representerer datapunktene og som derfor er den beste lineære modellen av forholdet mellom variablene. Regresjonslinja er således en modell. Som Thrane (2019) beskriver: den diagonale linja oppsummerer den typiske trenden i det statistiske forholdet mellom de to variablene - en linje vi kjenner som regresjonslinja. Hvis vi har et stort antall datapunkter er dette selvsagt en omfattende prosess å gjøre manuelt. Det program som jamovi gjør for oss er å regne ut kvadratsummen for et stort antall mulige linjer og deretter fortelle oss hvilken som har lavest kvadratsum. Hvis vi tenker tilbake til formelen for modellen vår: \\(Y_i=\\alpha\\:+\\beta x_i+e_i\\) kan vi nå fylle ut med verdier fra eksempelet. Det vi egentlig har gjort når vi finner den rette linja som gir minste kvadratsum er å identifisere \\(\\alpha\\) (skjæringspunktet på y-aksen) og \\(\\beta\\) (stigningstallet). Statistikkprogrammer vil gi oss verdiene på dette. Vi går ikke inn på en manuell utregning her, men bruker de to verdiene Løvås (2013) viser (\\(\\alpha=0.211\\) og \\(\\beta=0.00576\\)). Vår modell ser da slik ut: \\(Y_i=0,211\\:+0,00576x_i\\). Som sagt har vi ønsket å lage en modell som predikerer drivstofforbruk ut fra motorstørrelse  eller sagt på en annen måte: hvilket drivstofforbruk kan vi forvente med en motor på 100 hk? Vi får da: \\[Y_i=0,211\\:+0,00576x_i=0,211\\:+\\:0,00576\\times100\\:=\\:0,787\\] Dette blir vårt best guess, vår antakelse (vår prediksjon av verdien på y-aksen som er drivstofforbruket ut fra verdien på x-aksen som er motorstørrelse), om forventet drivstofforbruk for en motor med 100 hk basert på den modellen vi har laget om sammenhengen mellom motorstørrelse og drivstofforbruk (som er basert på de observasjonene vi har). Vi kan naturligvis umiddelbart tenke at drivstofforbruket er avhengig av mange andre faktorer enn motorstørrelse, for eksempel bilens design (luftmotstand), vekt, rullemotstand, temperatur, type motor og så videre. Dette belyser for så vidt et sentralt problem når vi ønsker å lage modeller for prediksjon: Virkeligheten er utrolig sammensatt, mange relevante variabler er vanskelig å måle, og man ønsker en modell som er enkel nok til å kunne brukes og sammensatt nok til å gi relevante prediksjoner. Tenk for eksempel bare på «klimamodellene» som brukes for å analysere og predikere temperatur, issmelting, global oppvarming og liknende. Det er klart at i de fleste tilfeller trenger vi flere prediktorer enn en  og i regresjonssammenheng snakker vi da om multippel regresjonsanalyse. Vi kan ha som en tommelfingerregel at vi skal ha med så mange variabler at modellen har praktisk verdi, men likevel så få som mulig. 1.2.2 Konfidensintervall Avslutningsvis i denne introduksjonen til regresjonsanalyse kan vi se kort på begrepet konfidensintervall (se for eksempel Løvås (2013) eller Hinkle et al. (2003)).For de som ønsker å fordype seg i effektstørrelser og konfidensintervaller anbefales Cumming &amp; Calin-Jageman (2017) Introduction to the new statistics: Estimation, open science, &amp; beyond. Man kan si at estimatet på stigningstallet \\(\\beta\\) er det viktigste resultatet i en regresjonsanalyse fordi dette sier noe om hvor sterk sammenhengen mellom de to variablene er. Løvås (2013) illustrerer dette slik: De røde stiplede linjene utgjør konfidensgrensene for 95 % konfidensintervall. I grafen over har vi kun 5 observasjoner, noe som selvsagt er lite. Vi kan gå litt dypere inn i hvordan konfidensgrensene framkommer i en regresjonsanalyse. La oss anta at vi har et datasett der vi har plottet korrelasjonen mellom en prediktor (den uavhengige variabelen) på x-aksen og en avhengig varaibel på y-aksen (se graf under). Den røde prikken markerer verdien x=8. Verdien på y-aksen (10,458) er vår prediksjon (vår buest guess) på hva verdien i den avhengige variabelen vil være ved den observerte verdien x=8. I punktet x=8 har vi en hel populasjon av mulige normalfordelte verdier. Vårt beste estimat av gjennomsnittsverdien for denne populasjonen er 10,458. Dette er et punktestimat. Det tilhørende intervallestimatet er vårt konfidensintervall. Vi må her tenke på konfidensintervallet som en vertikal linje. Så i stedet for å tenke punkt- og intervallestimat slik Kan vi tenke det slik: Overført til vårt eksempel får vi: Den røde streken er vårt 95 % konfidensintervall for punktestimatet. Hvis vi legger på 95 % konfidensintervaller på alle punktestimatene (alle punktene som utgjør regresjonslinja) kan vi lage de to stiplede linjene som toucher endepunktene på alle konfidensintervallene. Disse to stiplede linjene utgjør da konfidensgrensene for regresjonslinja. Vi ser at konfidensgrensene er lett buede mot hverandre med minst avstand mellom dem på midten. Vi skal kort se på hvorfor det er slik. Vi har nå lagt på et nytt kryss i grafen under. Dette krysset markerer punktet der gjennomsnittene av X og Y krysser. Regresjonslinja må gå gjennom dette punktet, slik at alle alternative regresjonslinjer må pivotere rundt dette punktet. Dette medfører at det er litt større usikkerhet rundt punktestimatenes konfidensintervaller i endene i forhold til i midten. Konfidensintervallene for hvert enkelt punkt blir derfor litt lenger jo lenger ut fra krysningspunket vi går, og resultatet blir en form for buet linje. 1.2.3 Steg i analyse Vi anbefaler at en analyse går gjennom disse stegene: Analyse av dataene Evtentuelt valg av prediktorer ut fra analyse av dataene Lage modell (kjøre regresjonsanalysen) Analyse av resultatene (diagnostikk) Sjekk av forutsetningene Eventuell revisjon av modellen Eventuell analyse av revidert modell Konklusjon / oppsummering / rapportering av resultater Vi skal i det følgende gå gjennom disse stegene i en regresjonsanalyse. 1.3 Enkel, lineær regresjonsanalyse Dette eksempelet bygger på Field (2009). Du kan laste ned datasettet i ulike formater her: Download Field_datasett_OLS.xlsx Download Field_datasett_OLS.sav Download Field_datasett_OLS.dta Datasettet kan også finnes her Vi skal nå gå gjennom våre anbefalte steg i analysen, og starter med en analyse av dataene. 1.3.1 Steg 1: Analyse av dataene As soon as you have collected your data, before you compute any statistics, look at your data. Data screening is not data snooping. It is not an opportunity to discard data or change values to favor your hypotheses. However, if you assess hypotheses without examining your data, you risk publishing nonsense (Wilkinson &amp; the Task Force on Statistical Inference, 1999). Vi ser på datasettet. # Bruker pakken: readxl Field_OLS_data &lt;- read_excel(&quot;Field_datasett_OLS.xlsx&quot;) # Bruker pakken: car brief(Field_OLS_data) ## # A tibble: 200 x 4 ## Adverts Sales Airplay Image ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.3 330 43 10 ## 2 986. 120 28 7 ## 3 1446. 360 35 7 ## 4 1188. 270 33 7 ## 5 575. 220 44 5 ## 6 569. 170 19 5 ## 7 472. 70 20 1 ## 8 537. 210 22 9 ## 9 514. 200 21 7 ## 10 174. 300 40 7 ## # ... with 190 more rows Vi kan se at datasettet består av 200 obervasjoner av 4 variabler. Hver av observasjonene er en CD: Adverts: Dette er summen brukt på reklame før lanseringsdato Sales: Dette er salgtall per uke Airplay: Antall ganger et spor fra CDen ble spilt på radio i uka før lanseringsdato Image: En rating på hvor attraktiv (positivt image) gruppen/artisten har Hvis vi ønsker å se på i hvilken grad vi kan predikere salgstall gjennom hvor mye vi bruker på reklame før lansering kan vi gjøre en lineær regresjonsanalyse med salg som avhengig variable og reklame (adverts) som uavhengig variabel. I en analyse av dataene kan vi for eksempel være interessert i noen nøkkeltall: # base R summary(Field_OLS_data) ## Adverts Sales Airplay Image ## Min. : 9.104 Min. : 10.0 Min. : 0.00 Min. : 1.00 ## 1st Qu.: 215.918 1st Qu.:137.5 1st Qu.:19.75 1st Qu.: 6.00 ## Median : 531.916 Median :200.0 Median :28.00 Median : 7.00 ## Mean : 614.412 Mean :193.2 Mean :27.50 Mean : 6.77 ## 3rd Qu.: 911.226 3rd Qu.:250.0 3rd Qu.:36.00 3rd Qu.: 8.00 ## Max. :2271.860 Max. :360.0 Max. :63.00 Max. :10.00 Ofte er det imidlertd mer hensiktsmessig å se på dataene grafisk i en utforskende hensikt (Tukey, 1977). 1.3.1.1 Histogram # base R par(mfrow=(c(1,2))) histSales &lt;- with(Field_OLS_data, hist(Sales)) histAdverts &lt;- with(Field_OLS_data, hist(Adverts)) Det kan se ut som at salgstallene er rimelig normalfordelte, mens reklamevariabelen er klart skjev. 1.3.1.2 Quantile-Quantile plott (qq) Q-Q plottet (quantile-quantile plot) kan tolkes ved å se om dataverdiene ligger langs en rett linje med ca 45 graders vinkel. Q-Q plottet (se video for forklaring på utregning) innebærer å se to distribusjoner mot hverandre  empirisk fordeling (dataene) og teoretisk forventning ut fra en fordelingsmodell (som normalfordeling om vi snakker om normal Q-Q plott - dvs vi ser om vår empiriske datafordeling og normalfordelingen er lik). Om de samsvarer perfekt ligger de på en helt rett linje (x = y). I eksempelet under vil da alle punktene ligge perfekt oppå den rette linjen. Siden vi vet den teoretiske distribusjonen til normalfordelingen, kan vi bruke denne teoretiske fordelingen til å plotte den mot datasettet vi sitter med. # Bruker pakken: car par(mfrow=(c(1,2))) qqSales2 &lt;- car::qqPlot(Field_OLS_data$Adverts) qqAdverts2 &lt;- car::qqPlot(Field_OLS_data$Sales) Som vi fikk indikert gjennom historgrammene er salgsvariabelen rimelig normalfordelt, mens reklamevariabelen viser avvik fra normalfordelingen - i dette tilfellet (ut fra histogram og qq-plott vil vi si den er høyreskjev). Under viser vi typiske mønstre for histogram og tilhørende qq-plott som kan være til hjelp i tolkning av dataene dine. Dette er genererte tall og ikke tallene fra eksempelet over: 1.3.1.2.1 Normalfordelt set.seed(89) # base R # Bruker pakken: tibble (Tidyverse) qqnorm &lt;- as_tibble(rnorm(10000, mean=90, sd=5)) # Bruker pakken: writexl write_xlsx(qqnorm,&quot;QQ_norm.xlsx&quot;) Download QQ_norm.xlsx # Bruker pakken: ggpubr ggqqplot(qqnorm$value) + ggtitle(&quot;Normal Q-Q plott&quot;) + labs(x = &quot;Teoretisk forventning&quot;, y = &quot;Data&quot;) Figure 1.1: Q-Q plott normalfordeling Vi ser at dette Q-Q plottet viser oss at vi kan være ganske sikre på at dette datasettet er normalfordelt (noe som gir meninig siden vi har brukt R til å lage et normalfordelt datasett). 1.3.1.2.2 Skjevhet høyre # Lage datasett med right skew # base R # Bruker pakken: tibble set.seed(90) N &lt;- 5000 qqrightskew &lt;- as_tibble(rnbinom(N, 10, .1)) # Eksportere datasettet # Bruker pakken: writexl write_xlsx(qqrightskew,&quot;QQ_norm_rs.xlsx&quot;) Download QQ_norm_rs.xlsx # Plotte histogram og Q-Q plott&quot; # Bruker pakken: ggplot2 qqrighthist &lt;- ggplot(qqrightskew, aes(x=value)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightblue&quot;) # Bruker pakken: ggpubr qqrightskew_plott &lt;- ggqqplot(qqrightskew$value) + ggtitle(&quot;Normal Q-Q plott - skjevhet høyre&quot;) + labs(x = &quot;Teoretisk forventning&quot;, y = &quot;Data&quot;) # Bruker pakken: gridExtra grid.arrange(qqrighthist, qqrightskew_plott, ncol=2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.2: Q-Q plott - fordeling skjevhet høyre I et datasett med høyreskjevhet vil ofte Q-Q plottet vise en bananform med bunnen/midten av bananen ned mot høyre hjørne og endene pekende oppover/utover fra den rette linjen. 1.3.1.2.3 Skjevhet venstre # Lage datasett med left skew # base R # Bruker pakken: tibble set.seed(91) N=5000 qqleftskew &lt;- as_tibble(rbeta(N,5,1,ncp=0)) # Eksportere datasettet # Bruker pakken: writexl write_xlsx(qqleftskew,&quot;QQ_norm_ls.xlsx&quot;) Download QQ_norm_ls.xlsx # Plotte histogram og Q-Q plott # Bruker pakken: ggplot2 qqlefthist &lt;- ggplot(qqleftskew, aes(x=value)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightblue&quot;) # Bruker pakken: ggpubr qqleftskew_plott &lt;- ggqqplot(qqleftskew$value) + ggtitle(&quot;Normal Q-Q plott - skjevhet venstre&quot;) + labs(x = &quot;Teoretisk forventning&quot;, y = &quot;Data&quot;) # Bruker pakken: gridExtra grid.arrange(qqlefthist, qqleftskew_plott, ncol=2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.3: Q-Q plott - fordeling skjevhet venstre I dette datasettet har vi generert en kraftig skjevhet til venstre. Q-Q plottet får da en omvendt bananform i forhold til høyre skjevhet, altså en topp på midten og to ender som svinger nedover ift den rette linja. 1.3.1.2.4 Tunge haler set.seed(14) N=100 # Bruker pakken: tibble # Base R qqcauchy &lt;- as_tibble(rcauchy(N, scale = 5)) # Eksportere datasettet # Bruker pakken: writexl write_xlsx(qqcauchy,&quot;QQ_ht.xlsx&quot;) Download QQ_ht.xlsx # Bruker pakken: ggplot2 qqcauchyhist &lt;- ggplot(qqcauchy, aes(x=value)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightblue&quot;) # Bruker pakken: ggpubr qqcauchy_plott &lt;- ggqqplot(qqcauchy$value) + ggtitle(&quot;Normal Q-Q plott - tung hale&quot;) + labs(x = &quot;Teoretisk forventning&quot;, y = &quot;Data&quot;) # Bruker pakken: gridExtra grid.arrange(qqcauchyhist, qqcauchy_plott, ncol=2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.4: Q-Q plott - heavy-tail Heavy-tailed (fete/tunge haler) har større sannsynlighet for at ekstreme verdier vil forekomme). Fordelinger med tunge haler vil ofte følge en slags S-form, men den er ofte mer liggende enn S-formen til fordeling med lette haler. Den starter med å vokse raskere enn normalfordelingen og ender med å vokse saktere. 1.3.1.2.5 Lette haler set.seed(81) # Base R # Bruker pakken: tibble qqlt &lt;- as_tibble(runif(n = 1000, min = -1, max = 1)) # Eksportere datasettet # Bruker pakken: writexl write_xlsx(qqlt,&quot;QQ_lt.xlsx&quot;) Download QQ_lt.xlsx # Bruker pakken: ggplot2 qqlthist &lt;- ggplot(qqlt, aes(x=value)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightblue&quot;) # Bruker pakken ggpubr qqlt_plott &lt;- ggqqplot(qqlt$value) + ggtitle(&quot;Normal Q-Q plott - lett hale&quot;) + labs(x = &quot;Teoretisk forventning&quot;, y = &quot;Data&quot;) # Bruker pakken: gridExtra grid.arrange(qqlthist, qqlt_plott, ncol=2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.5: Q-Q plott - light-tail Light-tailed (lette haler) har liten sannsynlighet for ekstreme verdier og utvalg tenderer til å ikke fravike gjennomsnittet med mye. Q-Q plottet for en fordeling med lette haler har ofte en S-form. Dataene vokser saktere enn normalfordelingen i starten før den følger vekstraten til normalfordelingen. Mot slutten vokser den raskere enn normalfordelingen. Derfor bøyer den av fra normalfordelingen. 1.3.1.2.6 Bimodalitet set.seed(10) # Base R # Bruker pakken: ggplot2 # Bruker pakken: tibble mode1 &lt;- rnorm(50,2,1) mode1 &lt;- mode1[mode1 &gt; 0] mode2 &lt;- rnorm(50,6,1) mode2 &lt;- mode2[mode2 &gt; 0] qqbimod &lt;- as_tibble(sort(c(mode1,mode2))) # Eksportere datasettet # Bruker pakken: writexl write_xlsx(qqbimod,&quot;QQ_bimod.xlsx&quot;) Download QQ_bimod.xlsx # Plotte histogram og Q-Q plott # Bruker pakken ggplot2 qqbimodhist &lt;- ggplot(qqbimod, aes(x=value)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightblue&quot;) # Bruker pakken: ggpubr qqbimod_plott &lt;- ggqqplot(qqbimod$value) + ggtitle(&quot;Normal Q-Q plott - bimodial&quot;) + labs(x = &quot;Teoretisk forventning&quot;, y = &quot;Data&quot;) # Bruker pakken: gridExtra grid.arrange(qqbimodhist, qqbimod_plott, ncol=2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.6: Q-Q plott - bimodal Den bimodiale fordelingen viser ofte et brudd eller et distinkt knekkpunkt rundt krysning av den rette linja, med en del av linja på hver side av den rette linja. 1.3.1.2.7 Multimodalitet # Bruker pakken: readxl # Bruker pakken: tibble qqmultimod &lt;- as_tibble(read_xlsx(&quot;Multimodal.xlsx&quot;)) # Eksportere datasettet # Bruker pakken: writexl write_xlsx(qqmultimod,&quot;QQ_multimod.xlsx&quot;) Download QQ_multimod.xlsx # Plotte histogram og Q-Q plott # Bruker pakken ggplot2 qqmultimodhist &lt;- ggplot(qqmultimod, aes(x=Verdi)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightblue&quot;) # Bruker pakken: ggpubr qqmultimod_plott &lt;- ggqqplot(qqmultimod$Verdi) + ggtitle(&quot;Normal Q-Q plott - multimodial&quot;) + labs(x = &quot;Teoretisk forventning&quot;, y = &quot;Data&quot;) # Bruker pakken: gridExtra grid.arrange(qqmultimodhist, qqmultimod_plott, ncol=2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.7: Q-Q plott - multimodal Multimodale fordelinger vil som regel vise flere brudd. 1.3.1.3 Statistiske tester for vurdering av dataenes distribusjon Vi har nå sett på noen typiske eksempler på mønstre i Q-Q plott. Det kan imidlertid være vanskelig å bedømme fordelinger som ligger nære normalfordelingen, men likevel ikke perfekt oppå (du vil trolig aldri se en perfekt match med mindre du har generert et normalfordelt datasett med mange datapunkter). Vi kan supplere Q-Q plottene med visse statistiske tester (men husk: disse statistiske testene har sine egne forutsetninger og er heller ikke uten utfordringer). 1.3.1.3.1 Anderson-Darling Anderson-Darlings test er en test for å se om et datasett kommer fra en gitt fordeling, f.eks. normalfordelingen (T. W. Anderson &amp; Darling, 1954; Theodore W. Anderson &amp; Darling, 1952). Testen setter opp to hypoteser: \\(H_0\\): Dataene følger normalfordelingen \\(H_1\\): Dataene følger ikke normalfordelingen Download Anderson-Darling_raw.xlsx # Bruker pakken: tibble # Bruker pakken: readxl addata &lt;- as_tibble(read_excel(&quot;Anderson-Darling_raw.xlsx&quot;)) # Bruker pakken: nortest ad.test(addata$Values) ## ## Anderson-Darling normality test ## ## data: addata$Values ## A = 0.74573, p-value = 0.04046 Siden vi vet at nullhypotesen er at datasettet har en normalfordeling vil vi forkaste nullhypotesen dersom vi har en signifikant p-verdi (grensen for hva som er signifikant bestemmer vi forsåvidt selv, men vanlige verdier er 0.01, 0.05 og 0.1). Altså - i dette tilfellet har vi en p-verdi=0.04. Vi forkaster derfor nullhypotesen og aksepterer \\(H_1\\) som sier at dataene er trolig ikke er normalfordelte (med andre ord: p-verdien må være større enn signifikansverdien for at vi skal si at dataene trolig er normalfordelte) - en huskeregel: If p is low, the null must go (her: low = under terskelverdien vi har satt, ofte 0.05). Generisk ser dette slik ut (Hartmann et al., 2018): Det er verdt å merke seg at Anderson-Darling testen egentlig ikke forteller deg at dataene dine er normalfordelte, men at det er usannsynlig at de ikke er det om testen viser det. Dette synes kanskje som samme sak, men er i realiteten en viktig erkjennelse  en tørr gressplen er et bevis for at det ikke har regnet, men en våt gressplen er ikke bevis for at det har regnet. En våt gressplen kan skyldes andre ting enn regn. Altså  en signifikant p-verdi på testen gjør at vi forkaster \\(H_0\\) og antar at fordelingen er ikke-normal. En ikke-signifikant p-verdi på gjør at vi med f.eks. 95% konfidens kan si at vi ikke har funnet avvik fra normalfordelingen. Tabellarisk kan vi oppsummere vurderingene slik: Betingelse Vurdering p-verdi \\(\\le\\) valgt signifikansnivå Forkast \\(H_0\\) - datene er trolig ikke normalfordelte p-verdi &gt; valgt signifikansnivå Behold \\(H_0\\) - dataene er trolig normalfordelte Testverdi (\\(A^2\\) verdi) &gt; kritisk verdi Forkast \\(H_0\\) - datene er trolig ikke normalfordelte Testverdi (\\(A^2\\) verdi) \\(\\le\\) kritisk verdi Behold \\(H_0\\) - dataene er trolig normalfordelte Det finnes flere andre statistiske tester som kan kjøres for å teste for normalitet, f.eks. Kologorov-Smirnov, Shapiro-Wilks og Cramer Von-Mises test. Anderson-Darling er en modifisering/videreutvikling av Kolmogorov-Smirnov og anses ofte som en bedre test av de to. Andre kilder (se f.eks. Razali &amp; Wah (2011)) finner at Shapiro-Wilks presterer best i 10 000 simuleringer på ulike distribusjoner. options(scipen=999) # Bruker pakken: tibble # Bruker pakken: readxl addata5 &lt;- as_tibble(read_excel(&quot;Anderson-Darling_raw.xlsx&quot;)) # Base R ks.test(addata5, &quot;pnorm&quot;) ## ## One-sample Kolmogorov-Smirnov test ## ## data: addata5 ## D = 0.88493, p-value = 0.0000000000000171 ## alternative hypothesis: two-sided # Base R shapiro.test(addata5$Values) ## ## Shapiro-Wilk normality test ## ## data: addata5$Values ## W = 0.87521, p-value = 0.04027 # bruker pakken: nortest cvm.test(addata$Values) ## ## Cramer-von Mises normality test ## ## data: addata$Values ## W = 0.12634, p-value = 0.04326 Tolkning Kolmogorov-Smirnov: Hvis p-verdien er under valgte signifikansnivå (f.eks. 0.05) skal vi anta at datasettet ikke er normalfordelt. Her vil testen peke på at datasettet ikke er normalfordelt. Tolkning av Shapiro-Wilks og Cramer-von Mieses test er lik som for Kolmogorov-Smirnov. Som et siste eksempel på en statistisk test for normalitet kan vi bruke Jarque-Bera test. Denne skiller seg litt ut fra de andre ved at den spesifikt ser på skjevhet og kurtosis i datasettet opp mot hva en normalfordeling vil ha. For å gjøre lykken komplett finnes det versjoner av testen: # Bruker pakken: tibble # bruker pakken: readxl addata6 &lt;- as_tibble(read_excel(&quot;Anderson-Darling_raw.xlsx&quot;)) # Bruker pakken: tseries jarque.bera.test(addata6$Values) ## ## Jarque Bera Test ## ## data: addata6$Values ## X-squared = 2.1953, df = 2, p-value = 0.3337 # Bruker pakken: normtest ajb.norm.test(addata6$Values, nrepl=2000) ## ## Adjusted Jarque-Bera test for normality ## ## data: addata6$Values ## AJB = 3.1014, p-value = 0.131 Tolkningen er lik som før - hvis p-verdien er mindre enn valgte signifikansnivå peker det mot at datasettet ikke er normalfordelt. Her, i motsetning til de øvrige testene, er p-verdien større enn signifikansnivået (0,05) så det peker mot at datasettet er normalfordelt. Dette er altså ikke så enkelt. Det finnes mange statistiske tester, som kan gi motsatte indikasjoner på om et datasett er normalfordelt eller ikke siden de ser på dataene fra ulik vinkel (fokuserer på ulike aspekter ved dataene). Vårt råd blir: Start alltid med Q-Q plott. Velg evt en teststatistikk, men vær klar over at alle teststatistikker bygger på forutsetninger eller tester ulike sider av distribusjonen. Det vi også kan huske på er at i henhold til sentralgrenseteoremet (Central Limit Theorem) vil populasjonens fordeling være av mindre interesse dersom utvalgsstørrelsen er stor nok. Hva er stor nok? De fleste kilder peker mot at over 30 er stort nok. 1.3.1.4 Boxplott # Base R par(mfrow=(c(1,2))) Boxplot(Field_OLS_data$Adverts, id = list(n=Inf), ylab = &quot;&quot;, main = &quot;Adverts&quot;, col = &quot;Blue&quot;) ## [1] 43 87 184 Boxplot(Field_OLS_data$Sales, id = list(n=Inf), ylab = &quot;&quot;, main = &quot;Sales&quot;, col = &quot;Green&quot;) Vi ser at vi får først ut en liste over uteliggerne identifisert ved id/case/observasjonsnummer (43, 87 og 184) for variabelen Adverts. Dette vises også som tre små sirkler i boxplottet. Et boxplott forteller oss mye om dataenes distribusjon. Selve boksen representerer 50 % av observasjonene/casene, det vil si at nedre kant representerer første kvartil (= 25.prosentil) og øvre kant tredje kvartil (= 75.prosentil). Den tykkere horisontale streken i boksen viser medianverdien (= andre kvartil = 50.prosentil) Dersom en observasjon ligger utenfor en terkselverdi (jfr figur under) vises dette med en liten sirkel. Dette defineres som uteliggere. Å identifisere uteliggere kan være viktig for mange statistiske tester. Galarnyk (2018) illustrerer boxplott slik: 1.3.1.5 Scatterplott # Base R scatterField &lt;- with(Field_OLS_data, plot(Sales, Adverts)) Et scatterplott viser oss på en god visuell måte hvordan de to variablene forholder seg til hverandre (vi plotter hver enkelt observasjon gjennom verdiene de har på de to variablene). Mønsteret kan derfor si oss mye om sammenhengen mellom de to. En god måte å fremstille et scatterplott på i R (gjennom pakken car) er denne: # Bruker pakken: car scatterplot(Adverts ~ Sales, data = Field_OLS_data, id = list(n=4)) ## [1] 1 87 169 184 Her kombinerer vi scatterplott og boxplott. Den rette blå linja er en minste kvadratssums regresjonslinje (OLS). Den stiplede blå linja bruker en ikke-parametrisk tilnærming. I tillegg får vi visualisert de fire mest ekstreme tilfellene (lengst vekk fra gjennomsnitt). 1.3.2 Steg 2: Evtentuelt valg av prediktorer ut fra analyse av dataene Dette er ikke relevant i en enkel lineær regresjonsanalyse. Når vi skal gjøre en multippel regresjonsanalyse - altså at vi har to eller flere uavhengige variabler (prediktorer) vil analysen av dataene våre - og i hvilken rekkefølge vi legger de uavhengige variablene inn i regresjonsmodellen (mer om det under eksempelet for multippel regresjon) kunne informeres av analysen vi gjør i forkant. Derfor viser vi dette nå selv om det altså ikke er relevant for enkel regresjonsanalyse. Vi lager en korrelasjonstabell for de tre uavhengige og den avhengige variabelen (Sales, Adverts, Airplay, Image): # Base R cor(Field_OLS_data, method = &quot;pearson&quot;) ## Adverts Sales Airplay Image ## Adverts 1.00000000 0.5784877 0.1018828 0.08075151 ## Sales 0.57848774 1.0000000 0.5989188 0.32611105 ## Airplay 0.10188281 0.5989188 1.0000000 0.18198863 ## Image 0.08075151 0.3261111 0.1819886 1.00000000 I standard multippel regresjonsanalyse legger vi alle de uavhengige variablene inn samtidig. Dersom vi skal gjøre en stegvis regresjonsanalyse vil vi legge de uavhengige variablene inn en og en ut fra statistiske kriterier - som hvor stor korrelasjonen er. Den uavhengige variabelen med størst korrelasjon legges inn først og så videre. For det eksempelet vi har ser vi at Sales korrelerer høyest med Airplay, og nesten like mye med Adverts. Korrelasjonen med Image er noe lavere. 1.3.3 Steg 3: Lage modell (og kjøre regresjonsanalysen) Vi ønsker å se om Adverts kan predikere Sales. Grafisk kan vi vise dette: # Base R lm(formula = Sales ~ Adverts, data = Field_OLS_data) ## ## Call: ## lm(formula = Sales ~ Adverts, data = Field_OLS_data) ## ## Coefficients: ## (Intercept) Adverts ## 134.13994 0.09612 # Base R FieldOLS_reg &lt;- lm(formula = Sales ~ Adverts, data = Field_OLS_data) 1.3.4 Steg 4: Analyse av resultatene (diagnostikk) # Base R summary(FieldOLS_reg) ## ## Call: ## lm(formula = Sales ~ Adverts, data = Field_OLS_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.949 -43.796 -0.393 37.040 211.866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 134.139938 7.536575 17.799 &lt;0.0000000000000002 *** ## Adverts 0.096124 0.009632 9.979 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 65.99 on 198 degrees of freedom ## Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 ## F-statistic: 99.59 on 1 and 198 DF, p-value: &lt; 0.00000000000000022 1.3.4.1 Hvor mye forklarer modellen vår? Det første vi kan se på er \\(R^{2}\\) som forteller oss hvor stor del av den totale variansen modellen forklarer. I dette tilfellet er \\(R^{2} = 0.3346\\). Det innebærer at modellen vår kan forklare 33.46 % av den totale variansen. Det betyr at reklame forklarer («accounts for») 33.5 % av variansen i salget. Det er med andre ord mange andre faktorer som kan forklare hvorfor noen plater selger bedre enn andre, men reklame kan forklare drøye 33 % av den totale variansen. Dette kan vi også se er statistisk signifikant p &lt; .001. Endel programmer vil også gi en R verdi. Siden vi kun har en uavhengig variabel (en prediktor) vil verdien R utgjøre den bivariate korrelasjonen (korrelasjonskoeffisienten mellom de to variablene - vi ser at dette er samme verdi som i tabellen over korrelasjonskoeffisienter lenger opp). Adjusted \\(R^{2}\\) er en modifisert versjon av \\(R^{2}\\) der det legges inn en korreksjon for antall prediktorer i modellen. Motivasjonen for dette er at det å legge til flere prodeiktorer alltid vil øke \\(R^{2}\\) verdien (Navarro &amp; Foxcroft, 2019). Navarro &amp; Foxcroft (2019) påpeker imidlertid at man ikke kan tolke adjusted \\(R^{2}\\) like rett fram som \\(R^{2}\\), og anbefaler at man bruker \\(R^{2}\\). Det vi også kan si er at dersom verdiene på henholdsvis \\(R^{2}\\) og adjusted \\(R^{2}\\) er nærme hverandre (eller like) indikerer dette en god kryssvaliditet i modellen, noe som kan gjøre oss sikrere i generalisering av funnene våre. 1.3.4.2 Modellens koeffisienter og regresjonslikning Koeffisientene vil fortelle oss mer i multippel regresjonsanalyse, men gir oss noen interessante opplysninger også her. I introduksjonen snakket vi om punktet der regresjonslinja skjærer y-aksen (konstanten). Fra analysen ser vi at (Intercept) = 134.14 og Adverts = 0.096. 134.14 er punktet på y-aksen regresjonslinja begynner (der x = 0). Altså, ettersom x-aksen angir verdier for hva vi bruker på reklame er Intercept estimatet antallet plater vi kan forvente å selge dersom vi bruker 0 kroner på reklame. Estimatet på «Adverts» på 0,096 er stigningstallet for regresjonslinja  hvis prediktoren (reklame) stiger med 1 enhet stiger salget med 0,096 plater. Vi kan da lage følgende likning: \\[ Sales=134,14\\:+\\:0,096\\left(Adverts\\right) \\] 1.3.4.3 Hvor god er modellen vår (goodness of fit)? Vi ønsker å ha en formening om hvor god modellen er («goodness of fit»). Altså, er regresjonsmodellen vår bedre enn en modell der vi ikke vet noe om forholdet mellom reklame og platesalg? Vi kan bruke gjennomsnitt av salgstallene som en modell for ingen sammenheng mellom reklame og platesalg, og deretter sammenlikne regresjonsmodellen med gjennomsnittsmodellen. Sammenlikningen mellom modellene skjer gjennom å se på forskjellene mellom de observerte målingene (salgstall) og verdier predikert at de to ulike modellene. Dersom regresjonsmodellen signifikant predikerer bedre er det en bedre modell enn alternativet. Analysen vår gitt oss F verdien 99.59 med p &lt; .001. Vi ser at verdien er statistisk signifikant. F verdien er et mål på forbedring i prediksjonen sett opp mot unøyaktigheter i modellen (alle modeller er unøyaktige (eller feil)). Vi kan sjekke F-verdien opp mot antall frihetsgrader (df) gjennom tabeller som ofte finnes i statistikkbøker, eller bruke onlineressurser som her Vi ser av resultatene fra analysen at antall df i teller er 1 og antall df i nevner er 198. Hvis vi leser av tabellen er vi at for df 1/df 200 er kritisk verdi 3,89 for  = 0,05 og 11,15 for  = 0,001. Vår F er med andre ord langt over kritisk verdi. Vi kan derfor si at vår regresjonsmodell gir en signifikant bedre prediksjon av platesalg enn alternativet. Reklame er med andre ord en god prediktor for platesalg. Helt nøyaktig kan vi regne ut kritisk verdi (som vi har gjort i R) for df 1 og df 198: # Base R qf(p=.05, df1=1, df2=198, lower.tail=FALSE) ## [1] 3.888853 Ut fra dette kan vi si det er svært usannsynlig at forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si at dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten i modellen så vil F &gt; 1. Til slutt vil vi se på konfidensintervallet: # Bruker pakken: car Confint(FieldOLS_reg) ## Estimate 2.5 % 97.5 % ## (Intercept) 134.13993781 119.27768082 149.0021948 ## Adverts 0.09612449 0.07712929 0.1151197 1.3.5 Steg 5: Sjekk av forutsetningene Dette er et tema som behandles og framstilles på noe ulike måter i litteraturen. Det som er klart er at brudd på forutsetningene kan gjøre modellen vår mer usikker opp til et punkt hvor regresjonsanalyse ikke bør gjennomføres. Noen av forutsetningene er empirisk testbare (vi kan få ut en eller annen form for analyse av et statistikkprogram som SPSS, Stata, R og så videre) mens noen er ikke empirisk testbare (det vil si vi må bruke egen vurdering). Vi skal i dette delkapittelet gå gjennom forutsetningene for lineær regresjon. Selv om noen ikke er aktuelle for enkel regresjon tar vi med alle forutsetningene her for oversiktens skyld. Ved regresjonsanalyse gjør vi en rekke sjekker av datamaterialet vi har for å avgjøre om regresjonsanalyse er en egnet teknikk og hvorvidt vi mener vi kan generalisere funnene. Dersom forutsetningene brytes gjør det at vi kan sette spørsmålstegn ved hvor nærme regresjonskoeffisienten er populasjonskoeffisienten  eller med andre ord: Hvis regresjonskoeffisienten er helt forventningsrett («unbiased», dvs 0) så vil regresjonskoeffisienten være lik populasjonskoeffisienten («estimatet er lik virkeligheten»). Nå vil det i praksis aldri være tilfelle, men ved å sette visse forutsetninger kan vi fastslå om våre data egner seg for regresjonsanalyse og hvor sikre vi føler oss for at funnene kan generaliseres. Det er verdt å merke seg hva Field et. al (2012, p. s.298) skriver: Its worth remembering that you can have a perfectly good model for your data (no outliers, influential cases, etc.) and you can use that model to draw conclusions about your sample, even if your assumptions are violated. However, its much more interesting to generalize your regression model and this is where assumptions become important. If they have been violated then you cannot generalize your findings beyond your sample. Med andre ord: Vi kan ha brudd på forutsetningene og likevel si noe meningsfullt om vårt utvalg/våre data, men resultatene våre blir mer usikre og vi skal være veldig forsiktige med å kreve generaliserbarhet dersom vi har brudd på forutsetningene. Så alt håp er ikke ute med brudd på forutsetningene, men vi skal behandle konklusjonene våre deretter. Regresjonsforutsetninger behandles ulikt av ulke kilder, og får ulik plass i diskusjoner om regresjon. Vi har undersøkt en rekke kilder for å framstille dette her (blant annet Green (1991), Berry (1993), Miles &amp; Shevlin (2001), Hinkle et al. (2003), Tabachnik &amp; Fidell (2007), Eikemo &amp; Clausen (2007), Hair Jr. et al. (2010), Lomax &amp; Hahs-Vaughn (2012)). 1.3.5.1 Kausalitet Forutsetningen om kausalitet hviler i fagunnskap og teoretiske vurderinger. Det sier seg for så vidt selv at vi ikke er interesserte i å ha med irrelevante variabler i modellen. Med irrelevant her menes variabler som korrelerer med den avhengige variabelen, men hvor korrelasjonen er ikke har noe med årsakssammenheng å gjøre (sammenhenger medfører ikke i seg selv kausalitet som vi har vært inne på i en tidligere modul). Kausalitet er dermed en forutsetning. Den uavhengige variabelen må variere korrelert med de avhengige, det er en kausal sammenheng (hvis ikke det er kausalitet har den/de uavhengige variablene ingen effekt på den avhengige  det kan like gjerne være motsatt). Når vi velger en avhengig variabel og en eller flere uavhengige variabler har vi også gjort en antakelse om kausalitet og retning på kausaliteten, og forutsatt at denne er tilstede - basert på teoretisk kunnskap om det vi undersøker. Men det er viktig å understreke at verken korrelasjon eller regresjon indikerer kausalitet. 1.3.5.2 Variablene er uten målefeil Vi må forutsette at vi ikke har systematiske målefeil i våre data. Thoresen (2003) viser for eksempel til en studie av MacMahon et al. (1990) der de fant en 60% sterkere sammneheng mellom blodtrykk og hjerte-karsykdommer i en stor metastudie når de korrigerte for skjevhet i estimatene i de tidligere studiene (som var inkludert i metastudien). Vi skal også være oppmerksom på utfordringer dersom feil i den ene variabelen korrelerer med feil i en annen variabel. Dersom målt eksponering og målt helseutfall er rammet av avhengige feil, blir resultatet oftest en falskt forhøyet sammenheng mellom de to. Slik resultatskjevhet er sannsynligvis ikke uvanlig i tverrsnittsstudier, hvor data om eksponering og utfall skaffes til veie gjennom spørreskjema (Kristensen, 2005). 1.3.5.3 Relevante og irrelevante variabler Alle relevante uavhengige variabler må være inkludert i modellen, og alle irrelevante uvhengige variabler er fjernet/er ikke med i modellen. Man kan si at en hovedgrunn til at vi veldig ofte kjører mulitippel regresjonsanalyse i stedet for bivariat regresjonsanalyse er for å unngå at vi ikke inkluderer relevante variabler (såkalt omitted variable bias) (Thrane, 2019). Imidlertid, som Thrane (2019) påpeker, er dette i praksis umulig, så det vi tilstrebe er å inkludere de mest relevante variablene. Dette faller igjen tilbake på teoretiske betraktninger og faglig kjennskap til området man holder på med. Du skal i hvert fall kunne begrunne valget av hvilke uavhengige variabler som er inkludert og hvilke som kanskje kunne tenkes å være inkludert, men som du har valgt å ikke inkludere. Teoretisk sett skal vi også forsikre oss om at ikke-relevante variabler ikke er inkludert i modellen. Igjen er dette delvis umulig og delvis forvirrende/unøyaktig. Det er delvis umulig fordi vi vanskelig kan vite eksakt hvilke potensielle variabler som er relevante og ikke. Det er delvis forvirrende/unøyaktig fordi det kan være viktig å identifisere variabler som ikke har noen effekt - dette kan være viktig i policyrevisjon/-utforming (jfr Thrane (2019)). 1.3.5.4 Forholdstall mellom caser/observasjoner og uavhengige variabler Forholdstallet mellom respondenter/caser/observasjoner og uavhengig variabler er av stor betydning dersom man skal gjennomføre en multippel regresjon. Dette gjelder spesielt ved skjevdistribusjon av den avhengige variabelen, effektstørrelsen er forventet liten eller man kan mistenke vesentlige målefeil (Tabachnik &amp; Fidell, 2007). Det finnes ulike anbefalte normer for vurdering av forholdstallet (vi tar her utgangspunkt i standard multippel regresjonsanalyse - stegvis multippel regresjonsanalyse kan gi andre vurderinger rundt forholdstallet). Under viser vi noen eksempler på hvordan man kan vurdere dette: .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-04bf0bac{}.cl-04b611f0{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-04b611f1{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-04b638d8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-04b67c30{width:324pt;height:21.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-04b67c31{width:180pt;height:21.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-04b67c32{width:324pt;height:21.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-04b67c33{width:180pt;height:21.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-04b67c34{width:324pt;height:21.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-04b67c35{width:180pt;height:21.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-04b67c36{width:324pt;background-color:rgba(173, 223, 173, 1.00);vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-04b67c37{width:180pt;background-color:rgba(173, 223, 173, 1.00);vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 1.1: Forholdstall KildeAntallMarks (1966, i Harris (2013)Minimum 200 uansettSchmidt (1971)15-1 til 25-1Nunally (1978)2-3 IV = minst 100, 9-10 IV = 300-400Stevens (1996)15 pr IVGreen (1991)N=50+8m (m=antall uavhengige variabler) ved «medium-sized relationship between the IVs and the DV, a=.05 and ß=.20Miles &amp; Shevlin (2001)Som Green (2001). Utvalgsstørrelse avhenger av størrelse på effekt og statistisk styrke (se Cohen, 1988 for effektstørrelser). Stor effekt: 80 respondenter er alltid nok for opptil 20 IVs. Middels effekt: 200 respondenter vil alltid være nok for opptil 20 IVs, 100 er nok for opptil 6 eller færre IVs. Lav effekt: Minst 600. Det er også verdt å merke seg at det ikke er ønskelig med for mange respondenter, da et svært stort antall respondenter vil gi statistisk signifikans for nesten enhver multippel korrelasjon  For both statistical and practical reasons, then, one wants to measure the smallest number of cases that has a decent chance of revealing a relationship of a specified size (Tabachnik &amp; Fidell, 2007, p. s.123). Dette har med andre ord mye å si for hvordan man planlegger en studie. Miles &amp; Shevlin (2001), som angitt i siste rad i tabellen over, ser på sammenhengen mellom effektstørrelse, statistisk styrke og utvalgsstørrelse. Field (2009, p. s.223, figur 7.10) har modifisert grafisk denne sammenhengen: 1.3.5.5 De uavhengige variablene er additiv for den avhengige variabelen Denne forutsetningen gjelder om man har minst to uavhengige variabler - altså i multippel regresjon. Med dette menes at vi må forvente at variasjonen i den avhengige variabelen er en funksjon av sum av endringer i de uavhengige variablene. Vi kan også uttrykke dette som at effekten av den uavhengige variabelen \\(x_1\\) på den avhengige variabelen \\(y\\) er uavhengig av eventuelle andre uavhengige variablers effekt på \\(y\\). Forutsetningen om additivitet betyr altså at det ikke er interaksjon mellom to eller flere uavhengige variabler. Med andre ord, hvis effekten av \\(x_1\\) på \\(y\\) er avhengig av hvordan \\(x_2...x_n\\) påvirker \\(y\\) brytes forutsetningen om additivitet. I praksis er det imidlertid ikke uvanlig at denne forutsetningen brytes i en eller annen grad. La oss vise dette med et eksempel fra Thrane (2019) (se figur 6.1). Dersom vi har data for kvinners og menns inntekt relatert til antall års utdannelse vil vi kunne se i en regresjonsanalyse at antall års utdanning predikerer inntekt. Forutsetningen om additivitet sier da at antall års utdanning har lik effekt på inntekt for menn og kvinner. Slik er det imidlertid ikke. Generisk kan det framstilles som i grafen under: Vi ser at linjene har ulikt stigningstall. Kvinner starter under menn, men har et høyere stigningstall. Det vil si at kvinner har større effekt av et (ekstra) års utdanning enn menn. Da er forutsetningen om additivitet brutt. Som Thrane (2019) påpeker: Additivity thus means parallel regression lines. Vi kan altså sjekke dette ved å kjøre to regresjonsanalyser. En annen måte, som vi ikke går inn på her, er å lage en interaksjonsvariabel som vi dernest bruker i modellen. En interaksjonsvariabel er et produkt av (minst) to uavhengige variabler. La oss se på et annet eksempel for å vise interaksjonsvariabel og -effekt basert på et eksempel fra Thomas (2017). # Bruker pakken: ISLR data(Carseats) head(Carseats) ## Sales CompPrice Income Advertising Population Price ShelveLoc Age Education ## 1 9.50 138 73 11 276 120 Bad 42 17 ## 2 11.22 111 48 16 260 83 Good 65 10 ## 3 10.06 113 35 10 269 80 Medium 59 12 ## 4 7.40 117 100 4 466 97 Medium 55 14 ## 5 4.15 141 64 3 340 128 Bad 38 13 ## 6 10.81 124 113 13 501 72 Bad 78 16 ## Urban US ## 1 Yes Yes ## 2 Yes Yes ## 3 Yes Yes ## 4 Yes Yes ## 5 Yes No ## 6 No Yes Datasettet inneholder ni variabler i tillegg til det vi vil bruke som avhengig variabel: Sales. Vi vil altså se om vi kan bruke de ni variablene til å predikere salg. # Base R saleslm &lt;- lm(Sales~. ,Carseats) summary(saleslm) ## ## Call: ## lm(formula = Sales ~ ., data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8692 -0.6908 0.0211 0.6636 3.4115 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.6606231 0.6034487 9.380 &lt; 0.0000000000000002 *** ## CompPrice 0.0928153 0.0041477 22.378 &lt; 0.0000000000000002 *** ## Income 0.0158028 0.0018451 8.565 0.000000000000000258 *** ## Advertising 0.1230951 0.0111237 11.066 &lt; 0.0000000000000002 *** ## Population 0.0002079 0.0003705 0.561 0.575 ## Price -0.0953579 0.0026711 -35.700 &lt; 0.0000000000000002 *** ## ShelveLocGood 4.8501827 0.1531100 31.678 &lt; 0.0000000000000002 *** ## ShelveLocMedium 1.9567148 0.1261056 15.516 &lt; 0.0000000000000002 *** ## Age -0.0460452 0.0031817 -14.472 &lt; 0.0000000000000002 *** ## Education -0.0211018 0.0197205 -1.070 0.285 ## UrbanYes 0.1228864 0.1129761 1.088 0.277 ## USYes -0.1840928 0.1498423 -1.229 0.220 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.019 on 388 degrees of freedom ## Multiple R-squared: 0.8734, Adjusted R-squared: 0.8698 ## F-statistic: 243.4 on 11 and 388 DF, p-value: &lt; 0.00000000000000022 Denne modellen kan altså forklare 87.3 % av variansen i Sales. Vi kan imidlertid mistenke at det kan være en interaksjon mellom variablene Income og Population - jo større befolkning, jo større inntekt tilgjengelig. # Base R saleslm1&lt;-lm(Sales~.+Population*Income, Carseats) # Bruker pakken: car compareCoefs(saleslm, saleslm1) ## Calls: ## 1: lm(formula = Sales ~ ., data = Carseats) ## 2: lm(formula = Sales ~ . + Population * Income, data = Carseats) ## ## Model 1 Model 2 ## (Intercept) 5.661 6.195 ## SE 0.603 0.644 ## ## CompPrice 0.09282 0.09262 ## SE 0.00415 0.00413 ## ## Income 0.01580 0.00797 ## SE 0.00185 0.00387 ## ## Advertising 0.1231 0.1237 ## SE 0.0111 0.0111 ## ## Population 0.000208 -0.001811 ## SE 0.000370 0.000952 ## ## Price -0.09536 -0.09511 ## SE 0.00267 0.00266 ## ## ShelveLocGood 4.850 4.859 ## SE 0.153 0.152 ## ## ShelveLocMedium 1.957 1.964 ## SE 0.126 0.125 ## ## Age -0.04605 -0.04566 ## SE 0.00318 0.00317 ## ## Education -0.0211 -0.0216 ## SE 0.0197 0.0196 ## ## UrbanYes 0.123 0.133 ## SE 0.113 0.112 ## ## USYes -0.184 -0.216 ## SE 0.150 0.150 ## ## Income:Population 0.0000288 ## SE 0.0000125 ## Forskjellen mellom modellene ligger altså i den nederste koeffisienten (Population * Income) som da er den kombinerte og samtidige effekten av de to variablene. Vi ser at effekten er veldig liten, men grafisk kan vi også se interaksjonseffekten: # Bruker pakken: ggplot2 ggplot(data=Carseats, aes(x=Income, y=Sales, group=1)) +geom_smooth(method=lm,se=F)+ geom_smooth(aes(Population,Sales), method=lm, se=F,color=&quot;black&quot;)+xlab(&quot;Income and Population&quot;)+labs( title=&quot;Inntekt i blått - Befolkning i svart&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; Vi ser en klar interaksjon ved at linjene krysser (jfr. Thranes merknad om at forutsetningen om additivitet gir parallelle linjer). I R har vi et hjelpemiddel i pakken interactions: # Bruker pakken: interactions interact_plot(saleslm1, pred = Population, modx = Income) Tolkningen av plottet fra pakken interactions er den samme: Parallelle linjer indikerer fravær av interaksjoneseffekt, mens ikke-parallelle linjer indikerer tilstedeværelse av interaksjoneseffekt. # Bruker pakken: jtools summ(saleslm1) Observations 400 Dependent variable Sales Type OLS linear regression F(12,387) 225.99 R² 0.88 Adj. R² 0.87 Est. S.E. t val. p (Intercept) 6.20 0.64 9.63 0.00 CompPrice 0.09 0.00 22.45 0.00 Income 0.01 0.00 2.06 0.04 Advertising 0.12 0.01 11.18 0.00 Population -0.00 0.00 -1.90 0.06 Price -0.10 0.00 -35.77 0.00 ShelveLocGood 4.86 0.15 31.90 0.00 ShelveLocMedium 1.96 0.13 15.65 0.00 Age -0.05 0.00 -14.41 0.00 Education -0.02 0.02 -1.10 0.27 UrbanYes 0.13 0.11 1.18 0.24 USYes -0.22 0.15 -1.44 0.15 Income:Population 0.00 0.00 2.30 0.02 Standard errors: OLS Vi ser i tillegg at interaksjonseffekten (Income:Population) er statistisk signifikant. 1.3.5.6 Linearitet Vi tar, som navnet linær regresjonsanalyse ganske klart indikerer, utgangspunkt i at forholdet mellom den/de uavhengige varaibelen(e) og den avhengige variabelen kan beskrives som en lineær funksjon (se for eksempel Ringdal (2007)). Sammenhengen mellom variablene må ikke være perfekt lineær, men må i hvert fall være tilnærmet lineær. Vi har allerede sett en grafisk framstilling under punktet analyse av dataene som lar oss visuelt vurdere denne forutsetningen: # Bruker pakken: car scatterplot(Adverts ~ Sales, data = Field_OLS_data) Den stiplede linjen som buer ca midt i det skraverte området gjør ingen forutsetninger, men plotter bare dataene (ofte kalt scatterplot smoother). Vi kan vurdere om denne er nærme eller langt fra en rett linje. I grafen over ligger det både en stiplet buet linje og en rett linje (regresjonslinje). I vårt tilfelle vil vi raskt konkludere med at forholdet mellom de to variablene er tilnærmet lineært. 1.3.5.7 Residualene skal være normalfordelte Forutsetningen er at residualene i modellen er tilfeldige, normalfordelte variabler med gjennomsnittsverdi 0 (Field et al., 2012), hvilket innebærer at forskjellen mellom modellen og de observerte dataene er 0 eller nær 0 i de fleste tilfeller (og at ulikhet skyldes tilfeldigheter). Poenget her er at dersom regresjonsmodellen er god skal det være omtrent like stor sannsynlighet for at den underestimerer som at den overestimerer. Er den det vil fordelingen være tilnærmet symmetrisk (perfekt normalfordeling vil i praksis ikke inntreffe). En tilnærming til å se på denne forutsetningen er å se på et histogram over residualene. Når vi lagrer regresjonsmodellen som et objekt (i R) kan vi se hvilke parametere som lagres i modellen: # Base R names(FieldOLS_reg) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; Residualene lagres altså i modellen og vi kan plotte ut disse: # Base R hist(FieldOLS_reg$residuals) Vi kan også se på et Q-Q plott og hente ut testverdier for ulike normalitetstester: # Bruker pakken: olsrr ols_plot_resid_qq(FieldOLS_reg) ols_test_normality(FieldOLS_reg) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9899 0.1757 ## Kolmogorov-Smirnov 0.0634 0.3970 ## Cramer-von Mises 16.055 0.0000 ## Anderson-Darling 0.4298 0.3057 ## ----------------------------------------------- Q-Q plottet viser noe avvik (jfr. ) En hendig graf er også et plott av residualene på y-aksen og fitted values på x-aksen: # Bruker pakken: olsrr ols_plot_resid_fit(FieldOLS_reg) I forhold til forutsetningen om normalfordelte residualer skal være spredd tilfeldig rundt 0. Sett under ett kan det i dette tilfellet se ut som at residualene er tilnærmet (nok) normalfordeling. 1.3.5.8 Fravær av multikolinearitet Multikolinearitet innebærer at det er korrelasjon mellom de uavhengige varaiblene (i multippel regresjon). Det kan ofte forekomme at vi har en viss korrelasjon, men hvis korrelasjonen blir stor blir det vanskelig å skille mellom effekten den enkelte uavhengige variabel har på den avhengige variabelen. Multikolinearitet inntreffer dersom vi har sterk korrelasjon mellom to eller flere av de uavhengige variablene. Multikollinearitet handler altså om det innbyrdes forholdet mellom de uavhengige variablene. Hvis disse er for høyt korrelerte har vi multikollinearitet, altså at det kan være (tilnærmet) perfekt linearitet mellom to uavhengige variabler (Berry, 1993) hvilket innebærer muligheten for at ingen av korrelasjonskoeffisientene er signifikante pga størrelsen på standardfeil. En perfekt kolinearitet har koeffisienten 1. Berry (1993) angir at angir at \\(r=.9\\) gir en dobling av standardfeil i regresjonskoeffisienten, og at selv \\(0.5\\) og \\(0.6\\) kan gi utfordringer for tolkningen av regresjonskoeffisientene. Field (2009) anser \\(0.8\\) til \\(0.9\\) som høy korrelasjon. Samtidig sier J. Pallant (2010) at det (naturligvis) bør være en viss korrelasjon mellom de uavhengige og den avhengige variabelen, og hevder de bør være på over \\(0.3\\), men samtidig at bivariat korrelasjon mellom de uavhengige variablene ikke bør være over \\(0.7\\). Vi kan derfor sjekke for multikolinearitet gjennom å se på en korrelasjonsmatrisen: # Base R FieldKorr &lt;- cor(Field_OLS_data, method = &quot;pearson&quot;) round(FieldKorr, 2) ## Adverts Sales Airplay Image ## Adverts 1.00 0.58 0.10 0.08 ## Sales 0.58 1.00 0.60 0.33 ## Airplay 0.10 0.60 1.00 0.18 ## Image 0.08 0.33 0.18 1.00 Hvis vi følger J. Pallant (2010) ser vi først at alle de tre uavhengige variablene korrelerer mellom \\(0.33\\) og \\(0.60\\) med den avhengige. De bivariate korrelasjonene mellom de uavhengige variablene erpå hhv. \\(0.08\\), \\(0.10\\) og \\(0.18\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet. Dette er aktuelt i multippel regresjonsanalyse, så for å vise dette lager vi en slik for datasettet til Field der vi inkluderer tre uavhengige variabler: # Base R FieldOLS_mult_reg &lt;- lm(Sales ~ ., data = Field_OLS_data) brief(FieldOLS_mult_reg) ## (Intercept) Adverts Airplay Image ## Estimate -26.6 0.08488 3.367 11.09 ## Std. Error 17.4 0.00692 0.278 2.44 ## ## Residual SD = 47.1 on 196 df, R-squared = 0.665 Vi kan også se på Variance Inflation Factor (VIF), som måler hvor mye variansen til en estimert regresjonskoeffisient øker pga. multikollinearitet. # Bruker pakken: olsrr ols_vif_tol(FieldOLS_mult_reg) ## Variables Tolerance VIF ## 1 Adverts 0.9856172 1.014593 ## 2 Airplay 0.9592287 1.042504 ## 3 Image 0.9629695 1.038455 Myers (1990) og Hair Jr. et al. (2010) opererer med en akseptabel grense for VIF på 10.0 (toleranse på .1). Her er rådene (også) litt ulike. En VIF-verdi på 10 for VIF anses f.eks. i pakken olsrr i R (som vi har brukt over) som et tegn på alvorlig multikolinearitet (jfr. Belsley et al., 1980). Der settes VIF på 4 som en grense der man bør se nærmere på om multikolinearitet kan være et problem. Bowerman &amp; OConnell (1990) tilføyer at snittet av VIF for de uavhengige variablene ikke bør være vesentlig over 1. 1.3.5.9 Fravær av heteroskedasisitet Variansen til residualene for de uavhengige variablene skal være lik (Miles &amp; Shevlin, 2001). Heteroskedasitsitet innebærer at residualene ikke har konstant varianse (motsatt: vi ønsker lik varians i residualene over alle x-verdier og kaller dette homoskedastisitet). Hvis vi har homoskedastisitet predikerer modellen vår likt på alle predikerte verdier av Y, noe vi ønsker. Variansen til residualen kan altså ikke avhenge av de uavhengige variablene, men være lik på alle nivåer av verdier for prediktorene. Dersom vi har heteroskedastisitet vil spredningen rundt regresjonslinja variere med X. Forutsetningen om homoskedastisitet er godt illustrert av Miles &amp; Shevlin (2001), s.100-101, fig. 4.22, 4.23 og 4.24: Figuren til venstre viser et scatterplot for residualer. I midten har vi samme fordeling av residualer med et antall normalfordelingskurver. Til høyre ser vi et alternativt scatterplot for residualer som bryter med forutsetningen om homoskedastisitet. Løvås (2013) illustrerer det samme slik i eksempelet om motorstørrelse og drivstofforbruk: Variasjonen i residualene er like stor uansett verdien av x. Den såkalte Whites test vil også kunne være et godt hjelpemiddel: # Base R FieldOLS_mult2 &lt;- lm(Sales ~ Adverts + Airplay + Image, data = Field_OLS_data) # Bruker pakken: lmtest bptest(FieldOLS_mult2, ~ Adverts*Airplay*Image + I(Adverts^2) + I(Airplay^2)+ I(Image^2), data = Field_OLS_data) ## ## studentized Breusch-Pagan test ## ## data: FieldOLS_mult2 ## BP = 10.44, df = 10, p-value = 0.4027 Her ser vi at p-verdien er \\(0.4027\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med at vi har heteroskedasisitet i regresjonsmodellen. 1.3.5.10 Fravær av autokorrelasjon Dette er typisk et problem i tidsserieanalyser eller geografiske analyser (Eikemo &amp; Clausen, 2007, p. a.124), der verdien på variabel X for enhet N i stor grad er bestemt av verdien på variabel X for enhet N-1. Et annet kjent eksempel er fra aksjemarkedet: Hvis en aksje stiger i dag er det mer sannsynlig at den stiger i morgen. Verdien av aksjen i morgen avhenger (delvis) av veriden i dag. Verdiene i dataene autokorrelerer. Grad av autokorrelasjon er således et mål på forholdet mellom en variabels verdi på tidspunkt X og verdien på tidspunkt før X. To tilfeldige observasjoner bør ikke ha korrelasjon i residualen. Dette kan testes gjennom en Durbin-Watson test (Durbin &amp; Watson, 1951). Durbin-Watson testen er en test på autokorrelasjon i residualene, og vil ha en verdi på mellom 0 og 4. Verdien 2 indikerer ingen autokorrelasjon. Verdier mellom 0 og 2 indikerer en positiv autokorrelasjon, mens verdier mellom 2 og 4 indikerer en negativ autokorrelasjon. En konservativ tommelfingerregel sier at verdier under 1 og over 3 er bekymringsfullt (Field et al., 2012). # Bruker pakken: car durbinWatsonTest(FieldOLS_reg) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.04394305 2.032324 0.762 ## Alternative hypothesis: rho != 0 Her viser verdien \\(2.03\\) noe som ikke gir grunn til bekymring. 1.3.5.11 Fravær av innflytelsesrike observasjoner/caser Vi så under punktet Analyse av dataene (se for eksempel Boxplottet av Adverts) at vi har noen observasjoner i modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene: # Bruker pakken: olsrr car::qqPlot(FieldOLS_reg, id.method=&quot;identify&quot;, main=&quot;Q-Q Plott&quot;) ## [1] 1 169 Vi ser at residualene for 1 og 169 identifiseres som statistisk signifikante uteliggere 1.3.5.11.1 Hampel filter Hampel filter innebærer at man ser alle observasjoner som ligger utenfor intervallet \\(median +/- 3 median\\ absolute\\ deviation\\ (MAS)\\). #Base R nedregrense &lt;- median(Field_OLS_data$Adverts) - 3*(mad(Field_OLS_data$Adverts, constant = 1)) nedregrense ## [1] -457.7405 ovregrense &lt;- median(Field_OLS_data$Adverts) + 3*(mad(Field_OLS_data$Adverts, constant = 1)) ovregrense ## [1] 1521.572 uteligger_ind &lt;- which(Field_OLS_data$Adverts &lt; nedregrense |Field_OLS_data$Adverts &gt; ovregrense) uteligger_ind ## [1] 11 23 28 43 55 87 88 93 126 175 184 1.3.5.11.2 Grubbs test Grubbs test ser om den høyeste verdien i variabelen bør regnes som en uteligger (hvis den høyeste ikke er det vil ingen andre heller være det). # Bruker pakken: outliers grubbstest &lt;- grubbs.test(Field_OLS_data$Adverts) grubbstest ## ## Grubbs test for one outlier ## ## data: Field_OLS_data$Adverts ## G = 3.41281, U = 0.94118, p-value = 0.05396 ## alternative hypothesis: highest value 2271.86 is an outlier 1.3.5.11.3 Rosners test Her angir vi det antallet vi tror er uteliggere, f.eks. fra et box plott. # Bruker pakken: EnvStats rosnerstest &lt;- rosnerTest(Field_OLS_data$Adverts, k = 3 ) rosnerstest$all.stats ## i Mean.i SD.i Value Obs.Num R.i+1 lambda.i+1 Outlier ## 1 0 614.4123 485.6552 2271.860 184 3.412808 3.605525 FALSE ## 2 1 606.0834 472.3432 2000.000 43 2.951068 3.604019 FALSE ## 3 2 599.0434 462.9555 1985.119 87 2.993971 3.602505 FALSE 1.3.5.12 Influential cases Vi kan også kjøre analyser som identifiserer betydning/innvirkning (influential cases): # Bruker pakken: car influenceIndexPlot(FieldOLS_reg, vars = &quot;hat&quot;, id = list(n=3)) Det som i grafen over kalles hat values er et vanlig mål for å finne observasjoner/caser som er relativt langt fra senter av prediksjonsrommet og som derfor potensielt har stor innflytelse på OLS-regresjonskoeffisientene (leverage) (Fox &amp; Weisberg, 2019). Huber (1981) anbefaler følgende grenseverdier: verdier under \\(0.2\\) er ønskelig, verdier over \\(0.5\\) uønskede, og verdier mellom \\(0.2\\) og \\(0.5\\) problematiske. Hat values er altså et mål på potensiell innflytelse. Neste mål - DfBetas - er et mål på observasjonens effekt på regresjonskoeffisinenten for hver variabel med og uten den innflytelsesrike observasjonen - eller med andre ord: observasjonenes innflytelse på variablene. Belsley et al. (1980) anbefaler 2 som cut-off verdi for å indikere innflytelsesrike observasjoner. # Bruker pakken: olsrr ols_plot_dfbetas(FieldOLS_reg, print_plot = TRUE) Pakken legger automatisk inn cutoff-verdi (i dette tilfellet 0.14). Formelen for utregning av dfbeta cutoff-verdi er \\(\\frac{2}{\\sqrt{n}}\\), der n=antall observasjoner. Vi kan også se på dffit (Welsch &amp; Kuh, 1977). # Bruker pakken: olsrr ols_plot_dffits(FieldOLS_reg) Cutoff-verdi i dette tilfellet er 0.2. Formel for utregning er \\(2*\\frac{\\sqrt{(k+1)}}{(n-k-1)}}\\), der k = antall prediktorer og n = antall observasjoner. Det siste målet vi ønsker å se på (og trolig den mest brukte) er Cooks distance, som gir et mål på observasjonens totale innflytelse på regresjonsmodellen. # Bruker pakken: car influenceIndexPlot(FieldOLS_reg, vars = &quot;Cook&quot;, id = list(n=3)) I grafen over vises Cooks distance - et mål på vektet kvadratsum for forskjellene mellom de individuelle elementene til koeffisienten. Sagt på en annen måte: Vi bruker Cooks distance til å se hvilke observasjoner/caser som kan påvirke modellen vår uforholdsmessig mye (totalt sett). Dersom vi har mange caser med høy verdi på Cooks distance kan det være en indikasjon på at lineær regresjon kanskje ikke er en egnet analyse for det foreliggende datasettet. Så hva er høy verdi på Cooks distance? Kilder som Cook &amp; Weisberg (1982) og Tabachnik &amp; Fidell (2007) angir at verdier over 1 er bekymringsfullt. Andre, som Fox (2020), advarer mot en ren numerisk vurdering (og fremhever viktigheten av både grafisk presentasjon og vurdering av hvert enkelt tilfelle). En tilnærming som er anbefalt (se f.eks. Zach (2019)) er å bruker forholdstallet \\(4/N\\) - i vårt tilfelle \\(4/200=0.02\\). La oss hente opp Cooks distance for de største verdiene for de enkelte observasjoner: # Base R mineCDverdier &lt;- cooks.distance(FieldOLS_reg) mineCDverdier &lt;- round(mineCDverdier, 3) head(sort(mineCDverdier, decreasing = TRUE), n = 10) ## 1 169 42 10 55 125 3 148 86 72 ## 0.057 0.051 0.041 0.024 0.024 0.023 0.018 0.018 0.017 0.016 Her kjenner vi igjen observasjonene 1, 169 og 42 som de med høyest verdi på Cooks distance, men også casene 10, 55 og 125 har verdier over anbefalingen som kommer fra \\(4/n\\). Men vi ser også at verdien er relativt lave hvis man tar utgangspunkt i 1 som bekymringsfullt. For å vise eventuell justering av modellen som følge av uteliggere viser vi likevel framgangsmåte. Vi bør også undersøke added variable plots - i en regresjon kan observasjonene ha både en individuell og en sammensatt/felles påvirkning. # Bruker pakken: car avPlots(FieldOLS_reg, id=list(cex=0.75, n=3, method=&quot;mahal&quot;)) Her sier Fox &amp; Weisberg (2019), s.44 at Points at the extreme left or right of the plot correspond to cases that have high leverage on the corresponding coefficients and consequenlty are potentially influential. 1.3.6 Steg 6: Eventuell revisjon av modell Her kan vi for eksempel se hvordan modellens presterer ved bortfall av visse ektreme verdier (spesielt innflytelsesrike observasjoner/caser, jfr. diskusjon under regresjonsforutsetnigner) eller ved inkludering/eksklusjon av gitte variabler i modellen (først og fremst ved multippel regresjonsanalyse). Vi bør vurdere punktene over og vurdere om vi ønsker å lage en revidert modell der vi tar ut veldig innflytelsesrike caser/observasjoner. Som tidligere nevnt har vi ikke svært store verdier her, men la oss som et eksempel si at vi ønsker å se om en modell uten observasjon 169. Vi anbefaler å ta bort en og en observasjon siden (som nevnt) observasjonene/casene har både en individuell og felles påvirkning. FieldOLS_reg2 &lt;- update(FieldOLS_reg, subset = -169) # Bruker pakken: car compareCoefs(FieldOLS_reg, FieldOLS_reg2) ## Calls: ## 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data) ## 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data, subset = -169) ## ## Model 1 Model 2 ## (Intercept) 134.14 131.76 ## SE 7.54 7.39 ## ## Adverts 0.09612 0.09826 ## SE 0.00963 0.00942 ## Som forventet ser vi ikke de store forskjellene. Vi kan ta bort de andre observasjonene for illustrasjonens skyld: FieldOLS_reg3 &lt;- update(FieldOLS_reg, subset = -c(1, 42, 169, 184)) compareCoefs(FieldOLS_reg, FieldOLS_reg2, FieldOLS_reg3) ## Calls: ## 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data) ## 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data, subset = -169) ## 3: lm(formula = Sales ~ Adverts, data = Field_OLS_data, subset = -c(1, 42, ## 169, 184)) ## ## Model 1 Model 2 Model 3 ## (Intercept) 134.14 131.76 126.14 ## SE 7.54 7.39 7.28 ## ## Adverts 0.09612 0.09826 0.10461 ## SE 0.00963 0.00942 0.00942 ## Igjen, ikke de store endringene. Vi kan se at Intercept (\\(\\beta\\)) går litt ned etter hvert som vi tar bort caser, og betydningen av Adverts går litt opp (men det er marginalt). La oss, for eksempelets skyld, manipulere datasettet slik at en analyse av Cooks distance ser slik ut: # Bruker pakken: readxl Field_OLS_data2 &lt;- read_excel(&quot;Field_datasett_OLS2.xlsx&quot;) # Base R FieldOLS_man &lt;- lm(formula = Sales ~ Adverts, data = Field_OLS_data2) # Bruker pakken: car influenceIndexPlot(FieldOLS_man, vars = c(&quot;Cook&quot;), id = list(n=3)) Hvis vi nå kjører denne modellen opp mot en modell der vi tar bort 11 og 23 får vi: FieldOLS_man2 &lt;- update(FieldOLS_man, subset = -c(11, 23)) # Bruker pakken: car compareCoefs(FieldOLS_man, FieldOLS_man2) ## Calls: ## 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data2) ## 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data2, subset = -c(11, ## 23)) ## ## Model 1 Model 2 ## (Intercept) 183.77 134.14 ## SE 5.97 7.64 ## ## Adverts 0.01207 0.09621 ## SE 0.00298 0.00999 ## Her ser vi koeffisienten for Adverts stiger fra 0.01 til 0.09, eller en endring på 11.1%. 1.3.7 Steg 7: Eventuell analyse av revidert modell Her vil vi prinsippet bare gjenta samme analyser som ved analyse av den opprinnelige modellen. 1.3.8 Steg 8: Konklusjon / oppsummering / rapportering av resultater Tabell 1: Deskriptiv statistikk # Bruker pakken: table1 table1::label(Field_OLS_data$Adverts) &lt;- &quot;Adverts&quot; table1::label(Field_OLS_data$Sales) &lt;- &quot;Sales&quot; table1::table1(~Adverts + Sales, data = Field_OLS_data) Overall(N=200) Adverts Mean (SD) 614 (486) Median [Min, Max] 532 [9.10, 2270] Sales Mean (SD) 193 (80.7) Median [Min, Max] 200 [10.0, 360] Tabell 2: Korrelasjonsmatrise # Bruker pakken: sjPlot FieldOLSkorr &lt;- tab_corr(Field_OLS_data, triangle = &quot;lower&quot;) FieldOLSkorr   Adverts Sales Airplay Image Adverts         Sales 0.578***       Airplay 0.102 0.599***     Image 0.081 0.326*** 0.182**   Computed correlation used pearson-method with listwise-deletion. p &lt; .0001**** , p &lt; .001*** , p &lt; .01**, p &lt; .05* Tabell 3: Hierarkisk regresjonsanalyse av prediktorer for totalt selvoppfattet stress (tpstress) # Bruker pakken: sjPlot tab_model(FieldOLS_reg)   Sales Predictors Estimates CI p (Intercept) 134.14 119.28  149.00 &lt;0.001 Adverts 0.10 0.08  0.12 &lt;0.001 Observations 200 R2 / R2 adjusted 0.335 / 0.331 Vi viser her rapportering av en enkel lineær regresjonsanalyse etter APA-standard: En enkel lineær regresjon ble gjennomført for å predikere salgstall per uke basert på sum brukt på reklame uka før lansering. Vi fant en signifikant regresjonslikning (F(1,198) = 99.59, \\(\\beta\\) = 134.14p &lt; .001) med en \\(R^2\\) på .335, 95% CI [119.28, 149.00]. 1.3.9 Til slutt for R-brukere Mehmetoglu &amp; Mittner (2020) har skrevet en veldig god bok om på norsk: Innføring i R for statistiske analyser som vi varmt kan anbefale. Den kommer med en pakke (rnorsk) som kan lastes ned gjennom kommandoen devtools::install_github(ihrke/rnorsk) (forutsetter at pakken devtools er på plass, hvis ikke så kjør package.install(devtools)). Forfatterne har laget en samling av regresjonsdiagnostikk som vi viser her på Fields data der vi laget en multippel regresjonsmodell: options(scipen=999) # Bruker pakken: rnorsk regression.diagnostics(FieldOLS_mult_reg) ## Tests of linear model assumptions ## --------------------------------- ## ## 1/11 (9.1 %) checks failed ## ## ## Identified problems: ## functional form ## Summary: ## # A tibble: 11 x 8 ## assumption variable test statistic p.value crit problem decision ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 heteroskedasticity global studen~ 6.19e+0 0.103 0.05 No Pro~ + ## 2 heteroskedasticity global Non-co~ 3.03e-1 0.582 0.05 No Pro~ + ## 3 multicollinearity Adverts Varian~ 1.01e+0 NA 5 No Pro~ + ## 4 multicollinearity Airplay Varian~ 1.04e+0 NA 5 No Pro~ + ## 5 multicollinearity Image Varian~ 1.04e+0 NA 5 No Pro~ + ## 6 normality global Shapir~ 9.95e-1 0.725 0.01 No Pro~ + ## 7 model specification global Stata ~ -6.99e-5 0.916 0.05 No Pro~ + ## 8 functional form global RESET ~ 3.72e+0 0.0261 0.05 Problem - ## 9 outliers global Cook&#39;s~ 7.08e-2 NA 1 No Pro~ + ## 10 outliers global Bonfer~ 3.16e+0 0.362 0.05 No Pro~ + ## 11 autocorrelation global Durbin~ 2.70e-3 0.814 0.05 No Pro~ + ## ## Outliers: ## ----------- ## Cook&#39;s distance (criterion=1.00): No outliers ## Outlier test (criterion=0.05): No outliers Analysen gir en samlet oversikt over et antall parametere og søker å hjelpe til med beslutning om forutsetninger er ok/ikke ok, men vi vil understreke at kunnskap om hva som ligger bak de ulike testene og kriteriene er essensielt. Mer informasjon om parameterene finner dere her. 1.4 Standard multippel regresjonsanalyse 1.4.1 Eksempel standard multippel regresjonsanalyse Vi skal i gå gjennom et eksempel på multippel regresjonsanalyse, ved å følge stegene under. Analyse av dataene Evtentuelt valg av prediktorer ut fra analyse av dataene Lage modell (kjøre regresjonsanalysen) Analyse av resultatene (diagnostikk) Sjekk av forutsetningene Eventuell revisjon av modellen Eventuell analyse av revidert modell Konklusjon / oppsummering / rapportering av resultater 1.4.2 Analyse av dataene Vi skal bruke et datasett fra J. Pallant (2010) som du kan finne her. Download Pallant_survey.xlsx Download Pallant_survey.sav Download Pallant_survey.dta # Base R # Bruker pakken: readxl Pallant_survey &lt;- as.data.frame(read_excel(&quot;Pallant_survey.xlsx&quot;)) brief(Pallant_survey) ## 439 x 141 data.frame (434 rows and 133 columns omitted) ## id sex age marital child educ . . . Rtslfest_mean_1 tslfest_diff_rank ## [n] [n] [n] [n] [n] [n] [n] [n] ## 1 9 1 39 3 1 5 227.1413 35.6413 ## 2 307 1 41 5 1 2 227.1413 185.3587 ## 3 440 1 23 1 2 5 227.1413 212.6413 ## . . . ## 438 426 2 21 1 2 4 212.1905 102.1905 ## 439 245 2 74 4 2 2 212.1905 159.3095 Vi skal bruke to uavhengige variabler - tmast (Control of external events) og tpcioss (Control of internal states) mot den avhengige variabelen tpstress (Perceived stress). Vi ønsker altså å se om en gruppe studenters (N = 439) egenrapporterte oppfattelse av sin kontroll over eksterne forhold som kan skape stress og deres evne til å kontrollere deres følelser, tanker og fysiske reaksjoner (J. F. Pallant, 2000). Vi kan derfor se nærmere på variablene. # Base R Pallant_survey2 &lt;- select(Pallant_survey, tmast, tpcoiss, tpstress) Pallant_survey2 &lt;- na.omit(Pallant_survey2) summary(Pallant_survey2) ## tmast tpcoiss tpstress ## Min. : 8.00 Min. :20.00 Min. :12.00 ## 1st Qu.:19.00 1st Qu.:53.00 1st Qu.:23.00 ## Median :22.00 Median :62.00 Median :26.00 ## Mean :21.74 Mean :60.56 Mean :26.75 ## 3rd Qu.:25.00 3rd Qu.:69.00 3rd Qu.:31.00 ## Max. :28.00 Max. :88.00 Max. :46.00 # Base R par(mfrow=(c(2,2))) histtmast &lt;- with(Pallant_survey2, hist(tmast)) histtpcoiss &lt;- with(Pallant_survey2, hist(tpcoiss)) histtpstress &lt;- with(Pallant_survey2, hist(tpstress)) # Bruker pakken: car qqtmast &lt;- car::qqPlot(~ tmast, data = Pallant_survey2) qqtpcoiss &lt;- car::qqPlot(~ tpcoiss, data = Pallant_survey2) qqtpstress &lt;- car::qqPlot(~ tpstress, data = Pallant_survey2) # Base R boxplot(Pallant_survey2) 1.4.3 Evtentuelt valg av prediktorer ut fra analyse av dataene Vi gjør ingen analyse av dette da vi har valgt to uavhengige variabler ut fra eksempelet til J. Pallant (2010). 1.4.4 Lage modell (kjøre regresjonsanalysen) 1.4.5 Analyse av resultatene (diagnostikk) 1.4.5.1 Antall prediktorer/uavhengige variabler, overfitting og predicted R-square I multippel regresjonsanalyse har vi mer enn en prediktor/uavhengig variabel. Som regel kan vi ønske å inkludere mer enn en prediktor fordi vi sjeldent klarer å fange nok av variansen i en avhengig variabel gjennom en prediktor. Samtidig ønsker vi ikke flere uavhengige variabler enn nødvendig for å lage en så enkel modell som mulig som gir oss prediksjoner vi kan bruke. Matematisk er det også slik at enhver uavhengig variabel som har en grad av korrelasjon med den avhengige variabelen vil bidra til å øke \\(R^2\\) uten at det nødvendigvis gjør modellen riktigere (men kan gjøre den vanskeligere å tolke). Dersom vi legger til unødvendige uavhengige variabler risikerer vi det som kalles overfitting - altså at modellen blir god på å beskrive tilfeldige feil i dataene heller enn å beskrive forholdet mellom variablene. Resultatet er at modellen ikke kan generaliseres. Vi kan også se tilbake til regresjonsforutsetningene og vil se at et stort antall uavhengige variabler er en invitasjon til multikolinearitet. En måte å sjekke dette er å dele datasettet slik at man gjør regresjonsanalysen på en del av datasettet og deretter tester modellen på den andre delen av datasettet. Dette kalles kryssvalidering. En annen måte er å se på predicted R-square som innebærer følgende prosedyre (som statistikkprogrammer gjør for oss naturligvis): Et datapunkt/observasjon tas ut av datasettet Regresjonslikningen kalkuleres Modellens evne til å predikere det datapunktet/observasjonen som ble tatt ut evalueres (altså - hvor nærme datapunktet kommer vår modell i sin prediksjon?) Dette gjentas for alle datapunktene i datasettet Tolkningen av dette er ganske grei. Man sammenlikner R-square med predicted R-square. Dersom det er liten forskjell mellom disse verdiene har man trolig liten sannsynlighet for at du har overfitting av modellen. Dersom forskjellen er stor er det grunn til å tro at man kan ha overfitting. For å unngå dette er det viktig å tenke på forholdstallene som ble diskutert under foregående punkt om regresjonsforutsetninger. Kjennskap til tidligere forskning og resultater vil gi informasjon om og et teoretisk grunnlag for hvilke variabler som bør inkluderes i modellen. Vi skal illustrere overfitting basert på et eksempel fra Frost (2022) (dette eksempelet har altså ikke noe direkte med analysen vi er inne i - Pallant sitt datasett - men er tatt med for å illustrere poenget med overfitting). # Base R PresidentRanking &lt;- read.csv(&quot;PresidentRanking.csv&quot;) head(PresidentRanking) ## Historians.rank Approval.High ## 1 2 84 ## 2 6 87 ## 3 8 79 ## 4 9 83 ## 5 12 79 ## 6 29 67 Datasettet inneholder to variabler: Hvordan historikere rangerer amerikanske presidenter (Historians.rank) og hvor stor generell støtte presidenter har hatt i befolkningen (Approval.High). # Base R presidentlm &lt;- lm(formula = Historians.rank ~ Approval.High, data = PresidentRanking) plot(Historians.rank ~ Approval.High, data = PresidentRanking) abline(presidentlm, lwd = 2, col = &quot;red&quot;) S(presidentlm) ## Call: lm(formula = Historians.rank ~ Approval.High, data = PresidentRanking) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.8088 33.9094 0.761 0.464 ## Approval.High -0.1119 0.4286 -0.261 0.799 ## ## Residual standard deviation: 11.39 on 10 degrees of freedom ## Multiple R-squared: 0.006766 ## F-statistic: 0.06812 on 1 and 10 DF, p-value: 0.7994 ## AIC BIC ## 96.25 97.71 Vi ser at R-squared er \\(0.0068\\) - hvilket vi vil tolke som at det i praksis ikke er noen sammenheng mellom variablene. Vi kan så bruke en polynomisk likning (her har vi brukt \\(x^3\\) for å lage regresjonslinjen): # Base R presidentlm2 &lt;- lm(Historians.rank ~ poly(Approval.High, degree=3), data=PresidentRanking) # Bruker pakken: ggplot2 ggplot(data=PresidentRanking, aes(Approval.High,Historians.rank)) + geom_point() + geom_smooth(method=&quot;lm&quot;, formula=y~I(x^3)+I(x^2)) S(presidentlm2) ## Call: lm(formula = Historians.rank ~ poly(Approval.High, degree = 3), data = ## PresidentRanking) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.000 2.138 7.951 0.0000457 *** ## poly(Approval.High, degree = 3)1 -2.973 7.407 -0.401 0.6987 ## poly(Approval.High, degree = 3)2 19.409 7.407 2.620 0.0306 * ## poly(Approval.High, degree = 3)3 21.944 7.407 2.963 0.0181 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard deviation: 7.407 on 8 degrees of freedom ## Multiple R-squared: 0.6639 ## F-statistic: 5.268 on 3 and 8 DF, p-value: 0.02682 ## AIC BIC ## 87.25 89.67 Vi har nå en R-squared på \\(0.664\\) for denne modellen mot \\(0.0068\\) for den lineære modellen. Dette betyr at den polynomiske regresjonsmodellen forklarer drøye 66% av variansen i den avhengige variabelen. Modellen vår er ut fra dette en god modell for våre data. Imidlertid gir en analyse av Predicted R-square oss et annet bilde av modellen: # Bruker pakken: olsrr ols_pred_rsq(presidentlm2) ## [1] -0.2162257 I realiteten forteller både verdien på predicted r-square på 0, og forskjellen mellom R-square (\\(0.664\\)) og predicted R-square \\(0\\), at vi har en seriøs overfitting. Vi har med andre ord funnet en modell som beskriver dataene våre veldig godt, men som ikke kan brukes på andre data enn de vi har (vel, den kan jo brukes, men vil ikke kunne gi oss noe av verdi). Vi kan altså ikke predikere noe ut fra modellen. # Bruker pakken: olsrr PallantOLS_reg &lt;- ols_regress(tpstress ~ tmast + tpcoiss, data = Pallant_survey2) PallantOLS_reg ## Model Summary ## -------------------------------------------------------------- ## R 0.683 RMSE 4.278 ## R-Squared 0.466 Coef. Var 15.990 ## Adj. R-Squared 0.463 MSE 18.298 ## Pred R-Squared 0.456 MAE 3.261 ## -------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## ## ANOVA ## ----------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## ----------------------------------------------------------------------- ## Regression 6751.509 2 3375.755 184.486 0.0000 ## Residual 7740.115 423 18.298 ## Total 14491.624 425 ## ----------------------------------------------------------------------- ## ## Parameter Estimates ## ----------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## ----------------------------------------------------------------------------------------- ## (Intercept) 50.828 1.271 39.997 0.000 48.330 53.326 ## tmast -0.621 0.061 -0.422 -10.101 0.000 -0.741 -0.500 ## tpcoiss -0.175 0.020 -0.358 -8.559 0.000 -0.215 -0.135 ## ----------------------------------------------------------------------------------------- Vi ser at \\(R^2 = 0.466\\). Modellen kan altså forklare \\(46.6%\\) av variansen i den avhengige variabelen. Vi ser at vi får en noe lavere verdi for «Adjusted R Squared» i forhold til «R Squared». Når vi legger til en uavhengig variabel i en regresjonsanalyse er det lite sannsynlig at korrelasjonen mellom den nye uavhengige variabelen og den avhengige variabelen vil være nøyaktig 0. Den vil i stedet fluktuere rundt 0. Pga. denne tilfeldige fluktuasjonen rundt 0 vil \\(R^2\\) alltid øke litt når man legger til en ny uavhengig variabel. Adjusted \\(R^2\\) søker å kompensere for dette å få fram en mer korrekt verdi. Jo større antall uavhengige variabler, jo større forskjell vil man se mellom \\(R^2\\) og Adjusted \\(R^2\\). Det samme vil være tilfelle ved mindre utvalgsstørrelser fordi variasjonen rundt 0 vil være større i mindre utvalg. Vi kan også legge merke til at Predicted R-Squared er \\(0.456\\) og dermed svært lik R-squared. 1.4.5.2 Modellens koeffisienter Vi ser først på tallene for Std.Beta under Parameter Estimated. Vi ser at tmast bidrar i større grad enn tpcoiss (fortegn er i denne sammenheng irrelevant). Begge bidrar signifikant. 1.4.5.3 Hvor god er modellen vår (goodness of fit)? Vi kan se at F-verdien er 184.5. # Base R qf(p=.05, df1=2, df2=423, lower.tail=FALSE) ## [1] 3.017049 F-verdien er dermed langt over kritisk verdi (p &lt; 0.001). Det ser derfor ut til at det er svært usannsynlig at forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si at dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten i modellen så vil F &gt; 1. 1.4.6 Sjekk av forutsetningene Vi viser resultater av tester og grafer, men diskuterer ikke disse nærmere. I stedet viser vi til gjennomgangen av forutsetningene lenger opp . 1.4.6.1 Kausalitet Vi antar at det foreligger godt teoretisk grunnlag for modellen. #### Variablene er uten målefeil Vi må forutsette at vi ikke har systematiske målefeil i variablene. 1.4.6.2 Relevante og irrelevante variabler Også dette forutsetter vi er på plass. 1.4.6.3 Forholdstall mellom caser/observasjoner og uavhengige variabler # Base R nrow(Pallant_survey2) ## [1] 426 Vi har altså 426 observasjoner. I forhold til tabellen vist under forutsetninger lenger opp tilfredsstiller dette godt de ulike måtene å betrakte forholdstallet vi har vist til. 1.4.6.4 De uavhengige variablene er additiv for den avhengige variabelen Vi kan mistenke at det kan være en interaksjon mellom variablene tmast og tpcoiss - jo større kontroll på eksterne hendelser, jo større er kontrollen over følelser, tanker og fysisker reaksjoner. Vi ser derfor etter en interaksjonseffekt. # Base R PallantOLS_reg2 &lt;- lm(tpstress ~ tmast + tpcoiss, Pallant_survey2) PallantOLS_inter &lt;- lm(tpstress ~.+tmast*tpcoiss, Pallant_survey2) # Bruker pakken: car compareCoefs(PallantOLS_reg2, PallantOLS_inter) ## Calls: ## 1: lm(formula = tpstress ~ tmast + tpcoiss, data = Pallant_survey2) ## 2: lm(formula = tpstress ~ . + tmast * tpcoiss, data = Pallant_survey2) ## ## Model 1 Model 2 ## (Intercept) 50.83 52.66 ## SE 1.27 4.34 ## ## tmast -0.6207 -0.7093 ## SE 0.0614 0.2103 ## ## tpcoiss -0.1747 -0.2071 ## SE 0.0204 0.0763 ## ## tmast:tpcoiss 0.00154 ## SE 0.00348 ## Forskjellen mellom modellene ligger altså i den nederste koeffisienten (tmast:tpcoiss) som da er den kombinerte og samtidige effekten av de to variablene. Vi ser at effekten er veldig liten, men grafisk kan vi også se interaksjonseffekten: # Bruker pakken: ggplot2 ggplot(data=Pallant_survey2, aes(x=tmast, y=tpstress, group=1)) +geom_smooth(method=lm,se=F)+ geom_smooth(aes(tmast,tpcoiss), method=lm, se=F,color=&quot;black&quot;)+xlab(&quot;tmast og tpcoiss&quot;)+labs( title=&quot;tmast i blått - tpcoiss i svart&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; Det kan se ut som en interaksjonseffekt ved at linjene ikke er parallelle. Det kan imidlertid være noe vanskelig å tolke interaksjonseffekt. Man kan f.eks. ha en interaksjonseffekt som ikke er statistisk signifikant. I R kan vi bruke pakken jtools som hjelp: # Bruker pakken: jtools summ(PallantOLS_inter) Observations 426 Dependent variable tpstress Type OLS linear regression F(3,422) 122.82 R² 0.47 Adj. R² 0.46 Est. S.E. t val. p (Intercept) 52.66 4.34 12.13 0.00 tmast -0.71 0.21 -3.37 0.00 tpcoiss -0.21 0.08 -2.72 0.01 tmast:tpcoiss 0.00 0.00 0.44 0.66 Standard errors: OLS Vi ser at interaksjonen tmast:tpcoiss ikke er statistisk signifikant. # Bruker pakken: interactions interact_plot(PallantOLS_inter, pred = tmast, modx = tpcoiss) Tolkningen her er som før: Parallelle linjer indikerer fravær av interaksjonseffekt. 1.4.6.5 Linearitet Vi kan ikke se på samme type scatterplott for sjekk av linearitet som vi gjorde under enkel OLS (på et todimensjonalt plott). Vi kan imidlertid lage added variable plots (Mosteller &amp; Tukey, 1977). # Bruker pakken: olsrr ols_plot_added_variable(PallantOLS_reg2, print_plot = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; X-aksene representerer en enkelt uavhengig variabel (per graf ovenfor). Y-aksen = den avhengige variabelen. Den blå linjen viser sammenhengen mellom den uavhengige variabelen og den avhengige variabelen når alle andre uavhengige variabler holdes konstant. Jo sterkere lineær sammenheng i plottene, jo sterkere er den respektive uavhengige variabelens bidrag i modellen. I vårt tilfelle vil vi nok konkludere med at forutsetningen om linearitet er (nok) oppfylt. 1.4.6.6 Residualene skal være normalfordelte Side vi vet at residualene lagres i modellen og vi kan plotte ut disse: # Base R hist(PallantOLS_reg2$residuals) Vi kan også se på et Q-Q plott og hente ut testverdier for ulike normalitetstester: # Bruker pakken: olsrr ols_plot_resid_qq(PallantOLS_reg2) ols_test_normality(PallantOLS_reg2) ## Warning in ks.test(y, &quot;pnorm&quot;, mean(y), sd(y)): ties should not be present for ## the Kolmogorov-Smirnov test ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9911 0.0115 ## Kolmogorov-Smirnov 0.0509 0.2197 ## Cramer-von Mises 31.6502 0.0000 ## Anderson-Darling 1.0978 0.0070 ## ----------------------------------------------- Q-Q plottet viser noe avvik. En hendig graf er også et plott av residualene på y-aksen og fitted values på x-aksen: # Bruker pakken: olsrr ols_plot_resid_fit(PallantOLS_reg2) I forhold til forutsetningen om normalfordelte residualer skal disse være spredd tilfeldig rundt 0. Sett under ett kan det i dette tilfellet se ut som at residualene er tilnærmet (nok) normalfordeling. 1.4.6.7 Fravær av multikolinearitet Vi kan derfor sjekke for multikolinearitet gjennom å se på en korrelasjonsmatrisen: # Base R PallantKorr &lt;- cor(Pallant_survey2, method = &quot;pearson&quot;, use=&quot;pairwise.complete.obs&quot;) round(PallantKorr, 2) ## tmast tpcoiss tpstress ## tmast 1.00 0.53 -0.61 ## tpcoiss 0.53 1.00 -0.58 ## tpstress -0.61 -0.58 1.00 Hvis vi følger J. Pallant (2010) ser vi først at alle de to uavhengige variablene korrelerer mellom \\(r=0.58\\) og \\(r=-0.61\\) med den avhengige. Den bivariate korrelasjonen mellom de uavhengige variablene er på \\(0.53\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet. # Bruker pakken. olsrr ols_vif_tol(PallantOLS_reg2) ## Variables Tolerance VIF ## 1 tmast 0.7220785 1.384891 ## 2 tpcoiss 0.7220785 1.384891 Det er ingenting som indikerer at vi har multikolinearitet i dataene i denne modellen. 1.4.6.8 Fravær av heteroskedasisitet Den såkalte Whites test vil også kunne være et godt hjelpemiddel: # Bruker pakken: lmtestbptest(PallantOLS_reg2, ~ tmast*tpcoiss + I(tmast^2) + I(tpcoiss^2), data = Pallant_survey2) Her ser vi at \\(p &lt; 0.001\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med at vi har heteroskedasisitet i regresjonsmodellen. 1.4.6.9 Fravær av autokorrelasjon # Bruker pakken: car durbinWatsonTest(PallantOLS_reg2) ## lag Autocorrelation D-W Statistic p-value ## 1 0.08185218 1.825972 0.078 ## Alternative hypothesis: rho != 0 Her viser verdien \\(1.826\\) noe som ikke gir grunn til bekymring. 1.4.6.10 Fravær av innflytelsesrike observasjoner/caser Vi så under punktet Analyse av dataene (se for eksempel Boxplottet av Adverts) at vi har noen observasjoner i modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene: # Bruker pakken: car car::qqPlot(PallantOLS_reg2, id = list(n=3)) ## 22 194 269 ## 21 190 263 Vi ser at residualene ligger innenfor 95% konfidenstintervall. Det er lite ved residualplottet som vekker bekymring. Vi kan også kjøre analyser som identifiserer betydning/innvirkning (influential cases): # Bruker pakken: car influenceIndexPlot(PallantOLS_reg2, vars = &quot;hat&quot;, id = list(n=3)) DfBetas: # Bruker pakken: olsrr ols_plot_dfbetas(PallantOLS_reg2, print_plot = TRUE) dffit: # Bruker pakken: olsrr ols_plot_dffits(PallantOLS_reg2, print_plot = TRUE) Cooks distance: # Bruker pakken: car influenceIndexPlot(PallantOLS_reg2, vars = &quot;Cook&quot;, id = list(n=3)) # Base R mineCDverdier2 &lt;- cooks.distance(PallantOLS_reg2) mineCDverdier2 &lt;- round(mineCDverdier2, 5) head(sort(mineCDverdier2, decreasing = TRUE)) ## 22 268 23 191 413 194 ## 0.09556 0.05833 0.05434 0.04374 0.03689 0.03601 Det er ingenting ved hat values, DfBetas eller Cooks disgance som er bekymringsfullt. 1.4.6.11 Oppsummert om forutsetninger/diagnostikk options(scipen=999) # Bruker pakken: rnorsk regression.diagnostics(PallantOLS_reg2) ## Tests of linear model assumptions ## --------------------------------- ## ## 0/10 (0.0 %) checks failed ## ## ## Identified problems: NONE ## Summary: ## # A tibble: 10 x 8 ## assumption variable test statistic p.value crit problem decision ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 heteroskedasticity global studen~ 3.95 0.139 0.05 No Pro~ + ## 2 heteroskedasticity global Non-co~ 2.91 0.0881 0.05 No Pro~ + ## 3 multicollinearity tmast Varian~ 1.38 NA 5 No Pro~ + ## 4 multicollinearity tpcoiss Varian~ 1.38 NA 5 No Pro~ + ## 5 normality global Shapir~ 0.991 0.0115 0.01 No Pro~ + ## 6 model specification global Stata ~ 0.00495 0.568 0.05 No Pro~ + ## 7 functional form global RESET ~ 0.419 0.658 0.05 No Pro~ + ## 8 outliers global Cook&#39;s~ 0.0956 NA 1 No Pro~ + ## 9 outliers global Bonfer~ 3.56 0.177 0.05 No Pro~ + ## 10 autocorrelation global Durbin~ 0.0819 0.0620 0.05 No Pro~ + ## ## Outliers: ## ----------- ## Cook&#39;s distance (criterion=1.00): No outliers ## Outlier test (criterion=0.05): No outliers 1.4.7 Eventuell revisjon av modellen Vi har ut fra foregående punkt der vi så på eventuelle innflytelsesrike observasjoner ikke behov for å revidere modellen. 1.4.8 Eventuell analyse av revidert modell Se forrige punkt. 1.4.9 Konklusjon / oppsummering / rapportering av resultater Tabell 1: Deskriptiv statistikk # Bruker pakken: table1 table1::label(Pallant_survey$tmast) &lt;- &quot;tmast&quot; table1::label(Pallant_survey$tpcoiss) &lt;- &quot;tpcoiss&quot; table1::label(Pallant_survey$tpstress) &lt;- &quot;tpstress&quot; table1::table1(~tmast + tpcoiss + tpstress, data = Pallant_survey) Overall(N=439) tmast Mean (SD) 21.8 (3.97) Median [Min, Max] 22.0 [8.00, 28.0] Missing 3 (0.7%) tpcoiss Mean (SD) 60.6 (12.0) Median [Min, Max] 62.0 [20.0, 88.0] Missing 9 (2.1%) tpstress Mean (SD) 26.7 (5.85) Median [Min, Max] 26.0 [12.0, 46.0] Missing 6 (1.4%) Tabell 2: Korrelasjonsmatrise # Bruker pakken: sjPlot Pallantkorr1 &lt;- tab_corr(Pallant_survey2, triangle = &quot;lower&quot;) Pallantkorr1   tmast tpcoiss tpstress tmast       tpcoiss 0.527***     tpstress -0.611*** -0.581***   Computed correlation used pearson-method with listwise-deletion. p &lt; .0001**** , p &lt; .001*** , p &lt; .01**, p &lt; .05* Tabell 3: Hierarkisk regresjonsanalyse av prediktorer for totalt selvoppfattet stress (tpstress) Modell0. modell1 og modell2: # Bruker pakken: sjPlot tab_model(PallantOLS_reg2)   tpstress Predictors Estimates CI p (Intercept) 50.83 48.33  53.33 &lt;0.001 tmast -0.62 -0.74  -0.50 &lt;0.001 tpcoiss -0.17 -0.21  -0.13 &lt;0.001 Observations 426 R2 / R2 adjusted 0.466 / 0.463 En multippel lineær regresjonsanalyse ble gjennomført for å teste om respondentenes oppfattede nivå av indre kontroll og kontroll på ytre faktorer predikerer totalt nivå av oppfattet stress. Analyser ble gjennomført for å sikre at det ikke var brudd på forutsetningene for en multippel lineær regresjonsanalyse. En statistisk signifikant regresjonsmodell ble funnet (F (2, 423) = 184.5, p &lt; .001), med en \\(R^2\\) på .466. Både nivå av indre kontroll og kontroll på ytre faktorer var signifikante prediktorer. 1.5 Hierarkisk multippel regresjonsanalyse Vi skal gå gjennom et eksempel på hierarksik multippel regresjonsanalyse, ved å følge stegene under. Hierarkisk vil dere også kunne se omtalt som blockwise entry fordi data legges inn i modellen i blokker). Analyse av dataene Evtentuelt valg av prediktorer ut fra analyse av dataene Lage modell (kjøre regresjonsanalysen) Analyse av resultatene (diagnostikk) Sjekk av forutsetningene Eventuell revisjon av modellen Eventuell analyse av revidert modell Konklusjon / oppsummering / rapportering av resultater Eksempelet utvider eksempelet over for standard multippel regresjonsanalyse. 1.5.1 Analyse av dataene Vi skal bruke samme datasett fra J. Pallant (2010) som du kan finne her. Download Pallant_survey.xlsx Download Pallant_survey.sav Download Pallant_survey.dta # Base R # Bruker pakken: readxl Pallant_survey &lt;- as.data.frame(read_excel(&quot;Pallant_survey.xlsx&quot;)) brief(Pallant_survey) ## 439 x 141 data.frame (434 rows and 133 columns omitted) ## id sex age marital child educ . . . Rtslfest_mean_1 tslfest_diff_rank ## [n] [n] [n] [n] [n] [n] [n] [n] ## 1 9 1 39 3 1 5 227.1413 35.6413 ## 2 307 1 41 5 1 2 227.1413 185.3587 ## 3 440 1 23 1 2 5 227.1413 212.6413 ## . . . ## 438 426 2 21 1 2 4 212.1905 102.1905 ## 439 245 2 74 4 2 2 212.1905 159.3095 I tillegg til de to uavhengige variabler - tmast (Control of external events) og tpcioss (Control of internal states) vil vi bruke variablene alder (age) og social desirability (tmarlow) som kontrollvariabler. Social desirability bias er et kjent fenomen der respondenter har en tendens til å ønske å framstå i et bedre lys heller enn i et sant/reelt lys (Preiss et al., 2015). Respondenter kan f.eks. ønske å framstå som mer ærlige enn de i realiteten er, og vil derfor også svare deretter på spørsmål. I tillegg kan man anta at alder kan ha påvirkning på opplevelsen av totalt stressnivå. Vi velger å lage et nytt datasett der vi trekker ut de relevante variablene, og fjerne observasjoner med NA (ingen verdi). # Base R Pallant_survey3 &lt;- select(Pallant_survey, age, tmarlow, tmast, tpcoiss, tpstress) Pallant_survey3 &lt;- na.omit(Pallant_survey3) brief(Pallant_survey3) ## 423 x 5 data.frame (418 rows omitted) ## age tmarlow tmast tpcoiss tpstress ## [n] [n] [n] [n] [n] ## 1 39 5 21 40 22 ## 2 41 3 26 63 31 ## 3 23 4 23 46 27 ## . . . ## 433 21 4 21 65 25 ## 437 48 7 19 72 28 I denne hierarkiske multipple regresjonsanalysen ønsker vi derfor å bruke alder og sosial ønskverdighet som kontrollvariabler. Vi legger i første blokk inn de variablene vi ønsker å kontrollere for, før vi i blokk to legger inn de samme uavhengige variablene som i forrige eksempel. Hensikten med dette er at vi ønsker å fjerne mulige effekter av de to kontrollvariablene. # Base R par(mfrow=(c(1,2))) histage &lt;- with(Pallant_survey3, hist(age)) histtmarlow &lt;- with(Pallant_survey3, hist(tmarlow)) 1.5.2 Evtentuelt valg av prediktorer ut fra analyse av dataene Vi gjør ingen analyse av dette da vi har valgt to kontrollvariabler og to uavhengige variabler ut fra eksempelet til J. Pallant (2010). 1.5.3 Lage modell (kjøre regresjonsanalysen) Her lager vi tre modeller: Modell 0 inneholder kun Intercept (som en referansemodell). Modell 1 inneholder kun age og tmarlow, modell 2 inneholder i tillegg tmast og tpcoiss. # Base R modell0 &lt;- lm(tpstress ~ 1, data = Pallant_survey3) modell1 &lt;- lm(tpstress ~ age + tmarlow, data = Pallant_survey3) modell2 &lt;- lm(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey3) 1.5.4 Analyse av resultatene (diagnostikk) # Base R summary(modell1) ## ## Call: ## lm(formula = tpstress ~ age + tmarlow, data = Pallant_survey3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.1180 -4.0683 -0.4526 3.7385 16.7538 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.24030 0.99691 31.34 &lt; 0.0000000000000002 *** ## age -0.03287 0.02191 -1.50 0.134 ## tmarlow -0.61905 0.14230 -4.35 0.0000171 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.69 on 420 degrees of freedom ## Multiple R-squared: 0.05899, Adjusted R-squared: 0.0545 ## F-statistic: 13.16 on 2 and 420 DF, p-value: 0.000002852 summary(modell2) ## ## Call: ## lm(formula = tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.7629 -2.5927 -0.1862 2.4873 12.3807 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.71592 1.37130 37.713 &lt; 0.0000000000000002 *** ## age -0.02057 0.01713 -1.201 0.231 ## tmarlow -0.14691 0.11066 -1.328 0.185 ## tmast -0.63142 0.06316 -9.997 &lt; 0.0000000000000002 *** ## tpcoiss -0.16004 0.02189 -7.311 0.00000000000136 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.28 on 418 degrees of freedom ## Multiple R-squared: 0.47, Adjusted R-squared: 0.4649 ## F-statistic: 92.67 on 4 and 418 DF, p-value: &lt; 0.00000000000000022 Den første modellen er modellen med kun alder og social desirability som forklarer 5,9 % av variansen. Den andre modellen, som består av både kontrollvariablene og de to tidligere uavhengige variablene forklarer til sammen 47 %. # Base R anova(modell0, modell1, modell2) ## Analysis of Variance Table ## ## Model 1: tpstress ~ 1 ## Model 2: tpstress ~ age + tmarlow ## Model 3: tpstress ~ age + tmarlow + tmast + tpcoiss ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 422 14450.3 ## 2 420 13598.0 2 852.4 23.26 0.0000000002642 *** ## 3 418 7658.8 2 5939.2 162.07 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Vi kan oppsummere modellene: Modell 0: \\(SS_Total\\) = 14450 (ingen prediktorer) Modell 1: \\(SS_Residual\\) = 13598.0, \\(SS_Difference\\) = 852.4, \\(F\\)(2, 420) = 23.26, \\(p\\)&lt;.001 (etter å ha lagt til age og tmarlow) Modell 2: \\(SS_Residual\\) = 7658.8, \\(SS_Difference\\) = 5939.2, \\(F\\)(2, 418) = 162.07, \\(p\\)&lt;.001 (etter å ha lagt til tmast og tpcoiss) Det vi jo ønsket å finne ut av var hvor mye våre to uavhengige variabler forklarer etter at effektene fra de to kontrollvariablene er tatt bort. Dette finner vi i «R Square Change» - altså hvor mye \\(R^2\\) endrer seg fra modell 1 til modell 2. Vi har i tillegg med hvor mye \\(R^2\\) endrer seg fra modell 0 ti lmodell 1. # Base R rsq_diff1 &lt;- summary(modell1)$r.squared - summary(modell0)$r.squared rsq_diff2 &lt;- summary(modell2)$r.squared - summary(modell1)$r.squared rsq_diff1 ## [1] 0.05898557 rsq_diff2 ## [1] 0.4110042 For modell 2 er verdien 0.411. Altså  de to uavhengige variablene forklarer 41.1 % av variansen i den avhengige variabelen etter at vi har kontrollert for alder og sosial ønskverdighet. Vi kan så se på de uavhengige variablenes unike bidrag (altså bidrag etter at interaksjonseffekter er tatt bort). 1.5.4.1 Modellens koeffisienter # Bruker pakken: olsrr modell2x &lt;- ols_regress(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey) modell2x ## Model Summary ## -------------------------------------------------------------- ## R 0.686 RMSE 4.280 ## R-Squared 0.470 Coef. Var 16.011 ## Adj. R-Squared 0.465 MSE 18.323 ## Pred R-Squared 0.455 MAE 3.277 ## -------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## ## ANOVA ## ---------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## ---------------------------------------------------------------------- ## Regression 6791.514 4 1697.879 92.666 0.0000 ## Residual 7658.831 418 18.323 ## Total 14450.345 422 ## ---------------------------------------------------------------------- ## ## Parameter Estimates ## ---------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## ---------------------------------------------------------------------------------------- ## (Intercept) 51.716 1.371 37.713 0.000 49.020 54.411 ## age -0.021 0.017 -0.046 -1.201 0.231 -0.054 0.013 ## tmarlow -0.147 0.111 -0.051 -1.328 0.185 -0.364 0.071 ## tmast -0.631 0.063 -0.429 -9.997 0.000 -0.756 -0.507 ## tpcoiss -0.160 0.022 -0.328 -7.311 0.000 -0.203 -0.117 ## ---------------------------------------------------------------------------------------- Vi ser på tallene for Std.Beta under Parameter Estimates. Vi ser at alder og social desirability ikke bidrar signifikant i seg selv. Beta verdiene i tabellen representerer de unike bidragene for hver variabel etter at overlappende effekter fra de andre variablene er fjernet. tmast bidrar i større grad (beta = -0,631) enn pcoiss (beta = -0,160). 1.5.4.2 Hvor god er modellen vår (goodness of fit)? Vi kan se at F-verdien er 92.67. # Base R qf(p=.05, df1=4, df2=418, lower.tail=FALSE) ## [1] 2.393283 F-verdien er dermed langt over kritisk verdi (p &lt; 0.001). Det ser derfor ut til at det er svært usannsynlig at forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si at dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten i modellen så vil F &gt; 1. 1.5.5 Sjekk av forutsetningene Vi viser resultater av tester og grafer, men diskuterer ikke disse nærmere. I stedet viser vi til gjennomgangen av forutsetningene lenger opp . 1.5.5.1 Kausalitet Vi antar at det foreligger godt teoretisk grunnlag for modellen. #### Variablene er uten målefeil Vi må forutsette at vi ikke har systematiske målefeil i variablene. 1.5.5.2 Relevante og irrelevante variabler Også dette forutsetter vi er på plass. 1.5.5.3 Forholdstall mellom caser/observasjoner og uavhengige variabler # Base R nrow(Pallant_survey2) ## [1] 426 Vi har altså 426 observasjoner. I forhold til tabellen vist under forutsetninger lenger opp tilfredsstiller dette godt de ulike måtene å betrakte forholdstallet vi har vist til. 1.5.5.4 De uavhengige variablene er additiv for den avhengige variabelen Vi kan mistenke at det kan være en interaksjon mellom variablene tmast og tpcoiss - jo større kontroll på eksterne hendelser, jo større er kontrollen over følelser, tanker og fysisker reaksjoner. Vi ser derfor etter en interaksjonseffekt. # Base R PallantOLS2 &lt;- lm(tpstress ~ age + tmarlow + tmast + tpcoiss, Pallant_survey) PallantOLS_inter &lt;- lm(tpstress ~ age + tmarlow + tmast + tpcoiss + tmast*tpcoiss, Pallant_survey) # Bruker pakken: car compareCoefs(PallantOLS2, PallantOLS_inter) ## Calls: ## 1: lm(formula = tpstress ~ age + tmarlow + tmast + tpcoiss, data = ## Pallant_survey) ## 2: lm(formula = tpstress ~ age + tmarlow + tmast + tpcoiss + tmast * ## tpcoiss, data = Pallant_survey) ## ## Model 1 Model 2 ## (Intercept) 51.72 53.80 ## SE 1.37 4.38 ## ## age -0.0206 -0.0206 ## SE 0.0171 0.0171 ## ## tmarlow -0.147 -0.148 ## SE 0.111 0.111 ## ## tmast -0.6314 -0.7324 ## SE 0.0632 0.2111 ## ## tpcoiss -0.1600 -0.1969 ## SE 0.0219 0.0768 ## ## tmast:tpcoiss 0.00175 ## SE 0.00349 ## Forskjellen mellom modellene ligger altså i den nederste koeffisienten (tmast:tpcoiss) som da er den kombinerte og samtidige effekten av de to variablene. Vi ser at effekten er veldig liten (vi gjentar ikke den grafiske framstillingen som er lik som forrige analyse). 1.5.5.5 Linearitet Se forrige regresjonsanalyse. 1.5.5.6 Residualene skal være normalfordelte Side vi vet at residualene lagres i modellen og vi kan plotte ut disse: # Base R hist(PallantOLS2$residuals) Vi kan også se på et Q-Q plott og hente ut testverdier for ulike normalitetstester: # Bruekr pakken: olsrr ols_plot_resid_qq(PallantOLS2) ols_test_normality(PallantOLS2) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9926 0.0345 ## Kolmogorov-Smirnov 0.038 0.5743 ## Cramer-von Mises 31.3398 0.0000 ## Anderson-Darling 0.8705 0.0255 ## ----------------------------------------------- Q-Q plottet viser noe avvik (jfr. ) En hendig graf er også et plott av residualene på y-aksen og fitted values på x-aksen: # Bruker pakken: olsrr ols_plot_resid_fit(PallantOLS2) I forhold til forutsetningen om normalfordelte residualer skal disse være spredd tilfeldig rundt 0. Sett under ett kan det i dette tilfellet se ut som at residualene er tilnærmet (nok) normalfordeling. 1.5.5.7 Fravær av multikolinearitet Vi kan derfor sjekke for multikolinearitet gjennom å se på en korrelasjonsmatrisen: # Base R PallantKorr &lt;- cor(Pallant_survey2, method = &quot;pearson&quot;) round(PallantKorr, 2) ## tmast tpcoiss tpstress ## tmast 1.00 0.53 -0.61 ## tpcoiss 0.53 1.00 -0.58 ## tpstress -0.61 -0.58 1.00 Hvis vi følger J. Pallant (2010) ser vi først at alle de to uavhengige variablene korrelerer mellom \\(r=0.52\\) og \\(r=-0.58\\) med den avhengige. Den bivariate korrelasjonen mellom de uavhengige variablene er på \\(0.52\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet. # Bruker pakken: olsrr ols_vif_tol(PallantOLS2) ## Variables Tolerance VIF ## 1 age 0.8636921 1.157820 ## 2 tmarlow 0.8729179 1.145583 ## 3 tmast 0.6897936 1.449709 ## 4 tpcoiss 0.6290247 1.589763 Det er ingenting som indikerer at vi har multikolinearitet i dataene i denne modellen. 1.5.5.8 Fravær av heteroskedasisitet Den såkalte Whites test vil også kunne være et godt hjelpemiddel: # Bruker pakken: lmtest bptest(PallantOLS2, ~ tmast*tpcoiss + I(tmast^2) + I(tpcoiss^2), data = Pallant_survey2) ## ## studentized Breusch-Pagan test ## ## data: PallantOLS2 ## BP = 40.384, df = 5, p-value = 0.0000001249 Her ser vi at \\(p &lt; 0.001\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med at vi har heteroskedasisitet i regresjonsmodellen. 1.5.5.9 Fravær av autokorrelasjon # Bruker pakke: car durbinWatsonTest(lm(tpstress ~ age + tmarlow + tmast + tpcoiss, Pallant_survey)) ## lag Autocorrelation D-W Statistic p-value ## 1 0.08634585 1.817403 0.046 ## Alternative hypothesis: rho != 0 Her viser verdien \\(1.817\\) med p &lt; 0.05 noe som indikerer at vi har autokorrelasjon. 1.5.5.10 Fravær av innflytelsesrike observasjoner/caser Vi så under punktet Analyse av dataene (se for eksempel Boxplottet av Adverts) at vi har noen observasjoner i modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene: # Bruker pakke: car car::qqPlot(PallantOLS2, id = list(n=3)) ## [1] 22 194 269 Vi ser at residualene ligger innenfor 95% konfidenstintervall. Det er lite ved residualplottet som vekker bekymring. Vi kan også kjøre analyser som identifiserer betydning/innvirkning (influential cases): # Bruker pakke: car influenceIndexPlot(PallantOLS2, vars = &quot;hat&quot;, id = list(n=3)) DfBetas: # Bruker pakken: olsrr ols_plot_dfbetas(PallantOLS2, print_plot = TRUE) Cooks distance: # Bruker pakke: car influenceIndexPlot(PallantOLS2, vars = &quot;Cook&quot;, id = list(n=3)) # Base R mineCDverdier2 &lt;- cooks.distance(PallantOLS2) mineCDverdier2 &lt;- round(mineCDverdier2, 5) head(sort(mineCDverdier2, decreasing = TRUE)) ## 22 23 268 389 191 413 ## 0.05880 0.04913 0.03618 0.02859 0.02728 0.02530 Det er ingenting ved hat values, DfBetas eller Cooks disgance som er bekymringsfullt. 1.5.5.11 Oppsummert om forutsetninger/diagnostikk options(scipen=999) # Bruekr pakken: rnorsk regression.diagnostics(PallantOLS2) ## Tests of linear model assumptions ## --------------------------------- ## ## 0/12 (0.0 %) checks failed ## ## ## Identified problems: NONE ## Summary: ## # A tibble: 12 x 8 ## assumption variable test statistic p.value crit problem decision ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 heteroskedasticity global studen~ 7.01 0.135 0.05 No Pro~ + ## 2 heteroskedasticity global Non-co~ 1.86 0.173 0.05 No Pro~ + ## 3 multicollinearity age Varian~ 1.16 NA 5 No Pro~ + ## 4 multicollinearity tmarlow Varian~ 1.15 NA 5 No Pro~ + ## 5 multicollinearity tmast Varian~ 1.45 NA 5 No Pro~ + ## 6 multicollinearity tpcoiss Varian~ 1.59 NA 5 No Pro~ + ## 7 normality global Shapir~ 0.993 0.0345 0.01 No Pro~ + ## 8 model specification global Stata ~ 0.00772 0.366 0.05 No Pro~ + ## 9 functional form global RESET ~ 0.897 0.409 0.05 No Pro~ + ## 10 outliers global Cook&#39;s~ 0.0588 NA 1 No Pro~ + ## 11 outliers global Bonfer~ 3.54 0.190 0.05 No Pro~ + ## 12 autocorrelation global Durbin~ 0.0863 0.0660 0.05 No Pro~ + ## ## Outliers: ## ----------- ## Cook&#39;s distance (criterion=1.00): No outliers ## Outlier test (criterion=0.05): No outliers 1.5.6 Eventuell revisjon av modellen Vi har ut fra foregående punkt der vi så på eventuelle innflytelsesrike observasjoner ikke behov for å revidere modellen. 1.5.7 Eventuell analyse av revidert modell Se forrige punkt. 1.5.8 Konklusjon / oppsummering / rapportering av resultater Tabell 1: Deskriptiv statistikk # Bruker pakken: table1 table1::label(Pallant_survey3$age) &lt;- &quot;age&quot; table1::label(Pallant_survey3$tmarlow) &lt;- &quot;tmarlow&quot; table1::label(Pallant_survey3$tmast) &lt;- &quot;tmast&quot; table1::label(Pallant_survey3$tpcoiss) &lt;- &quot;tpcoiss&quot; table1::label(Pallant_survey3$tpstress) &lt;- &quot;tpstress&quot; table1::table1(~age + tmarlow + tmast + tpcoiss + tpstress, data = Pallant_survey3) Overall(N=423) age Mean (SD) 37.3 (13.1) Median [Min, Max] 36.0 [18.0, 82.0] tmarlow Mean (SD) 5.30 (2.02) Median [Min, Max] 5.00 [0, 10.0] tmast Mean (SD) 21.8 (3.97) Median [Min, Max] 22.0 [8.00, 28.0] tpcoiss Mean (SD) 60.6 (12.0) Median [Min, Max] 62.0 [20.0, 88.0] tpstress Mean (SD) 26.7 (5.85) Median [Min, Max] 26.0 [12.0, 46.0] Tabell 2: Korrelasjonsmatrise # Bruker pakken: sjPlot Pallantkorr &lt;- tab_corr(Pallant_survey3, triangle = &quot;lower&quot;) Pallantkorr   age tmarlow tmast tpcoiss tpstress age           tmarlow 0.259***         tmast -0.028 0.169***       tpcoiss 0.249*** 0.296*** 0.530***     tpstress -0.129** -0.232*** -0.610*** -0.582***   Computed correlation used pearson-method with listwise-deletion. p &lt; .0001**** , p &lt; .001*** , p &lt; .01**, p &lt; .05* Tabell 3: Hierarkisk regresjonsanalyse av prediktorer for totalt selvoppfattet stress (tpstress) Modell0. modell1 og modell2: # Bruker pakken: sjPlot tab_model(modell0, modell1, modell2)   tpstress tpstress tpstress Predictors Estimates CI p Estimates CI p Estimates CI p (Intercept) 26.74 26.18  27.29 &lt;0.001 31.24 29.28  33.20 &lt;0.001 51.72 49.02  54.41 &lt;0.001 age -0.03 -0.08  0.01 0.134 -0.02 -0.05  0.01 0.231 tmarlow -0.62 -0.90  -0.34 &lt;0.001 -0.15 -0.36  0.07 0.185 tmast -0.63 -0.76  -0.51 &lt;0.001 tpcoiss -0.16 -0.20  -0.12 &lt;0.001 Observations 423 423 423 R2 / R2 adjusted 0.000 / 0.000 0.059 / 0.055 0.470 / 0.465 1.6 Stegvis mutlippel regresjonsanalyse Stegvis regresjon er en iterativ multippel regresjonsanalyse der prosessen gradvis (steg for steg = stegvis) setter inn eller tar bort uavhengige variabler som ikke bidrar til forklaring av variansen i den avhengige variabelen. Man kan f.eks. ha en situasjon der man har et stort antall uavhengige variabler der problemet er å avgjøre hvor mange og hvilke variabler som skal være med i regresjonsanalysen. Det finnes ulike strategier og teknikker for å velge de uavhengige variablene som til slutt skal gå inn i modellen. Målet er å finne den kombinasjonen av uavhengige variabler blant et større antall variabler som best forklarer variansen i den avhengige variabelen. Stepwise selection kan foretas på ulike måter: forward, backward og bidirectional (toveis). Forward Selection (også omtalt som «Step-Up») innebærer altså at man starter med null variabler. Først settes den variabelen med lavest p-verdi for F. I hvert steg deretter settes den variabelen som ennå ikke er lagt inn i modellen med lavest p-verdi for F inn. Når ingen av de gjenværende variablene er signifikante stoppes prosessen. Denne prosedyren kan ha sin nytte for en analyse av et stort antall «kandidatvariabler» til en regresjonsmodell. Det som gjøres er: Finn den uavhengige variabelen som alene forklarer mest av variansen. Legg denne inn i modellen dersom p-verdien er under terskelverdien (f.eks. 0.05). Sjekk p-verdiene til alle variablene i modellen. Dersom noen av variablene i modellen har p-verdi over terskelverdien (f.eks. 0.10) skal variabelen tas ut av modellen. Gjenta steg 1 og 2 inntil alle signifikante uavhengige variabler er inkludert i modellen og alle ikke-signifikante verdier er ute av modellen. Backward selection (også omtalt som Step-Down eller Backward Elimination) starter i motsatt ende av forward. Her puttes alle de uavhengige variablene inn først. Deretter tar man hver gang bort variabelen som har høyest p-verdi for F inntil man ikke har flere ikke-signifikante variabler igjen i modellen. Bidirectional utvelgelse er en kombinasjon av de to foregående. Den starter som forward selection, men hver gang en variabel settes inn blir alle variablene i modellen kontrollert for å se om p-verdien for F har endret seg til over en definert terskelverdi som følge av den nye variabelen. Hvis en ikke-signifikant variabel da blir funnet fjernes den. Når ingen variabler tilfredsstiller terskelverdiene for enten inkludering eller ekskludering i modellen stopper prosessen. Dette krever to definerte signifikansnivåer: En for å inkludere og en for å ekskludere, der terskelverdien for å inkludere må være lavere enn for å ekskludere (med mindre man ønsker en uendelig loop der man aldri finner en løsning). Det skal bemerkes at dette er en rent matematisk/statistisk operasjon. Det ligger ingen teoretiske vurderinger til grunn. Man har selvsagt ingen garanti for at de uavhengige variablene man ender opp med er fornuftige eller har noen praktisk signifikans. Ethvert resultat fra en stegvis regresjon må derfor vurderes kritisk. Man står også i fare for såvel overfitting som underfitting (jfr. tidligere punkt om overfitting) (Field, 2009). Det mangler ikke på advarsler om å bruke stegvis regresjon, f.eks. fra Miles &amp; Shevlin (2001) som advarer om at stegvis regresjonsanalyse should be used with extreme caution (s.38). Singer &amp; Willett (2003) hevder på sin side Never let a computer select predictors mechanically. The computer does not know your research questions nor the literature upon which they rest. It cannot distinguish predictors of direct substantive interest from those whose effects you want to control. Man skal i hvert fall se på resultatene med et veldig kritisk blikk siden The data analyst knows more than the computer (Henderson &amp; Velleman, 1981, p. s.391). Et alvorlig problem med stegvis regresjonsanalyse er at man får en \\(R^2\\)-verdi som har svært unøyaktig høy (Miles &amp; Shevlin, 2001).Dette skyldes at statistikkprogrammet tråler gjennom et stort antall uavhengige variabler på søken etter statistisk signifikante variabler og velger ut alle disse. En andel variabler vil være signifikante av tilfeldighet noe som øker verdien på \\(R^2\\). Forutsetningene er de samme som for standard multippel regresjon. Spesielt skal man være observant på uteliggere. En tommelfingerregel som angis er minimum 5 caser per variabel (50 variabler = minimum 250 caser). Vi skal se på et eksempel som er hentet fra (van den Berg, 2018). Du kan laste ned datasettet i ulike formater her: Download magazine_reg.xlsx Download magazine_reg.sav Download magazine_reg.dta 1.6.1 Eksempel stegvis multippel regresjonsanalyse Vi skal i gå gjennom et eksempel på stegvis multippel regresjonsanalyse, ved å følge stegene 1-4 under. Analyse av dataene Lage modell (kjøre regresjonsanalysen) Analyse av resultatene (diagnostikk) 1.6.2 Analyse av dataene # Base R magazine_data &lt;- as.data.frame(read_excel(&quot;magazine_reg.xlsx&quot;)) brief(magazine_data) ## 637 x 18 data.frame (632 rows and 6 columns omitted) ## intnr gender age educ prof whours satov sat1 sat2 sat3 . . . mis1 filt1 ## [n] [n] [c] [n] [n] [n] [n] [n] [n] [n] [n] [n] ## 1 46 2 26 5 1 NA 6 4 4 4 0 1 ## 2 49 1 47 6 2 5 8 3 4 4 0 1 ## 3 53 1 51 6 2 5 7 5 5 3 1 1 ## . . . ## 636 2492 1 38 5 2 4 8 5 5 4 0 1 ## 637 2497 1 22 5 1 NA 9 5 4 4 0 1 # Base R par(mfrow=(c(2,3))) histtsat1 &lt;- with(magazine_data, hist(sat1)) histtsat2 &lt;- with(magazine_data, hist(sat2)) histtsat3 &lt;- with(magazine_data, hist(sat3)) histtsat4 &lt;- with(magazine_data, hist(sat4)) histtsat5 &lt;- with(magazine_data, hist(sat5)) histtsat6 &lt;- with(magazine_data, hist(sat6)) histtsat7 &lt;- with(magazine_data, hist(sat7)) histtsat8 &lt;- with(magazine_data, hist(sat8)) histtsat9 &lt;- with(magazine_data, hist(sat9)) 1.6.3 Lage modell (kjøre regresjonsanalysen) Hvilke av de 9 uavhengige variablene (sat1sat9), som er ulike mål på kunders oppfatning av produktet, bidrar signifikant til å forklare variansen i hvordan kundene totalt sett skårer produktet (magasinet) («How would you rate our magazine altogether?»). De ulike målene er f.eks. grundighet, objektivitet, lesbarhet og pålitelighet. Spørsmålet er hvilke aspekter (sat1sat9) har størst innflytelse på kundetilfredsheten? # Base R magazine_intercept &lt;- lm(satov ~ 1, data = magazine_data) magazine_all &lt;- lm(satov ~ sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat8 + sat9, data = magazine_data) magazine_forward &lt;- step(magazine_intercept, direction = &quot;forward&quot;, scope = formula(magazine_all), trace = 0) 1.6.4 Analyse av resultatene (diagnostikk) - forward # Base R magazine_forward$anova ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 NA NA 636 381.2308 -325.0134 ## 2 + sat1 -1 71.7435240 635 309.4872 -455.8202 ## 3 + sat3 -1 33.8817998 634 275.6054 -527.6782 ## 4 + sat5 -1 20.9219246 633 254.6835 -575.9685 ## 5 + sat7 -1 9.7484194 632 244.9351 -598.8295 ## 6 + sat9 -1 6.5282509 631 238.4069 -614.0379 ## 7 + sat2 -1 2.2298960 630 236.1770 -618.0240 ## 8 + sat6 -1 0.8045547 629 235.3724 -618.1977 ## 9 + sat4 -1 1.0320596 628 234.3403 -618.9969 Første modell er kun intercept. Vi ser på AIC verdiene i tabellen over. AIC er forkortelse for Akaike Information Criterion som er en estimator for prediksjonsfeil og brukes som et mål på relativ kvalitet for statistiske modeller (hvor godt modellen passer til dataene modellen ble laget av - derfor er AIC egnet til å sammenlikne ulike mulige modeller fra det samme datasettet). Her skal vi legge merke til at fortegnet til AIC verdien er irrelevant - vi ser på absoluttverdien, der lavere AIC verdi er bedre enn høyere AIC verdi. Neste steg er at alle mulige modeller med en prediktor testes, og prediktoren som produserer den laveste AIC verdien inkluderes i modellen. Det er i vårt tilfelle sat1. Neste steg er å teste alle modeller/kombinasjoner med to prediktorer, hvor den ene er sat1. Den neste prediktoren som inkluderes er da sat3 fordi det er prediktoren som gir lavest AIC av alle mulige kombinasjoner av modeller med to prediktorer. Neste steg innebærer å teste alle modeller/kombinasjoner med tre prediktorer, der den første er sat1 og den andre er sat3. Da blir sat5 lagt til, osv. Modellen blir med denne prosedyren: # Base R magazine_forward$coefficients ## (Intercept) sat1 sat3 sat5 sat7 sat9 ## 3.78591494 0.17370968 0.17786494 0.19804309 0.14463696 0.10736930 ## sat2 sat6 sat4 ## 0.08912079 -0.05363069 0.04538282 For R-brukere kan vi kjøre hele analysen i en linje med pakken olsrr: # Bruker pakken: olsrr ols_step_forward_aic(magazine_all, progress = TRUE, details = FALSE) ## Forward Selection Method ## ------------------------ ## ## Candidate Terms: ## ## 1 . sat1 ## 2 . sat2 ## 3 . sat3 ## 4 . sat4 ## 5 . sat5 ## 6 . sat6 ## 7 . sat7 ## 8 . sat8 ## 9 . sat9 ## ## ## Variables Entered: ## ## - sat1 ## - sat3 ## - sat5 ## - sat7 ## - sat9 ## - sat2 ## - sat6 ## - sat4 ## ## No more variables to be added. ## ## Final Model Output ## ------------------ ## ## Model Summary ## ------------------------------------------------------------- ## R 0.621 RMSE 0.611 ## R-Squared 0.385 Coef. Var 8.151 ## Adj. R-Squared 0.377 MSE 0.373 ## Pred R-Squared 0.367 MAE 0.477 ## ------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## ## ANOVA ## -------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## -------------------------------------------------------------------- ## Regression 146.890 8 18.361 49.206 0.0000 ## Residual 234.340 628 0.373 ## Total 381.231 636 ## -------------------------------------------------------------------- ## ## Parameter Estimates ## --------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## --------------------------------------------------------------------------------------- ## (Intercept) 3.786 0.229 16.546 0.000 3.337 4.235 ## sat1 0.174 0.032 0.197 5.439 0.000 0.111 0.236 ## sat3 0.178 0.030 0.218 5.865 0.000 0.118 0.237 ## sat5 0.198 0.046 0.151 4.317 0.000 0.108 0.288 ## sat7 0.145 0.032 0.159 4.479 0.000 0.081 0.208 ## sat9 0.107 0.029 0.138 3.665 0.000 0.050 0.165 ## sat2 0.089 0.042 0.086 2.142 0.033 0.007 0.171 ## sat6 -0.054 0.031 -0.063 -1.725 0.085 -0.115 0.007 ## sat4 0.045 0.027 0.062 1.663 0.097 -0.008 0.099 ## --------------------------------------------------------------------------------------- ## ## Selection Summary ## ------------------------------------------------------------------- ## Variable AIC Sum Sq RSS R-Sq Adj. R-Sq ## ------------------------------------------------------------------- ## sat1 1353.907 71.744 309.487 0.18819 0.18691 ## sat3 1282.050 105.625 275.605 0.27706 0.27478 ## sat5 1233.759 126.547 254.684 0.33194 0.32878 ## sat7 1210.898 136.296 244.935 0.35751 0.35345 ## sat9 1195.690 142.824 238.407 0.37464 0.36968 ## sat2 1191.704 145.054 236.177 0.38049 0.37459 ## sat6 1191.530 145.858 235.372 0.38260 0.37573 ## sat4 1190.731 146.890 234.340 0.38531 0.37748 ## ------------------------------------------------------------------- 1.6.5 Analyse av resultatene (diagnostikk) - backward # Bruker pakken: olsrr ols_step_backward_aic(magazine_all, progress = TRUE, details = FALSE) ## Backward Elimination Method ## --------------------------- ## ## Candidate Terms: ## ## 1 . sat1 ## 2 . sat2 ## 3 . sat3 ## 4 . sat4 ## 5 . sat5 ## 6 . sat6 ## 7 . sat7 ## 8 . sat8 ## 9 . sat9 ## ## ## Variables Removed: ## ## - sat8 ## ## No more variables to be removed. ## ## Final Model Output ## ------------------ ## ## Model Summary ## ------------------------------------------------------------- ## R 0.621 RMSE 0.611 ## R-Squared 0.385 Coef. Var 8.151 ## Adj. R-Squared 0.377 MSE 0.373 ## Pred R-Squared 0.367 MAE 0.477 ## ------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## ## ANOVA ## -------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## -------------------------------------------------------------------- ## Regression 146.890 8 18.361 49.206 0.0000 ## Residual 234.340 628 0.373 ## Total 381.231 636 ## -------------------------------------------------------------------- ## ## Parameter Estimates ## --------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## --------------------------------------------------------------------------------------- ## (Intercept) 3.786 0.229 16.546 0.000 3.337 4.235 ## sat1 0.174 0.032 0.197 5.439 0.000 0.111 0.236 ## sat2 0.089 0.042 0.086 2.142 0.033 0.007 0.171 ## sat3 0.178 0.030 0.218 5.865 0.000 0.118 0.237 ## sat4 0.045 0.027 0.062 1.663 0.097 -0.008 0.099 ## sat5 0.198 0.046 0.151 4.317 0.000 0.108 0.288 ## sat6 -0.054 0.031 -0.063 -1.725 0.085 -0.115 0.007 ## sat7 0.145 0.032 0.159 4.479 0.000 0.081 0.208 ## sat9 0.107 0.029 0.138 3.665 0.000 0.050 0.165 ## --------------------------------------------------------------------------------------- ## ## ## Backward Elimination Summary ## -------------------------------------------------------------------- ## Variable AIC RSS Sum Sq R-Sq Adj. R-Sq ## -------------------------------------------------------------------- ## Full Model 1191.783 233.992 147.239 0.38622 0.37741 ## sat8 1190.731 234.340 146.890 0.38531 0.37748 ## -------------------------------------------------------------------- 1.6.6 Analyse av resultatene (diagnostikk) - bidirectional # Bruker pakken: olsrr ols_step_both_aic(magazine_all, progress = TRUE, details = FALSE) ## Stepwise Selection Method ## ------------------------- ## ## Candidate Terms: ## ## 1 . sat1 ## 2 . sat2 ## 3 . sat3 ## 4 . sat4 ## 5 . sat5 ## 6 . sat6 ## 7 . sat7 ## 8 . sat8 ## 9 . sat9 ## ## ## Variables Entered/Removed: ## ## - sat1 added ## - sat3 added ## - sat5 added ## - sat7 added ## - sat9 added ## - sat2 added ## - sat6 added ## - sat4 added ## ## No more variables to be added or removed. ## ## Final Model Output ## ------------------ ## ## Model Summary ## ------------------------------------------------------------- ## R 0.621 RMSE 0.611 ## R-Squared 0.385 Coef. Var 8.151 ## Adj. R-Squared 0.377 MSE 0.373 ## Pred R-Squared 0.367 MAE 0.477 ## ------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## ## ANOVA ## -------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## -------------------------------------------------------------------- ## Regression 146.890 8 18.361 49.206 0.0000 ## Residual 234.340 628 0.373 ## Total 381.231 636 ## -------------------------------------------------------------------- ## ## Parameter Estimates ## --------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## --------------------------------------------------------------------------------------- ## (Intercept) 3.786 0.229 16.546 0.000 3.337 4.235 ## sat1 0.174 0.032 0.197 5.439 0.000 0.111 0.236 ## sat3 0.178 0.030 0.218 5.865 0.000 0.118 0.237 ## sat5 0.198 0.046 0.151 4.317 0.000 0.108 0.288 ## sat7 0.145 0.032 0.159 4.479 0.000 0.081 0.208 ## sat9 0.107 0.029 0.138 3.665 0.000 0.050 0.165 ## sat2 0.089 0.042 0.086 2.142 0.033 0.007 0.171 ## sat6 -0.054 0.031 -0.063 -1.725 0.085 -0.115 0.007 ## sat4 0.045 0.027 0.062 1.663 0.097 -0.008 0.099 ## --------------------------------------------------------------------------------------- ## ## ## Stepwise Summary ## ------------------------------------------------------------------------------ ## Variable Method AIC RSS Sum Sq R-Sq Adj. R-Sq ## ------------------------------------------------------------------------------ ## sat1 addition 1353.907 309.487 71.744 0.18819 0.18691 ## sat3 addition 1282.050 275.605 105.625 0.27706 0.27478 ## sat5 addition 1233.759 254.684 126.547 0.33194 0.32878 ## sat7 addition 1210.898 244.935 136.296 0.35751 0.35345 ## sat9 addition 1195.690 238.407 142.824 0.37464 0.36968 ## sat2 addition 1191.704 236.177 145.054 0.38049 0.37459 ## sat6 addition 1191.530 235.372 145.858 0.38260 0.37573 ## sat4 addition 1190.731 234.340 146.890 0.38531 0.37748 ## ------------------------------------------------------------------------------ Vi kan se at modellene gir likt resultat for alle tre måtene, men det trenger absolutt ikke hende. Vi kan få ulike modeller ved de tre metodene. Husk at inkludering/eksludering av variabler skjer rent matematisk. 1.6.6.1 Hvor god er modellen vår (goodness of fit)? Vi kan se at F-verdien er 49.206. # Base R qf(p=.05, df1=8, df2=628, lower.tail=FALSE) ## [1] 1.953131 F-verdien er dermed langt over kritisk verdi (p &lt; 0.001). Det ser derfor ut til at det er svært usannsynlig at forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si at dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten i modellen så vil F &gt; 1. 1.6.7 Konklusjon / oppsummering / rapportering av resultater Tabell 1: Deskriptiv statistikk # Bruker pakken: table1 table1::label(magazine_data$sat1) &lt;- &quot;sat1&quot; table1::label(magazine_data$sat2) &lt;- &quot;sat2&quot; table1::label(magazine_data$sat3) &lt;- &quot;sat3&quot; table1::label(magazine_data$sat4) &lt;- &quot;sat4&quot; table1::label(magazine_data$sat5) &lt;- &quot;sat5&quot; table1::label(magazine_data$sat6) &lt;- &quot;sat6&quot; table1::label(magazine_data$sat7) &lt;- &quot;sat7&quot; table1::label(magazine_data$sat8) &lt;- &quot;sat8&quot; table1::label(magazine_data$sat9) &lt;- &quot;sat9&quot; table1::label(magazine_data$satov) &lt;- &quot;satov&quot; table1::table1(~sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat8 + sat9 + satov, data = magazine_data) Overall(N=637) sat1 Mean (SD) 4.11 (0.879) Median [Min, Max] 4.00 [1.00, 6.00] sat2 Mean (SD) 4.37 (0.750) Median [Min, Max] 4.00 [2.00, 6.00] sat3 Mean (SD) 3.77 (0.950) Median [Min, Max] 4.00 [1.00, 6.00] sat4 Mean (SD) 4.00 (1.06) Median [Min, Max] 4.00 [1.00, 6.00] sat5 Mean (SD) 4.67 (0.591) Median [Min, Max] 5.00 [2.00, 6.00] sat6 Mean (SD) 4.41 (0.912) Median [Min, Max] 5.00 [1.00, 6.00] sat7 Mean (SD) 4.41 (0.853) Median [Min, Max] 5.00 [1.00, 5.00] sat8 Mean (SD) 3.91 (0.920) Median [Min, Max] 4.00 [2.00, 6.00] sat9 Mean (SD) 3.97 (0.992) Median [Min, Max] 4.00 [1.00, 6.00] satov Mean (SD) 7.49 (0.774) Median [Min, Max] 8.00 [5.00, 10.0] Tabell 2: Korrelasjonsmatrise # Bruker pakken: sjPlot magazine_data2 &lt;- magazine_data[, c(&quot;sat1&quot;, &quot;sat2&quot;, &quot;sat3&quot;, &quot;sat4&quot;, &quot;sat5&quot;, &quot;sat6&quot;, &quot;sat7&quot;, &quot;sat9&quot;, &quot;satov&quot;)] magazinekorr &lt;- tab_corr(magazine_data2, triangle = &quot;lower&quot;) magazinekorr   sat1 sat2 sat3 sat4 sat5 sat6 sat7 sat9 satov sat1                   sat2 0.366***                 sat3 0.341*** 0.421***               sat4 0.335*** 0.481*** 0.280***             sat5 0.202*** 0.217*** 0.154*** 0.116**           sat6 0.285*** 0.443*** 0.359*** 0.375*** 0.161***         sat7 0.288*** 0.298*** 0.193*** 0.221*** 0.389*** 0.213***       sat9 0.373*** 0.374*** 0.420*** 0.316*** 0.297*** 0.302*** 0.234***     satov 0.434*** 0.384*** 0.428*** 0.303*** 0.343*** 0.233*** 0.375*** 0.418***   Computed correlation used pearson-method with listwise-deletion. p &lt; .0001**** , p &lt; .001*** , p &lt; .01**, p &lt; .05* Tabell 3: Hierarkisk regresjonsanalyse av prediktorer for satov # Bruker pakken: sjPlot magazine_finalmod &lt;- lm(satov ~ sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat9, data = magazine_data) tab_model(magazine_finalmod)   satov Predictors Estimates CI p (Intercept) 3.79 3.34  4.24 &lt;0.001 sat1 0.17 0.11  0.24 &lt;0.001 sat2 0.09 0.01  0.17 0.033 sat3 0.18 0.12  0.24 &lt;0.001 sat4 0.05 -0.01  0.10 0.097 sat5 0.20 0.11  0.29 &lt;0.001 sat6 -0.05 -0.11  0.01 0.085 sat7 0.14 0.08  0.21 &lt;0.001 sat9 0.11 0.05  0.16 &lt;0.001 Observations 637 R2 / R2 adjusted 0.385 / 0.377 pacman::p_load(readxl, car, rgl, flextable, plotly, latex2exp, ggfortify, gridExtra, factoextra, corrplot, Directional, tidyverse, palmerpenguins, psych, paran, kableExtra, multiUS, xtable, GPArotation, EFAtools, nFactors, rstatix, calibrate, Matrix) "],["dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html", "2 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA) 2.1 Innledning 2.2 Principal Component Analysis (PCA) 2.3 Faktoranalyse", " 2 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA) 2.1 Innledning En faktoranalyse er en analyseteknikk som brukes for å forstå korrelasjonsstrukturen i et sett av observerte variabler (Bjerkan, 2007, p. s.221). I følge Mehmetoglu &amp; Mittner (2020) brukes disse statistiske teknikkene i praksis som metoder som reduserer et større antall variabler til et mindre antall variabler uten å miste vesentlig informasjon om dataene i prosessen. De omtales derfor ofte som datareduksjonsteknikker (data reduction eller dimension reduction). Ofte vil vi bruke faktoranalyse for å kunne si noe om såkalte latente (eller skjulte) variabler i samfunnsvitenskapene - forhold vi ikke kan måle direkte, men som vi kan uttrykke gjennom å måle/observere en rekke andre forhold/variabler som vi så samler i en konstruert variabel gjennom nettopp faktoranalyse. Målet med faktoranalysen er da i følge Tinsley &amp; Tinsley (1987) to achieve parsimony by using the smallest number of explanatory concepts to explain the maximum amount of common variance in a correlation matrix. Det vi ønsker å finne er variabler som er korrelerte med hverandre, men relativt ukorrelerte med andre grupper/subset av variabler (som igjen er interkorrelerte i egen gruppe/subset) (Tabachnik &amp; Fidell, 2007). Den grunnleggende utfordringen er at vi ønsker å representere et stort antall variabler på en enklere måte, men hvis vi velger for få faktorer mister vi informasjon (noe som går ut over påliteligheten) og hvis vi velger for mange kan vi ende opp med en modell som er komplisert og vanskelig å tolke. Innledningsvis er det nødvendig å gjøre en grunnleggende begrepsavklaring rundt begrepet faktoranalyse da det i mange sammenhenger framstår som om begreper blandes sammen og det kan være uklart hva man egentlig snakker om. Begrepene faktoranalyse og komponentanalyse (Factor Analysis - FA - og Principal Component Analysis - PCA) brukes ofte om hverandre og noen ganger er det uklart hva som er hva (eller i hvert fall hva forfatteren mener). En klargjørende framstilling kan man f.eks. finne i Jöreskog et al. (2016). Vi vil i det følgende skille mellom faktoranalyse (EFA og CFA, og komponentanalyse (PCA). Felles for begge er: de er metoder for data reduksjon de brukes for å uttrykke multivariate data gjennom færre dimensjoner enn det opprinnelige datasettet de er metodikker for å identifisere mønstre i korrelasjonen mellom variabler. En grunnleggende forskjell er at PCA er en deskriptiv teknikk mens faktoranalyse er modelleringsteknikker (Unkel &amp; Trendafilov, 2010). PCA blir dermed en empirisk oppsummering av datamaterialet (Bjerkan, 2007, p. s.225). Strengt tatt er ikke PCA en faktoranalyse, men omtales (dessverre noe forvirrende) som sagt ofte som en faktoranalyse. I SPSS heter f.eks. menypunktet «Data reduction» og man velger «Factor» i neste valg uansett hvilken av metodene (PCA/EFA/CFA) man skal bruke. Grafisk kan forskjellene vises slik: I venstre del (PCA) kombineres fire målte variabler (\\(X_1...X_4\\)) til en komponent (\\(C\\)). Pilene indikerer at det er variablene som bidrar til å skape komponenten, og de kan gjøre det med ulike styrke (vekt), som er vist med \\(w_1...w_4\\). Variablene \\(X_1...X_4\\) utgjør altså ulike størrelser på bidraget til komponenten \\(C\\). I figurens høyre del ser vi en faktor \\(F\\) som skaper de fire målte variablene (\\(Y_1...Y_4\\)). Dette vises ved at pilene går fra \\(F\\) til \\(Y_1...Y_4\\). \\(F\\) kan typisk være en latent variabel vi ikke kan observere direkte - som f.eks. intelligens eller angst. Også her er det ulike vekter, så \\(F\\) kan påvirke \\(Y_1...Y_4\\) med ulik styrke. I tillegg har vi her et feilledd (\\(e_1...e_4\\)). \\(e_1\\) representerer f.eks. den delen av variansen i \\(Y_1\\) som ikke forklares av \\(F\\). Vi kan uttrykke sammenhengen for en enkelt varaibel som \\(Y_1\\) som en regresjonslikning: \\(Y_1 = b_1*F + e_1\\) (og tilsvarende for \\(Y_2...Y_4\\)). Som illustrert i figuren under består variansen til variabelen X av tre deler: felles varians med andre variabler, unik varians for selve variabel X og målefeil. PCA søker å forklare all varians for variabel X som en komponent (derav komponentanalyse), mens faktoranalyse kun søker å forklare den delen av den totale variansen som er felles (eller med andre ord: korrelasjonen mellom variablene). Modifisert fra Bjerkan (2007), s. 225, fig. 10.1 Matematisk er forskjellen mellom faktoranalyse og PCA altså hvordan varians blir analysert  i PCA blir all varians analysert, i faktoranalyse blir kun delt varians («shared variance») analysert. Eller med andre ord: PCA analyserer all varians (felles varians, unik varians og målefeil), FA analyserer kovarians (variabelens felles varians med andre variabler). Teoretisk er forskjellen mellom de to at i FA ses faktoren som årsaker til variabelen, mens i PCA ses variablene som årsaken til komponentene; i PCA er det ingen teoretisk forventning om hvilke variabler som forbindes med hvilke komponenter  det er kun empirisk assosiert (Tabachnik &amp; Fidell, 2007). En annen måte å illustrere forskjellen mellom PCA og EFA er gitt av Bastos (2021). Fra Bastos (2021) I figuren over representerer Aene spesifikk varians, Bene felles varians og Cene feilvarians (jfr. figuren fra Bjerkan (2007) lenger opp). Mens vi i PCA vil bruke all varians (A, B og C) bruker vi kun B i EFA. Praktisk er det imidlertid ikke helt trivielt å avgjøre om man skal bruke PCA eller EFA. Guadagnoli &amp; Velicer (1988) konkluderer også i en litteraturundersøkelse med at resultatene fra PCA i stor grad er like som resultatene fra faktoranalyse. Med minst 30 variabler vil løsningene være mer eller mindre like, men med under 20 variabler kan forskjeller inntreffe (Stevens, 2002). Som Field (2009) oppsummerer: However, to a non-statistician the difference between a principal component and a factor may be difficult to conceptualize (they are both linear models), and the difference arises largely from the calculation (s.760). 2.2 Principal Component Analysis (PCA) Tabachnik &amp; Fidell (2007) foreslår at dersom du ønsker en «empirisk oppsummering» av datasettet og redusere et større antall variabler til et mindre antall komponenter er PCA riktig valg. I en PCA transformeres et antall korrelerte variabler til et mindre antall «principal components». 2.2.1 PCA gjennom et lite eksempel La oss se på hva PCA er. Vi tar utgangspunkt i et lite, kontruert datasett der vi har registrert karakterer i ulike fag på 10 studenter. Dette eksempelet er modifisert fra Starmer (2018), med innslag fra Pittard (2012). .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-18a52778{}.cl-1899c054{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1899dcf6{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-189a2ae4{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2ae5{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2ae6{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2ae7{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2ae8{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2ae9{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2aea{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2aeb{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2aec{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-189a2aed{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} studentnrmatte1792763784755426457418469501049 Vi kan plotte den ene variabelen - matte. Vi kan se at studentgruppen deler seg i to klare grupper, en gruppe med høy score og en gruppe med lavere score. Studentene i gruppa med høy score likner mer på hverandre enn på studenter i den andre gruppa, og motsatt. Hvis vi legger til en variabel - f.eks. engelskkarakterer - får vi denne tabellen: .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-18fdfb5a{}.cl-18e6df7e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-18e70670{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-18e76c5a{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c5b{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c5c{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c5d{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c5e{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c5f{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c60{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c61{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c62{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c63{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e76c64{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-18e7934c{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} studentnrmatteengelsk179792766837871475705425364555741628466095064104949 Som vi også kan plotte: Vi kan nå si vi har to dimensjoner for hver student: den første dimensjonen - x-aksen - inneholder mattekarakterer, den andre dimensjonen - y-aksen - inneholder engelskkarakterer. Vi kan se også med to dimensjoner at det er to tydelige clustere. Så kan vi legge til nok en karakter - denne gangen norsk. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-19587efe{}.cl-193fa424{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-193fa425{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-19404488{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19404489{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940448a{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940448b{width:47.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940448c{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940448d{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940448e{width:47.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940448f{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19404490{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19404491{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19404492{width:47.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940491a{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940491b{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940491c{width:47.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940491d{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1940491e{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} studentnrmatteengelsknorsk17979672766868378716947570665425348645554674162478466050950645110494949 Med tre dimensjoner må vi har tre akser i plottet: Hvis vi nå legger til fysikkarakterer i tillegg kan vi ikke lenger plotte dette siden det vil kreve fire dimensjoner. PCA stepper inn og lager et 2D plott av flere dimensjoner. PCA vil også kunne si oss noe om hvilken av karakterene (= hvilken av variablene) som er viktigst for å skape klyngene/grupperingene av studenter. 2.2.1.1 Sentrering Vi går tilbake til datasettet med to karakterer (dimensjoner). .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-1997a962{}.cl-1981f126{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-19828e10{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-19828e11{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e12{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e13{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e14{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e15{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e16{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e17{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e18{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e19{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19828e1a{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19832a3c{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-19832a3d{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} studentnrmatteengelsk179792766837871475705425364555741628466095064104949 Vi regner så ut gjennomsnittet for de to variablene matte og engelsk: ## [1] 58.1 ## [1] 63.1 Vi bruker disse gjennomsnittsverdiene til å kalkulere senterpunktet (x = 58.1, y = 63.1) som vi deretter - ved å beholde de innbyrdes avstandene i x- og y-planet mellom datapunktene - sentrerer alle observasjonene rundt. Vi ser at datapunktene ligger nøyaktig likt i forhold til hverandre, men senterpunktet er nå (0, 0). Neste steg er å lage en regresjonslinje som passer best mulig til dataene. Vi begynner med å legge på en hvilken som helst linje som går gjennom (0, 0) og deretter roterer vi linja med (0, 0) som pivoteringspunkt til man finner linja som passer best (i dette tilfellet den røde stiplede linja): Og hvordan vet PCA hvilken linje som passer best? For å se på det skal vi ta en liten omveg ut av eksempelet vårt. 2.2.1.2 Forskjell i utregning av avvik - OLS og PCA Med utgangspunkt i Long (2010) kan vi illustrere den prinsippielle forskjellen i hvordan hhv. OLS (lineær regresjon) og PCA kalkulerer beste tilpasning. I en OLS (jfr. teori i kapittel om regresjonsanalyse) vil man søke å finne beste tilpasning: OLS forsøker å minimere feilleddet mellom den avhengige variabelen og modellen ved å regne på alle avstandene (og kvadrere dem) mellom datapunktene og modellen. I grafen over er dette illustrert med oransje strek for to av datapunktene. Hvis vi bytter om på den avhengige og uavhengige variabelen ser det i OLS slik ut for to av datapunktene: OLS forsøker alltid minimere y-avstanden (feilleddet = \\(y-\\hat{y}\\)). PCA vil minimere feilleddet ortogonalt (90\\(^\\circ\\) på modellen): PCA vil rotere modellen (regresjonslinja) rundt et senterpunkt og hele tiden kalkulere summen av de ortogonale feilleddene. Når du har en litt større mengde uavhengige variabler vil PCA gi deg hvilke lineære kombinasjoner som teller mest. En snasen forklaring og illustrasjon kan de se her Å gå i dybden på den matematiske utregningen av eigenvalues og eigenvectors er på grensen av dybdekunnskap i et notat med tittel Anvendt i seg, og trolig kan man leve godt uten denne dybdekunnskapen. En anbefalt kilde for å gå i dybden kan være Smith (2002). Likevel vil vi skissere under hvordan dette gjøres i det enkle eksempelet. 2.2.2 PCA gjennom et lite eksempel - del 2 Vi tar utgangspunkt i eksempelet der vi hadde to variabler som er sentrert: .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-1a778280{}.cl-1a6a8c2e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1a6ab33e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1a6b041a{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b041b{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b041c{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b041d{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b041e{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b041f{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b0420{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b0421{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b0422{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b0423{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b0424{width:48pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a6b2abc{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} studentnrmatteengelsk179792766837871475705425364555741628466095064104949 Vi legger på en tilfeldig linje som går gjennom (0, 0). Ettersom vi roterer linja vil avstanden \\(a\\) ikke forandre seg. Lengden på både \\(b\\) og \\(c\\) vil imidlertid endre seg relativt til hverandre. Når \\(b\\) blir lengre, blir \\(c\\) kortere og motsatt. I samme tanke som OLS-regresjon (se ovenfor) ønsker vi at \\(b\\) skal være så kort som mulig da det betyr at linja ligger så nærme datapunktet som mulig. For å få \\(b\\) så kort som mulig jobber iidlertid PCA for å maksimere \\(c\\) (det har samme effekt: når \\(c\\) er på sitt maksimale er \\(b\\) på sitt minimale). PCA finner altså den beste linja ved å maksimere den kvadrerte avstanden \\(c\\) (kvadrert pga. Pythagoras \\(a^2 + b^2 = c^2\\)). PCA summerer da \\(c^2\\) for alle de 10 punktene i dette eksempelet. Siden verdiene er kvadrerte slipper vi også problemet med at positive og negative verdier nuller hverandre ut (som for OLS - Ordinary Least Squares selv om vi her ikke regner squares men avstand). Summen = sum of squared distances SS(distances). 2.2.2.1 Identifikasjon av Principal Component 1 (PC1), eigenvectors og loading scores PCA roterer som sagt på linja pivotert i (0, 0) og til slutt finner den linja som maksimaliserer SS(Distances). Denne linja kalles Principal Component 1 (PC1). PC1 har et stigningstall på 0.4639586. Det betyr at en økning på 1 i x (matte) gir en økning på 0.46 i y (engelsk). Det betyr at matte har større innvirkning på grupepringen av studentene enn engelsk. I PCA skaleres \\(c\\) alltid til 1. Matematisk gjør vi det ved å dele hver side av Pythagoras med \\(c\\) (her 1.1). Disse verdiene - 0.91 og 0.42 - kalles Eigenvector for PC1, og de to verdiene kalles gjerne Loading Scores. Som vist i grafen over er \\(SS(Distances) = Eigenvalue for PC1\\). Eigenvalue er et begrep vi kommer tilbake til i PCA når vi skal velge ut hvor mange komponenter vi skal beholde. Det neste vi kan se på da er Principal Component 2 (PC2). PC2 er linja som går vinkelrett på PC1 og gjennom (0, 0). PC2 er altså linja som reflekterer den nest største kilden til variasjon i dataene, men som er ortogonal (vinkelrett) på PC1. Dette betyr at eigenvectoren er snudd og blir -0.42 og 0.91 (som altså er loading scores for PC2). For å få fram det endelige PCA-plottet roteres løsningen slik at PC1 utgjør x-aksen og PC2 y-aksen. Punktene plottes deretter på det roterte diagrammet. Til dette brukes trigonometri. La oss se på et enkelt eksempel for ett punkt - dette gjør vi selvsagt ikke manuelt. Vi har det opprinnelige plottet i et koordinatsystem og har funnet PC1 og PC2, og snur først aksene. Så må datapunktene posisjoneres etter de roterte aksene. La oss vise med ett punkt (1,0) - altså punktet 1 på x-aksen, 0 på y-aksen. Punkt (1,0) blir da - forutsatt \\(\\alpha=30\\): \\(y&#39;= sin(\\alpha) = 0.5\\) \\(x&#39;= cos(\\alpha) = 0.9\\) (1,0) blir da rotert (0.5, 0.9). Slik gjør man for alle datapunkter. 2.2.2.2 Litt mer i detalj om rotasjon Det vi har gjort ovenfor med et lite eksempel kjøres på hele datasettet i det verktøyet vi bruker (R, Stata, SPSS, osv.). Med utgangspunkt i en datamatrise roteres dataene i en gitt vinkel gjennom en rotasjonsmatrise til et ny datamatrise (PC1). Denne an igjen roteres på samme måte til PC2. Vi kan se på komponentene: # Base R ut1 &lt;- princomp(karaktererX2, cor = T) summary(ut1) ## Importance of components: ## Comp.1 Comp.2 ## Standard deviation 1.3533352 0.41046786 ## Proportion of Variance 0.9157581 0.08424193 ## Cumulative Proportion 0.9157581 1.00000000 Og matrisen etter en rotasjon som vi kan plotte (som vi kjenner igjen fra litt lenger oppe): # Base R utdata &lt;- ut1$score utdata ## Comp.1 Comp.2 ## [1,] 2.2283619 -0.34268382 ## [2,] 1.2036707 0.41133586 ## [3,] 1.5364460 0.25900821 ## [4,] 1.3202598 0.20452296 ## [5,] -1.5428918 0.09028815 ## [6,] -1.2458551 0.06392291 ## [7,] -0.8603493 -0.68247820 ## [8,] -0.7964907 -0.29521764 ## [9,] -0.2926411 -0.43817195 ## [10,] -1.5505104 0.72947352 plot(utdata) 2.2.2.3 Utvelgelse av antall komponenter Et sentralt element i PCA er valg av antall komponenter man vil beholde. Dette kan vi gjøre gjennom variansen for komponentnen (her: PC1 og PC2). Her kommer vi tilbake til begrepet eigenvalues som vi definerte lenger opp. Vi kan se på eigenvalue for en PC som hvor mange variabler som representeres av den respektive PC. Eigenvalues forholder seg til forklart varians slik: \\(Forklart\\ varians = \\frac{Eigenvalue}{antall\\ opprinnelige\\ variabler}\\) Dette kan også uttrykkes slik: \\(\\frac{SS(Distances for PC1)}{n - 1} = Varians for PC1\\) \\(\\frac{SS(Distances for PC2)}{n - 1} = Varians for PC2\\) Et viktig poeng er at en PCA ikke reduserer antall variabler i seg selv. Hvis du opprinnelig har 13 variabler vil du få 13 PC, men spørsmålet er hvor mange du faktisk trenger å se på for å forklare variansen (hvilket du ønsker skal være færre enn det opprinnelige antall variabler). Og det er her vi kommer fram til hele poenget med PCA: Hvir mange komponenter skal vi beholde? For å vise uilke metoder for å vurdere antall komponenter som bør beholdes bruker vi et datasett med flere variabler enn vårt eksempel ovenfor, modifisert fra DataViz (2020)) - vi går ikke inn på hva dataene er. # Bruker pakken: palmerpenguins penguins_data &lt;- penguins[,c(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, &quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;, &quot;year&quot;)] penguins_data &lt;- na.omit(penguins_data) 2.2.2.3.1 Kaisers kriterium Kaiser (1960) kriterium baserer seg på å beholde alle komponenter med eigenvalue på over 1,0. Enhver komponent med eigenvalue over 1 forklarer mer varianse enn en enkeltvariabel. Med andre ord  ut fra denne måten å vurdere ønsker vi ikke å beholde komponenter som forklarer mindre varians enn enkeltvariabler. Det er generelt anbefalt at man ikke bruker Kaisers kriterium alene da metoden har en tendens til å overvurdere antallet komponenter. Samtidig hevdes det at det er umulig å tillegge en komponent med verdi 1,01 som viktig og en annen med verdi 0,99 som uviktig (Fabrigar et al., 1999). # Base R pca_obj &lt;- prcomp(drop_na(penguins_data), scale. = TRUE) pca_obj_eigen &lt;- ((pca_obj$sdev)^2) pca_obj_eigen ## [1] 2.77008681 0.99348866 0.77191746 0.36520940 0.09929767 Ut fra dette bør vi beholde 1 komponent, men vi ser at komponent 2 er svært nærme 1. 2.2.2.3.2 Scree plott Et hjelpemiddel i dette er scree plot (Cattell, 1966). Et Scree Plot er en grafisk framstilling av komponentene langs x-aksen og de korresponderende eigenvalues på y-aksen. Et scree plot viser hvor stor del av variansen variabelen forklarer rundt f.eks. PC1. Når vi vurderer Scree Plot ønsker vi å identifisere knekkpunktet (også kalt «albuen»). La oss forutsette at variansen for PC1 = 13 og for PC2 = 4. Dvs. at den totale variansen = 17. Videre beytr det at PC1 forklarer \\(\\frac{13}{17}=0.765\\) - altså 76.5% av den totale variansen. PC2 forklarer på sin side \\(\\frac{4}{17}=0.235\\). Et scree plot er en grafisk framstilling av denne variansen. I vårt eksempel kan vi dermed fremstille dette scree plottet: # Bruker pakken: psych var_explained_df &lt;- data.frame(PC= paste0(&quot;PC&quot;,1:5), var_explained=(pca_obj$sdev)^2/sum((pca_obj$sdev)^2)) var_explained_df %&gt;% ggplot(aes(x=PC,y=var_explained, group=1))+ geom_point(size=4)+ geom_line()+ labs(title=&quot;Scree plott&quot;) summary(pca_obj) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 ## Standard deviation 1.664 0.9967 0.8786 0.60433 0.31512 ## Proportion of Variance 0.554 0.1987 0.1544 0.07304 0.01986 ## Cumulative Proportion 0.554 0.7527 0.9071 0.98014 1.00000 Cattell (1966) beskriver framgangsmåten som at man finner albuen og dropper alle komponenter etter komponenten som starter albuen (eller sagt på en annen måte: vi beholder alle komponentene over knekkpunktet). I vårt tilfelle indikerer det at vi beholder 1 komponent. I vårt scree plott er det et tydelig knekkpunkt, men i mange tilfeller er det ikke så tydelig, og det kan være vanskelig å identifisere «det rette» knekkpunktet. En alternativ, og ofte brukt meetode for å illlustrere et scree plot på, er å kombinere det med et histogram (eksempel fra Szczsna (2022)). # Bruker pakken: factoextra fviz_eig(pca_obj, col.var=&quot;blue&quot;) 2.2.2.3.3 Parallell analyse Parallell anlayse (PA) (Horn, 1965) sammenlikner korrelasjonsmatrisen fra våre data med tilfeldig genererte korrelasjonsmatriser med samme antall variabler og observasjoner, for å sammenlikne eigenvalues for de genererte med den observerte. I eksempelet under ber vi om 5000 tilfeldige korrelasjonsmatriser. # Bruker pakken: paran paran(penguins_data, iterations=5000) ## ## Using eigendecomposition of correlation matrix. ## Computing: 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## ## ## Results of Horn&#39;s Parallel Analysis for component retention ## 5000 iterations, using the mean estimate ## ## -------------------------------------------------- ## Component Adjusted Unadjusted Estimated ## Eigenvalue Eigenvalue Bias ## -------------------------------------------------- ## 1 2.617826 2.770086 0.152260 ## -------------------------------------------------- ## ## Adjusted eigenvalues &gt; 1 indicate dimensions to retain. ## (1 components retained) 2.2.3 Eksempel PCA En anvendelse av PCA kan være at vi ønsker å se på den underliggende strukturen for en skalavariabel. Her skal vi bruke Pallants datasett som vi også brukte i deler av kapittelet om regresjonsanalyse som du finner her. For PCA-eksempelet skal vi se på en av skalaene - PANAS - og har modifisert datasettet til å kun inneholde spørsmålene knyttet til denne skalaen. Download Pallant_survey_PANAS.xlsx Download Pallant_survey_PANAS.sav Download Pallant_survey_PANAS.dta # Base R # Bruker pakken: readxl Pallant_survey_PANAS &lt;- as.data.frame(read_excel(&quot;Pallant_survey_PANAS.xlsx&quot;)) Pallant_survey_PANAS &lt;- na.omit(Pallant_survey_PANAS) brief(Pallant_survey_PANAS) ## 435 x 20 data.frame (430 rows and 5 columns omitted) ## pn1 pn2 pn3 pn4 pn5 pn6 pn7 pn8 pn9 pn10 pn11 pn12 pn13 . . . pn19 pn20 ## [n] [n] [n] [n] [n] [n] [n] [n] [n] [n] [n] [n] [n] [n] [n] ## 1 4 5 4 5 3 3 3 2 5 4 4 4 4 4 4 ## 2 5 5 3 5 2 5 5 5 5 5 5 5 4 3 1 ## 3 1 1 1 1 1 1 1 2 3 1 1 1 1 1 1 ## . . . ## 435 4 3 2 3 1 5 5 3 4 1 1 5 5 1 1 ## 439 4 2 1 5 1 2 4 2 3 1 1 2 4 1 1 Datasettet består av 20 spørsmål som utgjør PANAS skalaen (the Positive and Negative Affect Schedule). # Base R resultat.pca &lt;- prcomp(Pallant_survey_PANAS, scale = TRUE) summary(resultat.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.4973 1.8443 1.1063 1.07642 0.94816 0.88714 0.8556 ## Proportion of Variance 0.3118 0.1701 0.0612 0.05793 0.04495 0.03935 0.0366 ## Cumulative Proportion 0.3118 0.4819 0.5431 0.60104 0.64599 0.68534 0.7219 ## PC8 PC9 PC10 PC11 PC12 PC13 PC14 ## Standard deviation 0.81001 0.80669 0.77118 0.76610 0.70723 0.70109 0.62728 ## Proportion of Variance 0.03281 0.03254 0.02974 0.02935 0.02501 0.02458 0.01967 ## Cumulative Proportion 0.75475 0.78729 0.81702 0.84637 0.87138 0.89595 0.91563 ## PC15 PC16 PC17 PC18 PC19 PC20 ## Standard deviation 0.61274 0.57483 0.54724 0.53320 0.47228 0.41805 ## Proportion of Variance 0.01877 0.01652 0.01497 0.01422 0.01115 0.00874 ## Cumulative Proportion 0.93440 0.95092 0.96589 0.98011 0.99126 1.00000 2.2.3.1 Valg av antall komponenter 2.2.3.1.1 Kaisers kriterium og eigenvalues # Base R eig.val &lt;- as.data.frame(get_eigenvalue(resultat.pca)) eig.val ## eigenvalue variance.percent cumulative.variance.percent ## Dim.1 6.2365752 31.1828761 31.18288 ## Dim.2 3.4015750 17.0078752 48.19075 ## Dim.3 1.2239359 6.1196796 54.31043 ## Dim.4 1.1586859 5.7934294 60.10386 ## Dim.5 0.8990120 4.4950602 64.59892 ## Dim.6 0.7870241 3.9351204 68.53404 ## Dim.7 0.7320491 3.6602453 72.19429 ## Dim.8 0.6561184 3.2805918 75.47488 ## Dim.9 0.6507440 3.2537200 78.72860 ## Dim.10 0.5947166 2.9735829 81.70218 ## Dim.11 0.5869064 2.9345320 84.63671 ## Dim.12 0.5001778 2.5008892 87.13760 ## Dim.13 0.4915331 2.4576653 89.59527 ## Dim.14 0.3934826 1.9674130 91.56268 ## Dim.15 0.3754471 1.8772354 93.43992 ## Dim.16 0.3304249 1.6521243 95.09204 ## Dim.17 0.2994713 1.4973567 96.58940 ## Dim.18 0.2843020 1.4215102 98.01091 ## Dim.19 0.2230511 1.1152553 99.12616 ## Dim.20 0.1747675 0.8738377 100.00000 Ut fra Kaisers kriterium beholder vi fire komponenter. 2.2.3.1.2 Scree plott # Bruker pakken: factoextra fviz_eig(resultat.pca, addlabels = TRUE) Dette skulle indikere at vi beholder to komponenter. 2.2.3.1.3 Parallell analyse # Bruker pakken: paran paran(Pallant_survey_PANAS, iterations=5000) ## ## Using eigendecomposition of correlation matrix. ## Computing: 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## ## ## Results of Horn&#39;s Parallel Analysis for component retention ## 5000 iterations, using the mean estimate ## ## -------------------------------------------------- ## Component Adjusted Unadjusted Estimated ## Eigenvalue Eigenvalue Bias ## -------------------------------------------------- ## 1 5.836032 6.236575 0.400542 ## 2 3.074520 3.401575 0.327054 ## -------------------------------------------------- ## ## Adjusted eigenvalues &gt; 1 indicate dimensions to retain. ## (2 components retained) Dette peker også mot at vi bør beholde to komponenter. 2.2.3.2 PCA låst til to komponenter # Bruker pakken: psych pca2 &lt;- psych::principal(Pallant_survey_PANAS, nfactors=2, scores = TRUE, rotate = &quot;varimax&quot;) pca2 ## Principal Components Analysis ## Call: psych::principal(r = Pallant_survey_PANAS, nfactors = 2, rotate = &quot;varimax&quot;, ## scores = TRUE) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## pn1 0.70 -0.14 0.50 0.50 1.1 ## pn2 -0.15 0.70 0.52 0.48 1.1 ## pn3 -0.11 0.73 0.54 0.46 1.0 ## pn4 0.54 -0.12 0.31 0.69 1.1 ## pn5 -0.12 0.49 0.26 0.74 1.1 ## pn6 0.61 0.02 0.38 0.62 1.0 ## pn7 0.62 -0.25 0.45 0.55 1.3 ## pn8 -0.16 0.73 0.55 0.45 1.1 ## pn9 0.66 -0.18 0.47 0.53 1.2 ## pn10 -0.01 0.60 0.35 0.65 1.0 ## pn11 -0.15 0.65 0.44 0.56 1.1 ## pn12 0.76 -0.04 0.58 0.42 1.0 ## pn13 0.72 -0.12 0.54 0.46 1.1 ## pn14 -0.11 0.73 0.55 0.45 1.0 ## pn15 0.68 0.02 0.46 0.54 1.0 ## pn16 -0.10 0.58 0.35 0.65 1.1 ## pn17 0.82 -0.12 0.69 0.31 1.0 ## pn18 0.74 -0.15 0.57 0.43 1.1 ## pn19 -0.04 0.79 0.62 0.38 1.0 ## pn20 -0.08 0.71 0.51 0.49 1.0 ## ## RC1 RC2 ## SS loadings 4.89 4.75 ## Proportion Var 0.24 0.24 ## Cumulative Var 0.24 0.48 ## Proportion Explained 0.51 0.49 ## Cumulative Proportion 0.51 1.00 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.07 ## with the empirical chi square 830.29 with prob &lt; 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000025 ## ## Fit based upon off diagonal values = 0.95 Vi ønsker å se på modellen med to komponenter på tre parametere. Først ønsker vi at mindre enn 50% av residualene skal ha absoluttverdi &gt; 0.05. # Base R antall_over &lt;- length(pca2$residual[pca2$residual&gt;0.05]) antall_residualverdier &lt;- nrow(pca2$residual)*ncol(pca2$residual) (antall_over/antall_residualverdier)*100 ## [1] 12.5 I vårt tilfelle er 12.5% av residualene &gt; 0.05. Den neste parameteren er model fit som bør være &gt; 0.9. # Base R pca2$fit ## [1] 0.8705564 Her er verdien 0.8705564. Til slutt ser vi på communalities. # Base R sort(pca2$communality) ## pn5 pn4 pn16 pn10 pn6 pn11 pn7 pn15 ## 0.2574873 0.3064322 0.3512776 0.3542041 0.3751836 0.4394038 0.4503231 0.4608460 ## pn9 pn1 pn20 pn2 pn13 pn3 pn14 pn8 ## 0.4746693 0.5034580 0.5064648 0.5160759 0.5377154 0.5422017 0.5471146 0.5534260 ## pn18 pn12 pn19 pn17 ## 0.5717316 0.5848691 0.6201048 0.6851615 J. Pallant (2010) foreslår å se etter verdier på under 0.3. En lav verdi indikerer at den respektive variabelen ikke passer godt sammen med de andre variablene i sin respektive komponent. Man kan vurdere å se om modellen blir bedre ved å ta vekk variabler med lav verdi (f.eks. under 0.3). I vårt tilfelle er variabelen pn5 under terskelverdien på 0.3. Vi kan prøve å ta den bort. Fra før ser vi at modellen forklarer 48% (se Cumulative Var i tabellen). # Base R Pallant_survey_PANAS2 &lt;- subset(Pallant_survey_PANAS, select = -(pn5)) pca3 &lt;- psych::principal(Pallant_survey_PANAS2, nfactors=2, scores = TRUE, rotate = &quot;varimax&quot;) pca3 ## Principal Components Analysis ## Call: psych::principal(r = Pallant_survey_PANAS2, nfactors = 2, rotate = &quot;varimax&quot;, ## scores = TRUE) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## pn1 0.70 -0.15 0.51 0.49 1.1 ## pn2 -0.14 0.71 0.53 0.47 1.1 ## pn3 -0.11 0.74 0.56 0.44 1.0 ## pn4 0.54 -0.11 0.31 0.69 1.1 ## pn6 0.61 0.02 0.38 0.62 1.0 ## pn7 0.62 -0.25 0.45 0.55 1.3 ## pn8 -0.15 0.74 0.57 0.43 1.1 ## pn9 0.66 -0.18 0.47 0.53 1.1 ## pn10 -0.01 0.60 0.36 0.64 1.0 ## pn11 -0.14 0.65 0.45 0.55 1.1 ## pn12 0.76 -0.05 0.58 0.42 1.0 ## pn13 0.72 -0.12 0.54 0.46 1.1 ## pn14 -0.11 0.74 0.56 0.44 1.0 ## pn15 0.68 0.02 0.46 0.54 1.0 ## pn16 -0.10 0.55 0.31 0.69 1.1 ## pn17 0.82 -0.13 0.68 0.32 1.0 ## pn18 0.74 -0.15 0.57 0.43 1.1 ## pn19 -0.03 0.80 0.64 0.36 1.0 ## pn20 -0.08 0.72 0.52 0.48 1.0 ## ## RC1 RC2 ## SS loadings 4.87 4.55 ## Proportion Var 0.26 0.24 ## Cumulative Var 0.26 0.50 ## Proportion Explained 0.52 0.48 ## Cumulative Proportion 0.52 1.00 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.07 ## with the empirical chi square 708.34 with prob &lt; 0.00000000000000000000000000000000000000000000000000000000000000000000000000000061 ## ## Fit based upon off diagonal values = 0.95 Vi ser ingen forbedring i kumulativ varians forklart. # Base R pca2$fit ## [1] 0.8705564 pca3$fit ## [1] 0.8785797 Model fit er marginalt bedre. 2.2.3.3 Forutsetninger Så langt har vi ikke sett på hvilke forutsetninger som må ligge til grunn for å kunne kjøre en PCA. Det skal vi nå. 2.2.3.3.1 Størrelse på datasettet/utvalgsstørrelse Antall cases (sample size) og forholdstallet mellom antall respondenter og antall variabler kan være av betydning for en faktoranalyse. I små utvalg er korrelasjonskoeffisientene mellom variablene mindre pålitelige/konsistente. Det finnes ulike anbefalinger, og Hogarty et al. (2005) hevder at det ikke finnes et minimum hva angår \\(N\\) og \\(\\frac{N}{variabler}\\) for å oppnå en god faktoranalyse. Arrindell &amp; van der Ende (1985) fant at verken et bestemt forholdstall eller et minimumsantall observasjoner hadde påvirkning på faktorstabiliteten. Guadagnoli &amp; Velicer (1988) viste at en faktor med fire eller flere faktorladninger på 0,6 eller høyere er stabil uavhengig av utvalgsstørrelsen. En faktor med 10 eller flere ladninger større enn 0,4 var stabil dersom utvalgsstørrelsen er minst 150. Antallet caser kan imidlertid ses opp mot hvor sterkt variablene lader på faktorene (Tabachnik &amp; Fidell, 2007) og korrelasjonene (MacCallum et al., 1999)  høyere korrelasjoner (&gt;.80) krever mindre sample size (Guadagnoli &amp; Velicer, 1988). I vårt datasett har vi 435 caser/observasjoner. Vi får da forholdstallet 22.9. Antallet og forholdstallet skulle i utgangspunktet ikke være til hinder for en faktoranalyse her. Både Hair Jr. et al. (2010) og Nunally (1978) anbefaler et forholdstall på 10:1. ##### Sphericity - er datasettet faktoriserbart Vi gjennomfører to tester: Bartletts test for sfæritet og KMO. Først Bartletts: # Bruker pakken: psych korrelasjonsmatrise &lt;- cor(Pallant_survey_PANAS2) cortest.bartlett(korrelasjonsmatrise, n = nrow(Pallant_survey_PANAS2)) ## $chisq ## [1] 3781.425 ## ## $p.value ## [1] 0 ## ## $df ## [1] 171 Her ser vi at p-verdien er under terskelverdi på 0.05, så vi får dermed indikert at dataene er egnet for PCA etter dette kriteriet. Deretter KMO. KMO er en utregning som indikerer andelen av varians i skalavariabelen som kan forklares av de underliggende faktorene. En høy verdi indikerer at en faktoranalyse er mulig. # Bruker pakken: psych KMO(Pallant_survey_PANAS2) ## i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data. ## ## -- Kaiser-Meyer-Olkin criterion (KMO) ------------------------------------------ ## ## v The overall KMO value for your data is meritorious. ## These data are probably suitable for factor analysis. ## ## Overall: 0.876 ## ## For each variable: ## pn1 pn2 pn3 pn4 pn6 pn7 pn8 pn9 pn10 pn11 pn12 pn13 pn14 ## 0.936 0.863 0.795 0.947 0.884 0.950 0.889 0.936 0.820 0.868 0.893 0.899 0.800 ## pn15 pn16 pn17 pn18 pn19 pn20 ## 0.870 0.918 0.905 0.915 0.827 0.803 J. Pallant (2010) anbefaler en grenseverdi på 0.60. Kaiser (1974) anbefaler følgende retningslinjer for KMO-verdier: Verdi Tolkning 0.00-0.50 Uakseptabel (unacceptable) 0.50-0.60 Dårlig (miserable) 0.60-0.70 Middels (mediocre) 0.70-0.80 Middels (middling) 0.80-0.90 Respektabel (meritorius) 0.90-1.00 Fantastisk (marvelous) KMO er derfor respektabel. 2.2.3.3.2 Anti-image korrelasjonsmatrise # Bruker pakken: multiUS antiimage &lt;- antiImage(Pallant_survey_PANAS2)$AIR antiimage2 &lt;- as.matrix(diag(antiimage), row.names = FALSE) sort(antiimage2, decreasing = TRUE) ## [1] 0.9499796 0.9469195 0.9364141 0.9359988 0.9182026 0.9146945 0.9048130 ## [8] 0.8987604 0.8931080 0.8887645 0.8837913 0.8703518 0.8682341 0.8629180 ## [15] 0.8265146 0.8197264 0.8028766 0.8001013 0.7945513 I resultatet over har vi kun hentet ut de diagonale verdiene i anti-image matrisen (som er de vi er interessert i) og sortert disse i synkende rekkefølge. Disse verdiene er KMO verdier for de individuelle variablene. Disse bør ifølge Field (2009) være på over 0,5. Verdier under 0,5 kan bety at vi bør ta denne variabelen ut. # Base R corPallant_survey_PANAS2 &lt;- cor(Pallant_survey_PANAS2) corPallant_survey_PANAS2 &lt;- round(corPallant_survey_PANAS2, 2) length(corPallant_survey_PANAS2[corPallant_survey_PANAS2&gt;0.9 &amp; corPallant_survey_PANAS2&lt;1]) ## [1] 0 Siden denne matrisen blir stor har vi over kun hentet ut antall korrelasjonsverdier som er over 0.9 (og under 1.0 siden det i matrisen alltid vil være 1.0 i diagonalene - der variablene korrelerer med seg selv). Field (2009) peker på at ingen korrelasjo-ner bør være over 0.9. Samtidig bør det være godt med korrelasjoner over 0.3. Det finnes ingen absolutte krav til hvor mange/hvor stor andel av korrelasjonene som bør være over 0.3, men hvis man har få over 0.3 indikerer det at dataene kanskje ikke egner seg for PCA. I tabellen under har vi fjernet alle korrelasjonsverdier under +/- 0.3 og 1 for å gjøre det litt lettere å se. # Base R under03&lt;- as.data.frame(apply(corPallant_survey_PANAS2, 2, function(x) ifelse (abs(x) &gt; 0.3 &amp; (x) &lt; 1,x,&quot;&quot;))) under03 ## pn1 pn2 pn3 pn4 pn6 pn7 pn8 pn9 pn10 pn11 pn12 pn13 pn14 pn15 pn16 ## pn1 0.34 0.35 0.41 0.41 0.48 0.49 0.41 ## pn2 0.46 0.64 0.41 0.5 0.41 0.31 ## pn3 0.46 0.49 0.33 0.81 0.33 ## pn4 0.34 0.33 0.4 0.31 0.33 0.32 ## pn6 0.35 0.33 0.43 0.4 0.33 ## pn7 0.41 0.33 0.33 0.48 0.4 0.39 0.33 ## pn8 0.64 0.49 0.38 0.46 0.46 0.38 ## pn9 0.41 0.4 0.43 0.48 0.41 0.43 0.36 ## pn10 0.41 0.38 0.58 0.31 ## pn11 0.5 0.33 0.46 0.58 0.32 0.35 ## pn12 0.48 0.31 0.4 0.4 0.41 0.58 0.51 ## pn13 0.49 0.33 0.33 0.39 0.43 0.58 0.36 ## pn14 0.41 0.81 0.46 0.32 0.36 ## pn15 0.41 0.32 0.33 0.36 0.51 0.36 ## pn16 0.31 0.33 0.38 0.31 0.35 0.36 ## pn17 0.56 0.37 0.39 0.49 0.46 0.64 0.55 0.62 ## pn18 0.47 0.34 0.45 0.46 0.46 0.47 0.58 0.41 ## pn19 0.46 0.56 0.48 0.34 0.39 0.56 0.35 ## pn20 0.42 0.42 0.43 0.37 0.37 0.45 ## pn17 pn18 pn19 pn20 ## pn1 0.56 0.47 ## pn2 0.46 0.42 ## pn3 0.56 0.42 ## pn4 0.37 0.34 ## pn6 0.39 0.45 ## pn7 0.49 0.46 ## pn8 0.48 0.43 ## pn9 0.46 0.46 ## pn10 0.34 0.37 ## pn11 0.39 0.37 ## pn12 0.64 0.47 ## pn13 0.55 0.58 ## pn14 0.56 0.45 ## pn15 0.62 0.41 ## pn16 0.35 ## pn17 0.58 ## pn18 0.58 ## pn19 0.75 ## pn20 0.75 Det kan se ut som vi har et greit antall korrelasjoner over 0.3. 2.3 Faktoranalyse Det er flere teknikker assosiert med begrepet faktoranalyse, men i hovedsak kan vi dele disse inn i to typer: eksplorerende og konfirmerende (Hoyle, 2000; Hurley et al., 1997). Eksplorerende faktoranalyse som søker å gruppere variabler i et datasett ut fra korrelasjon uten spesifikke hypoteser på forhånd om hvordan strukturen ser ut. Denne tilnærmingen er således induktiv og i mindre grad drevet av teori - one can always subject a data set to an EFA but not necessarily a CFA (Schriesheim i Hurley et al., 1997, p. s.672). Konfirmerende faktoranalyse starter i andre enden - med forhåndshypoteser om dataene og strukturen. Her ønsker vi å bekrefte (konfirmere) en antatt datastruktur i vårt datasett, f.eks. ut fra teoretiske forventninger og tidligere undersøkelser. 2.3.1 Eksplorerende faktoranalyse (EFA) Hensikten med en ekspolerende faktoranalyse er altså å undersøke om vi har variabler som korrelerer med hverandre og se om disse kan grupperes på en meningsfull måte. Vi ser på graden av korrelasjon - variabler som er sterkt korrelerte grupperes og skilles fra andre som er mindre korrelerte (som igjen kan inneholde grupper av relativt sterkt korrelerte variabler). Målet er altså å få grupper av variabler som internt er sterkt korrelerte med hverandre, og lite korrelert med variabler utenfor gruppen. 2.3.1.1 Hva er en faktor? En faktor kan ses på som en skjult variabel - en variabel vi ikke kan observere eller måle direkte, men som påvirker flere andre synlige/målbare variabler. Variablene A, B, C osv er med andre ord observerbare/målbare fenomen for underliggende, skjulte faktorer. Når vi grupperer variabler som er høyt korrelerte antar vi at deres variasjon og korrelasjon skyldes den underliggende/skjulte variabelen. Et typisk eksempel er den såkalte five-factor model (McCrae &amp; John, 1992) som antar at personlighetstrekk kan grupperes i fem faktorer: Åpenhet, planmessighet, ekstroversjon, omgjengelighet og nevrotisisme (se f.eks. Kennair (2021) for en kort introduksjon til modellen på norsk). Man kan imidlertid ikke måle disse fem faktorene direkte, men man antar at de påvirker en rekke målbare fohold. Disse kan man spørre om/måle/observere. Faktoren planmessighet kan for eksempel påvirke spørsmål/atferd som Jeg er alltid forberedt, Jeg følger en plan eller Jeg utfører mine oppgaver med en gang de er gitt. La oss se på dette visuelt på en forenklet framstilling. Vi har en teoretisk modell, der vi sier at positive tilbakemeldinger på jobben predikerer jobbtilfredshet, økonomi predikerer tilfredshet i hjemmet, og tilsammen predikerer jobbtilfredshet og tilfredshet i hjemmet den totale personlige tilfredsheten: Dette er det vi teoretisk forventer og vår modell. Når vi samler data ser vi alltid at dataene (selvsagt) aldri passer perfekt inn i vår teoretiske modell. I figuren under er våre faktiske (empiriske) data fra vår undersøkelse representert gjennom de fargede sirklene som knyttes til sin respektive variabel. Så det vi faktisk ser - empirisk - er egentlig dette: Ikke alle målte variabler måler sterkest på den underliggende faktoret, det er overlapping mellom variablene og hva de måler (og det kan se langt verre ut enn figuren under). Faktoranalysen vil hjelpe oss i å rydde litt opp i dette, ved å se på hvilke variabler som faktisk (ikker teoretisk) korrelerer sterkt med hvilke, og hvilke som korrelerer svakt, for så å hjelpe oss strukturere modellen vi tester. I figuren over kan vi se at vi nok har en struktur av sterkt korrelerte variabler som måler sine underliggende faktorer, men vi ser også at det er en gruppe som teoretisk burde ligge nørmere sine respektive faktorer, men som ser ut til å klumpe seg i midten. Kanskje dette er en bedre representasjon? Her må vi trolig gå tilbake til vårt teoretiske utgangspunkt og spørreskjemate for å se på om vi har funnet en ny faktor, eller om vi skal utelate enkelte målinger/sørsmål for å få en bedre modell. 2.3.1.2 Eksempel Vi skal bruke et generert datasett for gjennomgang av eksplorerende faktoranalyse. Download fa_spm.xlsx Download fa_spm.sav Download fa_spm.dta # Base R # Bruker pakken: readxl fa_spm &lt;- as.data.frame(read_excel(&quot;fa_spm.xlsx&quot;)) brief(fa_spm) ## 366 x 9 data.frame (361 rows omitted) ## spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8 spm_9 ## [n] [n] [n] [n] [n] [n] [n] [n] [n] ## 1 1 1 1 1 1 1 1 1 1 ## 2 2 2 3 3 2 1 2 2 1 ## 3 1 4 4 4 4 4 1 1 1 ## . . . ## 365 1 1 2 3 4 4 4 2 4 ## 366 1 1 1 1 4 4 2 4 4 Vi kan se for oss at vi har stilt en rekke mennesker 9 spørsmål der de har svart på en skala fra 1-4. Det vi ønsker å se er om disse spørsmålene kan si noe om en eller flere latente variabler. De 9 spørsmålene er altså direkte målt, og vi vil se om vi kan si noe om latente variabler ut fra dette. Inngangsverderdiene i en faktoranalyse er korrelasjonsmatrisen som tygges (i programvare) til en struktur/et mønster. 2.3.1.3 Korrelasjonsmatrise Vi ser på korrelasjonsmatrisen. # Bruker pakken: rstatix fa_spm_cor &lt;- round(cor(fa_spm, use=&quot;complete.obs&quot;),2) pull_lower_triangle(fa_spm_cor, diagonal = FALSE) ## rowname spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8 spm_9 ## 1 spm_1 ## 2 spm_2 0.59 ## 3 spm_3 0.5 0.54 ## 4 spm_4 0.19 0.22 0.21 ## 5 spm_5 0.07 0.17 0.12 0.46 ## 6 spm_6 0.17 0.23 0.21 0.47 0.7 ## 7 spm_7 0.34 0.33 0.27 0.18 0.11 0.26 ## 8 spm_8 0.26 0.27 0.25 0.25 0.29 0.34 0.52 ## 9 spm_9 0.35 0.3 0.23 0.17 0.22 0.32 0.63 0.54 Henter fram antall korrelasjoner over 0.9. length(fa_spm_cor[fa_spm_cor&gt;0.9 &amp; fa_spm_cor&lt;1]) ## [1] 0 Videre tar vi vekk verdier under 0.3 og de som = 1: # Bruker pakken: rstatix under03x&lt;- as.data.frame(apply(fa_spm_cor, 2, function(x) ifelse (abs(x) &gt; 0.3 &amp; (x) &lt; 1,x,&quot;&quot;))) pull_lower_triangle(under03x, diagonal = FALSE) ## rowname spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8 spm_9 ## 1 spm_1 ## 2 spm_2 0.59 ## 3 spm_3 0.5 0.54 ## 4 spm_4 ## 5 spm_5 0.46 ## 6 spm_6 0.47 0.7 ## 7 spm_7 0.34 0.33 ## 8 spm_8 0.34 0.52 ## 9 spm_9 0.35 0.32 0.63 0.54 2.3.1.4 KMO og Bartletts # Bruker pakken: EFAtools KMO(fa_spm_cor) ## ## -- Kaiser-Meyer-Olkin criterion (KMO) ------------------------------------------ ## ## v The overall KMO value for your data is middling. ## These data are probably suitable for factor analysis. ## ## Overall: 0.779 ## ## For each variable: ## spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8 spm_9 ## 0.791 0.790 0.821 0.870 0.659 0.729 0.775 0.863 0.783 BARTLETT(fa_spm_cor, N = nrow(fa_spm)) ## ## v The Bartlett&#39;s test of sphericity was significant at an alpha level of .05. ## These data are probably suitable for factor analysis. ## ## &lt;U+0001D712&gt;²(36) = 1157.84, p &lt; .001 Vi bruker KMO og Bartletts til å se på faktoriserbarheten. Kaiser (1974) anbefaler 0.60 som cut-off verdi. Bartletts test er signifkant. Bartletts test sammenligner våre faktiske korrelasjonsmatrise med en identity matrix. Identity matrix er en konstruert korrelasjonsmatrise med verdi 1 i diagnoalen og 0 på alle andre korrelasjoner: ## [1] &quot;Identity Matrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1 0 0 0 0 0 0 ## [2,] 0 1 0 0 0 0 0 ## [3,] 0 0 1 0 0 0 0 ## [4,] 0 0 0 1 0 0 0 ## [5,] 0 0 0 0 1 0 0 ## [6,] 0 0 0 0 0 1 0 ## [7,] 0 0 0 0 0 0 1 Vi forventer selvsagt korrelasjon i vår korrelasjonsmatrise, og siden Bartletts test bruker nullhypotsene om at det ikke er korrelasjon, forteller en signifikant test at det er meningsfullt å gjennomføre en datareduksjonsteknikk. Hvis vår korrelasjonsmatrise ikke er signifikant forskjellig fra en matrise med null korrelasjon mellom variablene gir det ingen mening i å se etter strukturer av korrelasjoner. I tillegg kan vi se på determinant: # Bruker pakken: Matrix det(fa_spm_cor) ## [1] 0.04052472 En positiv verdi indikerer også at datasettet egner seg for faktoranalyse. 2.3.2 Antall faktorer 2.3.2.1 Kaisers kriterium # Bruker pakken: nFactors eigenComputes(fa_spm) ## [1] 3.5289174 1.6219116 1.2294074 0.6032709 0.5332548 0.4581451 0.4093023 ## [8] 0.3446454 0.2711450 Kaisers kriterium tilsier 3 faktorer. 2.3.2.2 Scree plott # Bruker pakken: EFAtools SCREE(fa_spm_cor, eigen_type = &quot;EFA&quot;) ## ## Eigenvalues were found using EFA. Det kan se ut til at albuen/knekkpunktet tilsier 3 faktorer. 2.3.2.3 Parallell analyse # Bruker pakken: EFAtools PARALLEL(fa_spm, eigen_type = &quot;EFA&quot;) ## i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data. ## Parallel Analysis performed using 1000 simulated random data sets ## Eigenvalues were found using EFA ## ## Decision rule used: means ## ## -- Number of factors to retain according to ------------------------------------ ## ## ( ) EFA-determined eigenvalues: 3 Dette peker mot tre faktorer. 2.3.3 Analyse Vi kjører en faktoranalyse med 3 faktorer uten rotasjon. # Bruker pakken: EFAtool urotasjon &lt;- EFA(fa_spm, n_factors = 3, method = &quot;ML&quot;) ## i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data. urotasjon ## ## EFA performed with type = &#39;EFAtools&#39;, method = &#39;ML&#39;, and rotation = &#39;none&#39;. ## ## -- Unrotated Loadings ---------------------------------------------------------- ## ## F1 F2 F3 ## spm_1 .502 .454 .338 ## spm_2 .556 .371 .418 ## spm_3 .470 .318 .378 ## spm_4 .499 -.241 .119 ## spm_5 .635 -.592 .056 ## spm_6 .711 -.428 .016 ## spm_7 .594 .395 -.364 ## spm_8 .605 .151 -.282 ## spm_9 .638 .289 -.386 ## ## -- Variances Accounted for ----------------------------------------------------- ## ## F1 F2 F3 ## SS loadings 3.063 1.299 0.810 ## Prop Tot Var 0.340 0.144 0.090 ## Cum Prop Tot Var 0.340 0.485 0.575 ## Prop Comm Var 0.592 0.251 0.157 ## Cum Prop Comm Var 0.592 0.843 1.000 ## ## -- Model Fit ------------------------------------------------------------------- ## ## &lt;U+0001D712&gt;²(12) = 11.82, p = .460 ## CFI = 1.00 ## RMSEA [90% CI] = .00 [.00; .05] ## AIC = -12.18 ## BIC = -59.01 ## CAF = .50 Vi kan se på Unrotated loadings hvordan de ulike spørsmålene lader på de tre faktorene. Det er ikke gitt at bildet er helt enkelt å tolke. For noen spørsmål - som 1, 2, og 3 - ser vi loadings på alle tre faktorene. For andre er det klarere loading på en faktor, eller positivt på en og negativt på en annen. Et hjelpemiddler for å tolke modellen er rotasjon. Rotasjon innebærer egnetlig brae å se på variablene og faktorene fra en annen vinkelt. Som Hartmann et al. (2018) påpeker: The purpose of a rotation is to produce factors with a mix of high and low loadings and few moderate-sized loadings. The idea is to give meaning to the factors, which helps interpret them. From a mathematical viewpoint, there is no difference between a rotated and unrotated matrix. The fitted model is the same, the uniquenesses are the same, and the proportion of variance explained is the same. Det finnes to hovedgrupper rotasjoner: ortogonal og oblikk. Blant ortogonale rotasjonsteknikker finner vi varimax, quartimax og equimax. (Direct) oblimin og promax er vanlige oblikke rotasjoner. En hovedforskjell er at ved ortogonal rotasjon tillates ikke at faktorene er korrelerte, mens ved oblikk rotasjon kan faktorene korrelere. Her må vi altså gå tilbake til vår teoretiske forståelse av hva vi undersøker. Svært ofte i samfunnsvitenskapene (vil vi hevde) ønsker vi å tillate at faktorene kan korrelere ved rotasjon (vi antar at i veldig mange tilfeller vil dette være teoretisk fornuftig). I så fall bør vi bruke oblikk rotasjon. Hvis vi har teoretiske vurderinger som tilsier at faktorene ikke korrelerer velger vi ortogonalt. Under kapittelet om PCA viste vi ortogonal rotasjon. Dette innebærer at aksene forblir ortogonale på hverandre, mens ved oblikk rotasjon kan aksenens vinkler på hverandre variere. Når vi kjører en ny faktoranalyse med 3 faktorer ser det slik ut: # Bruker pakken: EFAtool mrotasjon &lt;- EFA(fa_spm, n_factors = 3, method = &quot;ML&quot;, rotation = &quot;promax&quot;) ## i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data. mrotasjon ## ## EFA performed with type = &#39;EFAtools&#39;, method = &#39;ML&#39;, and rotation = &#39;promax&#39;. ## ## -- Rotated Loadings ------------------------------------------------------------ ## ## F1 F3 F2 ## spm_1 -.066 .076 .732 ## spm_2 .056 -.022 .782 ## spm_3 .049 -.043 .688 ## spm_4 .521 -.018 .140 ## spm_5 .903 -.055 -.069 ## spm_6 .788 .094 .004 ## spm_7 -.103 .814 .039 ## spm_8 .142 .623 -.010 ## spm_9 .013 .808 -.027 ## ## -- Factor Intercorrelations ---------------------------------------------------- ## ## F1 F2 F3 ## F1 1.000 0.380 0.268 ## F2 0.380 1.000 0.504 ## F3 0.268 0.504 1.000 ## ## -- Variances Accounted for ----------------------------------------------------- ## ## F1 F2 F3 ## SS loadings 3.063 1.299 0.810 ## Prop Tot Var 0.340 0.144 0.090 ## Cum Prop Tot Var 0.340 0.485 0.575 ## Prop Comm Var 0.592 0.251 0.157 ## Cum Prop Comm Var 0.592 0.843 1.000 ## ## -- Model Fit ------------------------------------------------------------------- ## ## &lt;U+0001D712&gt;²(12) = 11.82, p = .460 ## CFI = 1.00 ## RMSEA [90% CI] = .00 [.00; .05] ## AIC = -12.18 ## BIC = -59.01 ## CAF = .50 Vi ser en klar struktur i hvilke spørsmål som lader på hvilke faktorer (dette eksempelet er konstruert for å vise en veldig klar faktorstruktur, ofte vil det være større rom for tolkning). 2.3.3.1 Model fit Vi ser at vi presenteres for flere mål på model fit (se f.eks. Finch (2020)). De ulike indeksene måler ulike aspekter av model fit. Vi skal gå inn på to av dem. Husk at dette datasettet er generert for å vise en utmerket modell - med andre data vil du sjeldent oppleve så gode verdier på model fit som her. CFI = Comparative Fit Index. Verdiene her kan være mellom 0 og 1, og verdier over 0.9 regnes som en god fit (Hu &amp; Bentler, 1999) (en mer konservativ terskel kan være 0.95). RMSEA = Root Mean Square Error of Approximation. Her oppgis ofte verdiene 0.01, 0.05 og 0.08 som henhodsvis utmerket, god og middels. Finch (2020) viser til 0.05 som en cut-off verdi. 2.3.3.2 Sammenlikning uten rotasjon, med ortogonal (varimax) og oblikk (promax) rotasjon fa_ingenrot &lt;- factanal(fa_spm, factors = 3, rotation = &quot;none&quot;) fa_varimax &lt;- factanal(fa_spm, factors = 3, rotation = &quot;varimax&quot;) fa_promax &lt;- factanal(fa_spm, factors = 3, rotation = &quot;promax&quot;) par(mfrow = c(1,3)) plot(fa_ingenrot$loadings[,1], fa_ingenrot$loadings[,2], xlab = &quot;Faktor 1&quot;, ylab = &quot;Faktor 2&quot;, ylim = c(-1,1), xlim = c(-1,1), main = &quot;Uten rotasjon&quot;) abline(h = 0, v = 0) plot(fa_varimax$loadings[,1], fa_varimax$loadings[,2], xlab = &quot;Faktor 1&quot;, ylab = &quot;Faktor 2&quot;, ylim = c(-1,1), xlim = c(-1,1), main = &quot;Med varimax rotasjon&quot;) abline(h = 0, v = 0) plot(fa_promax$loadings[,1], fa_promax$loadings[,2], xlab = &quot;Faktor 1&quot;, ylab = &quot;Faktor 2&quot;, ylim = c(-1,1), xlim = c(-1,1), main = &quot;Med promax rotasjon&quot;) abline(h = 0, v = 0) 2.3.4 Konfirmerende faktoranalyse (CFA) "],["referanser.html", "Referanser", " Referanser "],["vedlegg-x---session-info.html", "Vedlegg X - Session Info", " Vedlegg X - Session Info ## R version 4.1.2 (2021-11-01) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19044) ## ## Locale: ## LC_COLLATE=Norwegian Bokmål_Norway.1252 ## LC_CTYPE=Norwegian Bokmål_Norway.1252 ## LC_MONETARY=Norwegian Bokmål_Norway.1252 ## LC_NUMERIC=C ## LC_TIME=Norwegian Bokmål_Norway.1252 ## ## Package version: ## abind_1.4-5 askpass_1.1 assertthat_0.2.1 ## backports_1.4.1 base64enc_0.1-3 bayestestR_0.11.5 ## bigassertr_0.1.5 bigparallelr_0.3.2 bigstatsr_1.5.6 ## bit_4.0.4 bit64_4.0.5 bitops_1.0.7 ## blob_1.2.3 bookdown_0.26 boot_1.3-28 ## brio_1.1.3 broom_0.8.0 broom.helpers_1.7.0 ## bslib_0.3.1 calibrate_1.7.7 callr_3.7.0 ## car_3.0-12 carData_3.0-5 caTools_1.18.2 ## cellranger_1.1.0 checkmate_2.1.0 class_7.3-19 ## classInt_0.4-3 cli_3.2.0 clipr_0.8.0 ## cluster_2.1.2 coda_0.19-4 codetools_0.2-18 ## colorspace_2.0-3 commonmark_1.8.0 compiler_4.1.2 ## corrplot_0.92 cowplot_1.1.1 cpp11_0.4.2 ## crayon_1.5.1 crosstalk_1.2.0 curl_4.3.2 ## data.table_1.14.2 datasets_4.1.2 datawizard_0.4.0 ## DBI_1.1.2 dbplyr_2.1.1 dendextend_1.15.2 ## DEoptimR_1.0-11 desc_1.4.1 diffobj_0.3.5 ## digest_0.6.29 Directional_5.3 doParallel_1.0.17 ## dplyr_1.0.8 DT_0.22 dtplyr_1.2.1 ## e1071_1.7-9 EFAtools_0.4.1 effects_4.2-1 ## effectsize_0.6.0.1 ellipse_0.4.2 ellipsis_0.3.2 ## emmeans_1.7.3 EnvStats_2.7.0 estimability_1.3 ## evaluate_0.15 extrafont_0.18 extrafontdb_1.0 ## factoextra_1.0.7 FactoMineR_2.4 fansi_1.0.3 ## farver_2.1.0 fastmap_1.1.0 ff_4.0.5 ## flashClust_1.1.2 flextable_0.7.0 flock_0.7 ## forcats_0.5.1 foreach_1.5.2 foreign_0.8-81 ## Formula_1.2-4 fs_1.5.2 future_1.25.0 ## future.apply_1.9.0 gargle_1.2.0 gdtools_0.2.4 ## generics_0.1.2 ggeffects_1.1.2 ggfortify_0.4.14 ## ggplot2_3.3.5 ggpubr_0.4.0 ggrepel_0.9.1 ## ggsci_2.9 ggsignif_0.6.3 globals_0.14.0 ## glue_1.6.2 goftest_1.2-3 googledrive_2.0.0 ## googlesheets4_1.0.0 GPArotation_2022.4-1 gplots_3.1.3 ## graphics_4.1.2 grDevices_4.1.2 grid_4.1.2 ## gridExtra_2.3 gt_0.5.0 gtable_0.3.0 ## gtools_3.9.2 gtsummary_1.6.0 haven_2.4.3 ## highr_0.9 Hmisc_4.7-0 hms_1.1.1 ## htmlTable_2.4.0 htmltools_0.5.2 htmlwidgets_1.5.4 ## httr_1.4.2 ids_1.0.1 insight_0.17.0 ## interactions_1.1.5 ISLR_1.4 isoband_0.2.5 ## iterators_1.0.14 jpeg_0.1-9 jquerylib_0.1.4 ## jsonlite_1.8.0 jtools_2.2.0 kableExtra_1.3.4 ## KernSmooth_2.23-20 knitr_1.39 labeling_0.4.2 ## labelled_2.9.0 later_1.3.0 latex2exp_0.9.4 ## lattice_0.20-45 latticeExtra_0.6-29 lavaan_0.6.11 ## lazyeval_0.2.2 leaps_3.1 lifecycle_1.0.1 ## listenv_0.8.0 lme4_1.1-29 lmtest_0.9-40 ## lubridate_1.8.0 magrittr_2.0.3 maptools_1.1.4 ## MASS_7.3-54 Matrix_1.3-4 MatrixModels_0.5.0 ## methods_4.1.2 mgcv_1.8-38 mime_0.12 ## minqa_1.2.4 mitools_2.4 mnormt_2.0.2 ## modelr_0.1.8 modelsummary_0.10.0 multiUS_1.1.0 ## munsell_0.5.0 mvtnorm_1.1-3 nFactors_2.4.1 ## nlme_3.1-153 nloptr_2.0.0 nnet_7.3-16 ## normtest_1.1 nortest_1.0-4 numDeriv_2016.8.1.1 ## officer_0.4.2 olsrr_0.5.3 openssl_2.0.0 ## outliers_0.15 pacman_0.5.1 palmerpenguins_0.1.0 ## pander_0.6.5 parallel_4.1.2 parallelly_1.31.1 ## parameters_0.17.0 paran_1.5.2 pbivnorm_0.6.0 ## pbkrtest_0.5.1 performance_0.9.0 pillar_1.7.0 ## pkgconfig_2.0.3 pkgload_1.2.4 plotly_4.10.0 ## plyr_1.8.7 png_0.1-7 polynom_1.4.1 ## praise_1.0.0 prettyunits_1.1.1 processx_3.5.3 ## progress_1.2.2 progressr_0.10.0 promises_1.2.0.1 ## proxy_0.4-26 ps_1.7.0 psych_2.2.3 ## purrr_0.3.4 qqplotr_0.0.5 quadprog_1.5-8 ## quantmod_0.4.20 quantreg_5.88 qwraps2_0.5.2 ## R6_2.5.1 RANN_2.6.1 rappdirs_0.3.3 ## RColorBrewer_1.1-3 Rcpp_1.0.8.3 RcppArmadillo_0.11.0.0.0 ## RcppEigen_0.3.3.9.2 RcppGSL_0.3.11 RcppZiggurat_0.1.6 ## readr_2.1.2 readxl_1.4.0 rematch_1.0.1 ## rematch2_2.1.2 remotes_2.4.2 reprex_2.0.1 ## reshape2_1.4.4 Rfast_2.0.6 Rfast2_0.1.3 ## rgl_0.108.3 RhpcBLASctl_0.21.247.1 rjstat_0.4.2 ## rlang_1.0.2 rmarkdown_2.14 rmio_0.4.0 ## rnaturalearth_0.1.0 rnorsk_0.1.0 robustbase_0.95-0 ## rpart_4.1-15 rprojroot_2.0.3 RSpectra_0.16.1 ## rstatix_0.7.0 rstudioapi_0.13 Rttf2pt1_1.3.10 ## rvest_1.0.2 s2_1.0.7 sass_0.4.1 ## scales_1.2.0 scatterplot3d_0.3.41 selectr_0.4.2 ## sf_1.0-7 sjlabelled_1.2.0 sjmisc_2.8.9 ## sjPlot_2.8.10 sjstats_0.18.1 sp_1.4-6 ## SparseM_1.81 splines_4.1.2 stats_4.1.2 ## stats4_4.1.2 stringi_1.7.6 stringr_1.4.0 ## survey_4.1-1 survival_3.2-13 svglite_2.1.0 ## sys_3.4 systemfonts_1.0.4 table1_1.4.2 ## tables_0.9.6 testthat_3.1.4 tibble_3.1.6 ## tidyr_1.2.0 tidyselect_1.1.2 tidyverse_1.3.1 ## tinytex_0.38 tmvnsim_1.0-2 tools_4.1.2 ## tseries_0.10-50 TTR_0.24.3 tzdb_0.3.0 ## units_0.8-0 utf8_1.2.2 utils_4.1.2 ## uuid_1.0-4 vctrs_0.4.0 viridis_0.6.2 ## viridisLite_0.4.0 vroom_1.5.7 waldo_0.4.0 ## webshot_0.5.3 withr_2.5.0 wk_0.6.0 ## writexl_1.4.0 xfun_0.30 xml2_1.3.3 ## xtable_1.8-4 xts_0.12.1 yaml_2.3.5 ## zip_2.2.0 zoo_1.8-10 "]]
